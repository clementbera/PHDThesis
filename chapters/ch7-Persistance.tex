\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Runtime state persistence across start-ups}
\label{chap:persistence}
\minitoc

This chapter describes how the Sista VM persists the runtime state across multiple VM start-ups, including the running green threads and the optimised code. The first Section discusses how snapshots are implemented in the existing Pharo runtime, including how the code and the running green threads are persisted across start-ups and their interactions with Sista. Section \ref{sec:warmup} focuses on the main issue: the start-up performance of many VMs today is significantly worse than the peak performance. Several cases where the start-up performance is a problem are described.  Section \ref{sec:relWork} compares our approach to existing VMs. Few VMs attempts to persist the runtime state across multiple start-ups, but some VMs include solutions to improve start-up performance, solving the same problem.

\section{Snapshots and persistence}

Snapshots are available in multiple object-oriented languages such as Smalltalk \cite{Gold83a} and later Dart \cite{Anna13a}. As discussed in Section \ref{par:snapshot}, in our case we use Pharo which features snapshots. A snapshot is a serialized form of all the objects present at a precise moment in the runtime. Everything is an object in Pharo, including green threads or v-functions. To start-up Pharo, the virtual machine loads all the objects from a snapshot and resumes the execution based on the green thread that was active at snapshot time. This is how Pharo is normally launched.

\paragraph{Pharo development workflow.}A Pharo programmer does not modify source code in files as many other programming languages. For development, Pharo is started using a snapshot which includes development tools, user interface elements and a source code to v-function compiler. When started, the programmer can open the development tools and write or edit the source code of a function. Then, the compiler generates a v-function from the source code (which is implicitely added in the heap). Then, the programmer may take a new snapshot, which includes the changes made. Further start-ups, on the new snapshot, features the changes made by the programmer. We note that in this paragraph we described the normal development flow of a Smalltalk programmer: this is not a workflow the programmer can do but does not normally do, this is how all programmers currently do it.

\paragraph{Application to Sista.}In the context of Sista, optimised v-functions are installed at runtime by Scorch. Those functions effectively modify the current heap of objects. Hence, when a new snapshot is taken, optimised v-functions are persisted. The next start-up of Pharo will use directly the optimised v-functions.

\paragraph{Green threads and snapshots.}To persist running green threads in a platform-independent way, to take a snapshot, the VM reifies each stack frame to a context object as explained in Section \ref{par:frameToContext}. Effectively, this means that only v-frames are persisted: n-frames are converted to v-frames to be part of the snapshot. N-frames cannot be persisted in any case as snapshots are machine-independent.

When the VM starts from a snapshot, all running green threads are executed using the v-function interpreter. However, once a function is called multiple times or a loop is interpreted a certain number of iterations, Cogit generates a n-function for the corresponding v-function (optimised or not), and the runtime resumes with the n-function.

\paragraph{Conclusion.}To conclude, programmers normally work on Pharo by modifying the current heap, for example by adding new v-functions to method dictionaries of classes, and then take a snapshot of the heap to save their code. V-functions are persisted in snapshot but n-functions are not. 

In Sista, the optimising JIT is the combination of Scorch, which generates and installs optimised v-functions, and Cogit, which generates and installs n-functions. 

Optimised v-functions generated by Scorch are, without any additional work, persisted across multiple start-ups as part of the snapshot like unoptimised v-functions. N-functions generated by Cogit are never persisted. 

Most of the compilation time is currently spent in Scorch, hence, if Pharo is started using a snapshot including optimised v-functions, Pharo can reach peak performance very quickly. Green threads using optimised functions are persisted in the snapshot in the form of optimised and unoptimised v-frames (n-frames are converted to v-frames, they cannot be persisted because they refer to n-functions). 

Sista allows Pharo to have very good peak performance thanks to the optimising JIT while avoiding one of the major draw-back of just-in-time compilation: peak performance can be reached quickly after start-up thanks to persisted optimised v-functions.

\section{Warm-up time problem}
\label{sec:warmup}

The most important problem solved by persisting the runtime state across start-up is the warm-up time problem, i.e., the time wasted by the VM at each start-up to reach peak performance. Depending on use-cases, the warm-up time may or may not matter. In long-running applications, the warm-up time required to reach peak performance is negligible compared to the overall uptime of the application. However, when applications are started frequently and are short-lived, warm-up time matters.

We give three examples where the virtual machine start-up time matters. These examples are specific, but they give an idea where the warm-up performance is a serious problem.

\paragraph{Distributed applications.}
Modern large distributed applications run on hundreds, if not thousands, of machines such as the slaves one can rent on Amazon Web Services. Slaves are usually rented per hour, though now some services such as Amazon Lambda allows one to rent a slave for small amount of time down to a hundred milliseconds. Depending on usage, the application automatically rents new slaves or frees used slaves. This way, the application scales up very well as thousands of slaves are rent if needed, while a single slave is rent if the application is used very little. The server cost of the distributed application depends purely on how much computation power is needed: one pays for slaves when they are used and does not pay when they are not used.

The problem is that to reduce the cost to the minimum, the application needs to rent a slave when needed, but frees it at the 10th of second where the slave is not used to avoid paying for an unused slave. Doing so implies having potentially very short-lived slaves when the application usage varies greatly from a 10th of a second to the next 10th of a second. Slaves could have a life expectancy of a couple hundred milliseconds. Now, if the slave does not have enough time to reach peak performance in its short life-time, the money saved by not paying for unused slaves is dominated by the money wasted in computation power used in the optimising JIT to reach peak performance. To have very short-lived slaves worth it, the time between the slave start-up and the peak performance of the application used has to be as small as possible. A good VM for such kind of scenariois a VM where peak performance is reached immediately after start-up.

\paragraph{Mobile application.}
In the case of mobile applications, the start-up performance matters because of battery consumption. During warm-up time, the optimising compiler recompiles frequently used code. All this compilation process requires time and energy, whereas the application is not run. In the example of the Android runtime, the implementation used JIT compilation with the Dalvik VM \cite{Born08a}, then switched to client-side ahead of time compilation (ART) to avoid that energy consumption at start-up, and is now switching back to JIT compilation because of the AOT (Ahead of Time compiler) constraints \cite{Geof15a}. These different attempts show the difficulty to build a system that requires JIT compilation for high performance but can't afford an energy consuming start-up time.

\paragraph{Web pages.}
Web pages sometimes execute just a bit of Javascript code at start-up, or use extensively Javascript in their lifetime (in this latter case, one usually talk about web application). A Javascript virtual machine has to reach peak performance as quickly as possible to perform well on web pages where only a bit of Javascript code is executed at start-up, while it has also to perform well on long running web applications.

\section{Related work}
\label{sec:relWork}

This section discusses other strategies implemented in other VMs and research projects to decrease the warm-up time.

\subsection{Preheating through snapshots}

\paragraph{Dart snapshots.}

The Dart programming languages features snapshots for fast application start-up. In Dart, the programmer can generate different kind of snapshots \cite{Anna13a}. Since that publication, the Dart team have added two new kind of snapshots, specialized for iOS and Android application deployment, which are the most similar to our snapshots.

\subparagraph{Android.} A Dart snapshot for an Android application is a complete representation of the application code and the heap once the application code has been loaded but before the execution of the application. The Android snapshots are taken after a warm-up phase to be able to record call site caches in the snapshot. The call site cache is a regular heap object accessed from machine code, and its presence in the snapshot allows to persist type feedback and call site frequency.

In this case, the code is loaded pre-optimised with inline caches prefilled values. However, optimised functions are not loaded as our architecture allows to do. Only unoptimised code with precomputed runtime information is loaded.

\subparagraph{iOS.} For iOS, the Dart snapshot is slightly different as iOS does not allow JIT compilers. All reachable functions from the iOS application are compiled ahead of time, using only the features of the Dart optimising compiler that don't require dynamic deoptimisation. A shared library is generated, including all the instructions, and a snapshot that includes all the classes, functions, literal pools, call site caches, etc.

This second case is difficult to compare to our architecture: iOS forbids machine code generation, which is currently required by our architecture. A good application of our architecture to iOS is future work.

\paragraph{Cloneable VMs.}

In the work of Kawachiya and all~\cite{Kawa07a}, extensions of a Java VM allows to clone a running VM to quicken the start-up. The heap is cloned, in a similar way to our snapshot, but the n-functions are also cloned. Cloning n-functions improves start-up performance over our approach, but the clone is processor-dependent: there is no way of cloning with their approach a Java runtime from an x86 machine to an ARMv6 machine. Our approach requires slightly more warm-up time to quickly compile our optimised n-functions to n-functions, but our approach is platform-independent.

\subsection{Fast warm-up}

An alternative to snapshots is to improve the JIT compiler so the peak performance can be reached as early as possible. The improvements would consists of decreasing the JIT compilation time by improving the efficiency of the JIT code, or have better heuristic so the JIT can generate optimised code with the correct speculations with little runtime information.

\paragraph{Tiered architecture}
One solution, used the Webkit VM\cite{Webkit15}, is to have a tiered architecture with many tiers. In the version of Webkit in production from March 2015 to February 2016 \cite{Webkit15}, the code is:
\begin{itemize}
\item interpreted by a bytecode interpreter the first 6 executions.
\item compiled to machine code at 7th execution, with a non-optimising compiler, and executed as machine code up to 66 executions.
\item recompiled to more optimised machine code at 67th execution, with an optimising compiler doing some but not all optimisations, up to 666 executions.
\item recompiled to heavily optimised machine code at 667th execution, with an optimising compiler using LLVM as a backend.
\end{itemize}

At each step, the compilation time is greater but the execution time decreases. This tiered approach (4 tiers in the case of Webkit), allows to have good performance from start-up, while reaching high performance for long running code. This kind of approaches has also draw-backs: the VM development team needs to maintain and evolve four different tiers.

\paragraph{Saving runtime information.}

To reach quickly peak performance, an alternative of saving optimised code is to save the runtime information. The Dart snapshot saves already the call site information in its Android snapshots. Other techniques are available.

In Strongtalk \cite{Sun06}, a high-performance Smalltalk that has never reached production, it is possible to save the inlining decision of the optimising compiler in a separate file. The optimising compiler can then reuse this file to make the right inlining decision the first time a hot spot is detected.

In the work of Arnold and all~\cite{Arno05c}, the profiling information of unoptimised runs is persisted in a repository shared by multiple VMs. This allows new VM starting-up to re-use the information of other and previous VM runs to direct compiler optimisations.

Saving runtime information decreases the warm-up time as the optimising JIT can speculate accurately on the program behavior with very few runs. However, on the contrary to our approach, time is still wasted optimising functions.

\paragraph{Saving machine code.}

In the Azul VM Zing \cite{Azul}, available for Java, the official web site claims that "operations teams can save accumulated optimisations from one day or set of market conditions for later reuse" thanks to the technology called \emph{Ready Now!}. In addition, the website precises that the Azul VM provides an API for the developer to help the JIT to make the right optimisation decisions. As Azul is closed source, implementation details are not entirely known. 

However, word has been that the Azul VM reduces the warm-up time by saving machine code across multiple start-ups. If the application is started on another processor, then the saved machine code is simply discarded. We did not go in this direction to persist the optimisation in a platform-independent way (in our architecture, starting the application on x86 instead of ARMv5 does not require the saved optimised code to be discarded), but we have a small overhead due to the bytecode to machine code translation at each start-up. In addition, we believe it's very difficult to persist correctly machine code compared to persisting bytecodes.

Aside from Azul, the work of Reddi and all~\cite{Redd07a} details how they persist the machine code generated by the optimising JIT across multiple start-ups of the VM. JRockit~\cite{JRockit}, an Oracle product, is a production Java VM allowing to persist the machine code generated by the optimising JIT across multiple start-ups.

We did not go in the direction of machine code persistence as we wanted to keep the snapshot platform-independent way: in our architecture, starting the application on x86 instead of ARMv5 does not require the saved optimized code to be discarded, while the other solutions discussed in this paragraph do. However, we have a small overhead due to the bytecode to machine code translation at each start-up. In addition, the added complexity of machine code persistence over bytecode persistence should not be underestimated.

\paragraph{Ahead-of-time analysis.}

In the work of Krintz and Calder, static analysis done ahead of time on Java code generate annotations that are used by the optimising JIT to reduce compilation time (and hence, the warm-up time). As for the persistence of runtime information, on the contrary to our approach, time is still wasted at runtime optimising functions.

\paragraph{Ahead-of-time compilation.}

The last alternative is to pre-optimise the code ahead of time. This can be done by doing static analysis over the code to try to infer types. Applications for the iPhone are a good example where static analysis is used to pre-optimise the Objective-C application. The peak performance is lower than with a JIT compiler if the program uses a lot of virtual calls, as static analysis are not as precised as runtime information on highly dynamic language. 
However, if the program uses few dynamic features (for example most of the calls are not virtual) and is running on top of a high-performance language kernel like the Objective-C kernel, the result can be satisfying.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Conclusion} This chapter discusses how the runtime state is persisted across multiple start-ups, improving the performance during start-up. The next chapter validates the Sista architecture, mainly through performance evaluation in a set of benchmarks. The validation chapter also evaluates the Sista VM performance when the runtime state is persisted across multiple start-ups.

\ifx\wholebook\relax\else
    \end{document}
\fi