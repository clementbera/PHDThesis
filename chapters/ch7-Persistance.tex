\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Runtime state persistance across start-ups}
\label{chap:persistance}
\minitoc

TODO: sec1,sec3,sec4 add papers from review in conf

This chapter describes how the Sista VM persists the runtime state across multiple VM start-ups, including the running green threads and the optimised code. The first section discusses how snapshots are implemented in Smalltalk, including how the code and the running green threads are persisted across start-ups. Section \ref{sec:warmup} focuses on the main issue: the start-up performance of many VMs today is far worse than the peak performance. Several cases where the start-up performance is a problem are described. Section \ref{sec:persistanceSol} details our solution: optimised v-functions are persisted across multiple start-ups, allowing to persist optimised code and the running green threads. Section \ref{sec:relWork} compares our approach to existing VMs. Few VMs attempts to persist the runtime state across multiple start-ups, but some VMs include solutions to improve start-up performance, solving the same problem of poor start-up performance.

\section{Snapshots and persistance}

- discuss snapshots again. 
and persist v-function, 
Explain briefly cogit mapping and v-function persistance.
Explain saving processes
platform independent
Restart from snapshot and move from v-function interpretation to n-func in loop and function call.

TO CHECK


In the Smalltalk terminology, a snapshot, also called \emph{image}, is a sequence of bytes that represents a serialized form of all the objects present at a precise moment in the runtime. As everything is an object in Smalltalk, including processes, the virtual machine can, at start-up, load all the objects from a snapshot and resume the execution based on the active process precised by the snapshot. In fact, this is the normal way of launching a Smalltalk runtime. 

In Dart, the word snapshot refers to the serialized form of one or more Dart objects \cite{Anna13a}. Dart snapshots can save the whole heap, as part of their \emph{full snapshots}, but as far as we know it is not possible in this language to save processes. Hence, the virtual machine always restarts at the \ct{main} function once the snapshot is loaded.

One interesting problem in snapshots is how to save the call stack, \ie the processes. It possible in the Smalltalk virtual machine to convert each stack frame to a context object reifying the function activation. To perform a snapshot, each stack frame is reified and only objects are saved in the snapshot. When the snapshot is restarted, the virtual machine recreates a stack frame for each function activation lazily from the context objects. 

TOCHECK


To avoid this warm-up time, this paper introduces an architecture to save a platform-independent version of the optimized code as part of a snapshot. Snapshots are available in multiple object-oriented languages such as Smalltalk \cite{Gold83a} and later Dart \cite{Anna13a}. Snapshots allow the program to save the heap in a given state, and the virtual machine can resume execution from this snapshot later. Usually, compiled code is available in different versions. On the one hand, a bytecoded version, which is on the heap if the bytecoded version of functions is reified as an object (as in Dart and Smalltalk).  On the other hand one or several machine code versions are available in the machine code zone. Machine code versions are usually not part of the heap directly but of a separated part of memory which is marked as executable. Snapshots cannot save easily machine code versions of functions as a snapshot needs to be platform-independent and machine code versions of functions are not regular objects.

END TO CHECK

\section{Warm-up time problem}
\label{sec:warmup}

The most important problem we attempt to solve by persisting the runtime state across start-up is the warm-up time problem, i.e., the time wasted by the VM at each start-up to reach peak performance. Depending on use-cases, the warm-up time may not matter. The warm-up time required to reach peak performance may be negligible compared to the overall runtime of the application. However, when applications are started frequently and are short-lived, this time can matter.

We give three examples where the virtual machine start-up time matters. These examples are specific, but they give an idea where the warm-up performance is a serious problem.

\paragraph{Distributed applications.}
Modern large distributed applications run on hundreds, if not thousands, of machines such as the slaves one can rent on Amazon Web Services. Slaves are usually rented per hour, though now some contracts allow one to rent a slave for 5 minutes or even 30 seconds. If the application needs more power, it rents new slaves, if it does not need it anymore, it frees the slaves. The slaves are paid only when needed, no application users imply no cost whereas the application can scale very well.

The problem is that to reduce the cost to the minimum, the best would be to rent a slave when needed, and at the second where the slave is not used, to free it not to pay anymore for it. Doing that implies having very short lived slaves, with an order of 30 seconds life-time for example. To be worth it, the time between the slave start-up and the peak performance of the language used has to be as small as possible. A good VM for such kind of scenario should reach peak performance very fast.

\paragraph{Mobile application.}
In the case of mobile applications, the start-up performance matters because of  battery consumption. During warm-up time, the optimizing compiler recompiles frequently used code. All this compilation process requires time and energy, whereas the application is not run. In the example of the Android runtime, the implementation used JIT compilation with the Dalvik VM \cite{Born08a}, then switched to client-side ahead of time compilation (ART) to avoid that energy consumption at start-up, and is now switching back to JIT compilation because of the AOT (Ahead of Time compiler) constraints \cite{Geof15a}. These different attempts show the difficulty to build a system that requires JIT compilation for high performance but can't afford an energy consuming start-up time.

\paragraph{Web pages.}
Web pages sometimes execute just a bit of Javascript code at start-up, or use extensively Javascript in their lifetime (in this latter case, one usually talk about web application). A Javascript virtual machine has to reach peak performance as quickly as possible to perform well on web pages where only a bit of Javascript code is executed at start-up, while it has also to perform well on long running web applications.

\section{Persistance of optimised virtual functions}
\label{sec:persistanceSol}

TO CHECK

With this architecture, the Sista VM can reach peak performance almost immediately after start-up if it starts from a snapshot where optimized code was persisted.

TO CHECK



Solution to the problem is to persist opt code, but we want to keep it platform-independent and not too complicated.

- Solution snapshot optimised v-functions - work the same way.
platform independent, moving from v-func interpretation to n-function.
Discussion on reg alloc and Cogit compila


Next PARAGRAPH IS FIXED
The Sista architecture solves this warm-up time problem by persisting optimised v-functions across start-ups. This way, the virtual machine can start in a pre-heated state. The architecture allows to reuse and take advantages of all the existing runtime compilation techiniques to have a very good peak performance, but with the advantage of being effective with limited warm-up time. 



\paragraph{Platform-independent.}We save optimizations across start-ups in a platform-independent way: this implies that we cannot save directly machine code. As our technique depends on snapshots, the platform-dependency depends on the snapshot being dependent on a platform or not. 

\paragraph{Bytecode saved.}Our approach saves the optimized code as a bytecoded version because the languages with snapshots already supports saving bytecoded functions as part of the snapshot. Bytecode is already a compressed and platform-independent representation of executable code. The optimized code is saved using an extended bytecode set to encode unchecked operations. %\sd{However, we believe that our approach is not exclusively tight with snapshots but could be used with a system serialising bytecodes.} \md{the problem is that then the whole point of the paper is a bit weak... I would maybe discuss it in the discussion section} 

\paragraph{Simplicity.}We try to keep the solution simple by reusing the existing snapshot infrastructure, which can persist the bytecode version of each methods. We do not want to extend the snapshot logic to be able to persist machine code as it is very complex. More precisely, we would need to extend the snapshot logic with specific code for each back-end supported (currently at least ARMv5, x86, x64 and MIPS little endian) and we would need to handle cases such as position-dependent machine code.


\section{Related work}
\label{sec:relWork}

\emph{TO DO: add the ones from author review.
+ from review of author}

This section discusses other strategies implemented in other VMs and research projects to decrease the warm-up time.

\subsection{Preheating through snapshots in Dart}

The Dart programming languages features snapshots for fast application start-up. In Dart, the programmer can generate different kind of snapshots \cite{Anna13a}. Since that publication, the Dart team have added two new kind of snapshots, specialized for iOS and Android application deployment, which are the most similar to our snapshots.

\paragraph{Android.} A Dart snapshot for an Android application is a complete representation of the application code and the heap once the application code has been loaded but before the execution of the application. The Android snapshots are taken after a warm-up phase to be able to record call site caches in the snapshot. The call site cache is a regular heap object accessed from machine code, and its presence in the snapshot allows to persist type feedback and call site frequency.

In this case, the code is loaded pre-optimized with inline caches prefilled values. However, optimized functions are not loaded as our architecture allows to do. Only unoptimized code with precomputed runtime information is loaded.

\paragraph{iOS.} For iOS, the Dart snapshot is slightly different as iOS does not allow JIT compilers. All reachable functions from the iOS application are compiled ahead of time, using only the features of the Dart optimizing compiler that don't require dynamic deoptimization. A shared library is generated, including all the instructions, and a snapshot that includes all the classes, functions, literal pools, call site caches, etc.

This second case is difficult to compare to our architecture: iOS forbids machine code generation, which is currently required by our architecture. A good application of our architecture to iOS is future work.

\subsection{Fast warm-up}

An alternative to snapshots is to improve the JIT compiler so the peak performance can be reached as early as possible. The improvements would consists of decreasing the JIT compilation time by improving the efficiency of the JIT code, or have better heuristic so the JIT can generate optimized code with the correct speculations with little runtime information.

\paragraph{Tiered architecture}
One solution, used the Webkit VM\cite{Webkit15}, is to have a tiered architecture with many tiers. In the version of Webkit in production from March 2015 to February 2016 \cite{Webkit15}, the code is:
\begin{itemize}
\item interpreted by a bytecode interpreter the first 6 executions.
\item compiled to machine code at 7th execution, with a non optimizing compiler, and executed as machine code up to 66 executions.
\item recompiled to more optimized machine code at 67th execution, with an optimizing compiler doing some but not all optimizations, up to 666 executions.
\item recompiled to heavily optimized machine code at 667th execution, with an optimizing compiler using LLVM as a backend.
\end{itemize}

At each step, the compilation time is greater but the execution time decreases. This tiered approach (4 tiers in the case of Webkit), allows to have good performance from start-up, while reaching high performance for long running code. This kind of approaches has also draw-backs: the VM development team needs to maintain and evolve four different tiers.

\paragraph{Saving metadata.}

To reach quickly peak performance, an alternative of saving optimized code is to save the runtime information. The Dart snapshot saves already the call site information in its Android snapshots. Other techniques are available.

In Strongtalk \cite{Sun06}, a high-performance Smalltalk, it is possible to save the inlining decision of the optimizing compiler in a separate file. The optimizing compiler can then reuse this file to make the right inlining decision the first time a hot spot is detected.

\paragraph{Saving machine code.}

In the Azul VM Zing \cite{Azul}, available for Java, the official web site claims that "operations teams can save accumulated optimizations from one day or set of market conditions for later reuse" thanks to the technology called \emph{Ready Now!}. In addition, the website precises that the Azul VM provides an API for the developer to help the JIT to make the right optimization decisions. As Azul is closed source, implementation details are not entirely known. 

However, word has been that the Azul VM reduces the warm-up time by saving machine code across multiple start-ups. If the application is started on another processor, then the saved machine code is simply discarded. We did not go in this direction to persist the optimization in a platform-independent way (in our architecture, starting the application on x86 instead of ARMv5 does not require the saved optimized code to be discarded), but we have a small overhead due to the bytecode to machine code translation at each start-up. In addition, we believe it's very difficult to persist correctly machine code compared to persisting bytecodes.

\paragraph{Ahead-of-time compilation.}

The last alternative is to pre-optimize the code ahead of time. This can be done by doing static analysis over the code to try to infer types. Applications for the iPhone are a good example where static analysis is used to pre-optimize the Objective-C application. The peak performance is lower than with a JIT compiler if the program uses a lot of virtual calls, as static analysis are not as precised as runtime information on highly dynamic language. 
However, if the program uses few dynamic features (for example most of the calls are not virtual) and is running on top of a high-performance language kernel like the Objective-C kernel, the result can be satisfying.

\section{Conclusion.} This chapter discusses how the runtime state is persisted across multiple start-ups, improving the performance during start-up. The next chapter validates the Sista architecture, mainly through performance evaluation in a set of benchmarks. The validation chapter also evaluates the Sista VM performance when the runtime state is persisted across multiple start-ups.

\ifx\wholebook\relax\else
    \end{document}
\fi