\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Runtime state persistance across start-ups}
\label{chap:persistance}
\minitoc

This chapter describes how the Sista VM persists the runtime state across multiple VM start-ups, including the running green threads and the optimised code. The first Section discusses how snapshots are implemented in Smalltalk, including how the code and the running green threads are persisted across start-ups. The Section then details how snapshots interact with Sista. Section \ref{sec:warmup} focuses on the main issue: the start-up performance of many VMs today is far worse than the peak performance. Several cases where the start-up performance is a problem are described.  Section \ref{sec:relWork} compares our approach to existing VMs. Few VMs attempts to persist the runtime state across multiple start-ups, but some VMs include solutions to improve start-up performance, solving the same problem of poor start-up performance.

\section{Snapshots and persistance}

Snapshots are available in multiple object-oriented languages such as Smalltalk \cite{Gold83a} and later Dart \cite{Anna13a}. As discussed in Section \ref{par:snapshot}, in our case we use Pharo which features snapshots. A snapshot is a serialized form of all the objects present at a precise moment in the runtime. Everything is an object in Pharo, including green threads or v-functions. To start-up Pharo, the virtual machine loads all the objects from a snapshot and resumes the execution based on the green thread that was active at snapshot time. This is how Pharo is normally launched.

\paragraph{Pharo development workflow.}A Pharo programmer does not modify source code in files as many other programming languages. Pharo is started, for development, with a snapshot which includes development tools, user interface elements and a source code to v-function compiler. When started, the programmer can open the development tools and write or edit the source code of a function. When done, the compiler is requested to generate a v-function from the source code and the v-function created is added in the heap. Then, the programmer may take a new snapshot, which includes the changes made. Further start-ups, on the new snapshot, features the changes made by the programmer. We note that in this paragraph we described the normal development flow of a Smalltalk programmer: this is not a workflow the programmer can do but does not normally do, this is how all programmers currently do it.

\paragraph{Application to Sista.}In the context of Sista, optimised v-functions are installed at runtime by Scorch. Those functions effectively modify the current heap of objects. Hence, when a new snapshot is taken, optimised v-functions are persisted. The next start-up of Pharo will use directly the optimised v-functions.

\paragraph{Green threads and snapshots.}To persist running green threads in a platform-independent way, to take a snapshot, the VM reifies each stack frame to a context object as explained in Section \ref{par:frameToContext}. Effectively, this means that only v-frames are persisted: n-frames are converted to v-frames to be able to be snapshot. N-frames cannot be persisted in any case as a snapshot taken on a specific processor has to be able to be started on another processor.

When the VM starts from a snapshot, all running green threads are executed using the v-function interpreter. However, once a function is called multiple times or a loop is interpreted a certain number of iteration, Cogit generates a n-function for the corresponding v-function (optimised or not), and the runtime resumes by executing n-functions.

\paragraph{Conclusion.}To conclude, programmers normally work on Pharo by modifying the current heap, for example by adding new v-functions to method dictionaries of classes, and then take a snapshot of the heap to save their code. V-functions are persisted in snapshot but n-functions are not. In Sista, the optimising JIT is the combination of Scorch, which generates and installs optimised v-functions, and Cogit, which generates and installs n-functions. Hence, optimised v-functions generated by Scorch are, without any additional work, persisted across multiple start-ups as part of the snapshot. N-functions generated by Cogit are never persisted. Most of the compilation time is currently spent in Scorch, hence, if Pharo is started using a snapshot including optimised v-functions, Pharo can reach peak performance very quickly. Green threads using optimised functions are persisted in the snapshot in the form of optimised and unoptimised v-frames. This technique allows Pharo to reach very good peak performance thanks to the optimising JIT while limited the time to reach peak performance by allowing the optimised functions to be persisted across start-ups.

\section{Warm-up time problem}
\label{sec:warmup}

The most important problem solved by persisting the runtime state across start-up is the warm-up time problem, i.e., the time wasted by the VM at each start-up to reach peak performance. Depending on use-cases, the warm-up time may not matter. The warm-up time required to reach peak performance may be negligible compared to the overall runtime of the application. However, when applications are started frequently and are short-lived, this time can matter.

We give three examples where the virtual machine start-up time matters. These examples are specific, but they give an idea where the warm-up performance is a serious problem.

\paragraph{Distributed applications.}
Modern large distributed applications run on hundreds, if not thousands, of machines such as the slaves one can rent on Amazon Web Services. Slaves are usually rented per hour, though now some contracts allow one to rent a slave for 5 minutes or even 30 seconds. If the application needs more power, it rents new slaves, if it does not need it anymore, it frees the slaves. The slaves are paid only when needed, no application users imply no cost whereas the application can scale very well.

The problem is that to reduce the cost to the minimum, the best would be to rent a slave when needed, and at the second where the slave is not used, to free it not to pay anymore for it. Doing that implies having very short lived slaves, with an order of 30 seconds life-time for example. To be worth it, the time between the slave start-up and the peak performance of the language used has to be as small as possible. A good VM for such kind of scenario should reach peak performance very fast.

\paragraph{Mobile application.}
In the case of mobile applications, the start-up performance matters because of  battery consumption. During warm-up time, the optimizing compiler recompiles frequently used code. All this compilation process requires time and energy, whereas the application is not run. In the example of the Android runtime, the implementation used JIT compilation with the Dalvik VM \cite{Born08a}, then switched to client-side ahead of time compilation (ART) to avoid that energy consumption at start-up, and is now switching back to JIT compilation because of the AOT (Ahead of Time compiler) constraints \cite{Geof15a}. These different attempts show the difficulty to build a system that requires JIT compilation for high performance but can't afford an energy consuming start-up time.

\paragraph{Web pages.}
Web pages sometimes execute just a bit of Javascript code at start-up, or use extensively Javascript in their lifetime (in this latter case, one usually talk about web application). A Javascript virtual machine has to reach peak performance as quickly as possible to perform well on web pages where only a bit of Javascript code is executed at start-up, while it has also to perform well on long running web applications.

\section{Related work}
\label{sec:relWork}

This section discusses other strategies implemented in other VMs and research projects to decrease the warm-up time.

\subsection{Preheating through snapshots}

\paragraph{Dart snapshots.}

The Dart programming languages features snapshots for fast application start-up. In Dart, the programmer can generate different kind of snapshots \cite{Anna13a}. Since that publication, the Dart team have added two new kind of snapshots, specialized for iOS and Android application deployment, which are the most similar to our snapshots.

\subparagraph{Android.} A Dart snapshot for an Android application is a complete representation of the application code and the heap once the application code has been loaded but before the execution of the application. The Android snapshots are taken after a warm-up phase to be able to record call site caches in the snapshot. The call site cache is a regular heap object accessed from machine code, and its presence in the snapshot allows to persist type feedback and call site frequency.

In this case, the code is loaded pre-optimized with inline caches prefilled values. However, optimized functions are not loaded as our architecture allows to do. Only unoptimized code with precomputed runtime information is loaded.

\subparagraph{iOS.} For iOS, the Dart snapshot is slightly different as iOS does not allow JIT compilers. All reachable functions from the iOS application are compiled ahead of time, using only the features of the Dart optimizing compiler that don't require dynamic deoptimization. A shared library is generated, including all the instructions, and a snapshot that includes all the classes, functions, literal pools, call site caches, etc.

This second case is difficult to compare to our architecture: iOS forbids machine code generation, which is currently required by our architecture. A good application of our architecture to iOS is future work.

\paragraph{Cloneable VMs.}

ADD
Cloneable JVM
\cite{Kawa07a}

\subsection{Fast warm-up}

An alternative to snapshots is to improve the JIT compiler so the peak performance can be reached as early as possible. The improvements would consists of decreasing the JIT compilation time by improving the efficiency of the JIT code, or have better heuristic so the JIT can generate optimized code with the correct speculations with little runtime information.

\paragraph{Tiered architecture}
One solution, used the Webkit VM\cite{Webkit15}, is to have a tiered architecture with many tiers. In the version of Webkit in production from March 2015 to February 2016 \cite{Webkit15}, the code is:
\begin{itemize}
\item interpreted by a bytecode interpreter the first 6 executions.
\item compiled to machine code at 7th execution, with a non optimizing compiler, and executed as machine code up to 66 executions.
\item recompiled to more optimized machine code at 67th execution, with an optimizing compiler doing some but not all optimizations, up to 666 executions.
\item recompiled to heavily optimized machine code at 667th execution, with an optimizing compiler using LLVM as a backend.
\end{itemize}

At each step, the compilation time is greater but the execution time decreases. This tiered approach (4 tiers in the case of Webkit), allows to have good performance from start-up, while reaching high performance for long running code. This kind of approaches has also draw-backs: the VM development team needs to maintain and evolve four different tiers.

\paragraph{Saving runtime information.}

To reach quickly peak performance, an alternative of saving optimized code is to save the runtime information. The Dart snapshot saves already the call site information in its Android snapshots. Other techniques are available.

In Strongtalk \cite{Sun06}, a high-performance Smalltalk, it is possible to save the inlining decision of the optimizing compiler in a separate file. The optimizing compiler can then reuse this file to make the right inlining decision the first time a hot spot is detected.

ADD
profiling information
Importing VM perf using cross-run prof repository \cite{Arno05c}

\paragraph{Saving machine code.}

In the Azul VM Zing \cite{Azul}, available for Java, the official web site claims that "operations teams can save accumulated optimizations from one day or set of market conditions for later reuse" thanks to the technology called \emph{Ready Now!}. In addition, the website precises that the Azul VM provides an API for the developer to help the JIT to make the right optimization decisions. As Azul is closed source, implementation details are not entirely known. 

However, word has been that the Azul VM reduces the warm-up time by saving machine code across multiple start-ups. If the application is started on another processor, then the saved machine code is simply discarded. We did not go in this direction to persist the optimization in a platform-independent way (in our architecture, starting the application on x86 instead of ARMv5 does not require the saved optimized code to be discarded), but we have a small overhead due to the bytecode to machine code translation at each start-up. In addition, we believe it's very difficult to persist correctly machine code compared to persisting bytecodes.

ADD 
JRockit\cite{JRockit}
ADD
Persistent code caching: exploiting code reuse across executions and applications
\cite{Redd07a}

\paragraph{Ahead-of-time analysis.}

ADD
Static ahead of time analysis (Using annotations to reduce dynamic optimisation time) \cite{Krin01a}

\paragraph{Ahead-of-time compilation.}

The last alternative is to pre-optimize the code ahead of time. This can be done by doing static analysis over the code to try to infer types. Applications for the iPhone are a good example where static analysis is used to pre-optimize the Objective-C application. The peak performance is lower than with a JIT compiler if the program uses a lot of virtual calls, as static analysis are not as precised as runtime information on highly dynamic language. 
However, if the program uses few dynamic features (for example most of the calls are not virtual) and is running on top of a high-performance language kernel like the Objective-C kernel, the result can be satisfying.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Conclusion} This chapter discusses how the runtime state is persisted across multiple start-ups, improving the performance during start-up. The next chapter validates the Sista architecture, mainly through performance evaluation in a set of benchmarks. The validation chapter also evaluates the Sista VM performance when the runtime state is persisted across multiple start-ups.

\ifx\wholebook\relax\else
    \end{document}
\fi