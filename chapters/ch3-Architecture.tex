\ifx\wholebook\relax\else
% --------------------------------------------
% Lulu:
    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}	
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{The Sista Architecture}
\label{chap:archTheory}
\minitoc

Paper: sista arch
small intro. 

\section{General approach}

Our architecture, \emph{Sista} (Speculative Inlining SmallTalk Architecture) works as follows. Our optimizing compiler, after doing language-specific optimizations such as speculative inlining or array bounds check elimination, generates an optimized version of the function using a bytecode representation and does not directly generate machine code. This optimized version has access to an extended bytecode set to encode unchecked operations such as array access without bounds checks similar to the work of B\'era et al.\cite{Bera14a}. Optimized bytecoded functions are reifed as objects the same as normal bytecoded functions, hence they can be saved without any additional work as part of the snapshot. Then, the VM uses the baseline Just-in-Time compiler (JIT) as a back-end to generate machine code from the optimized bytecoded function. The optimized functions are marked, so the VM can decide to handle differently optimized bytecoded functions. 

Dynamic deoptimization is also split in two steps. Firstly, Sista asks the baseline JIT to reify the stack frame from machine state to bytecode interpreter state of the optimized bytecoded function, mapping correctly the register to stack entries and converting object representations from unboxed versions to boxed versions, as it would do for any unoptimized version of the function. Secondly, a separate deoptimizer maps the bytecode interpreter state of the optimized bytecoded function to multiple stack frame corresponding to the bytecode interpreter state of multiple unoptimized functions, rematerializing objects from constants and stack values.

\subsection{Overview}



%split architecture, what is where. 

%in theory, optimised bc method could be interpreted. In practice it does not really make sense. But in some cases it does.

\subsection {Optimisation process}

The runtime optimization process has overall the same behavior than other virtual machines: in the unoptimized machine code, a portion of code frequently used is detected, recompiled on-the-fly using information relative to the previous runs. Then the virtual machine uses the optimized portion of code. The difference lies in the generation of an optimized bytecoded function in the middle. The full runtime optimization process is as follows:
\begin{enumerate}
\item \emph{Hot spot detection:} When the baseline JIT compiler  generates unoptimized version of functions in machine code, it  inserts counters on specific locations detailled later in the paper. Each time the execution flow reaches a counter, it increments it by one, and when the counter reaches a threshold, the portion of code is detected as frequently used, \ie as being a hot spot.
\item \emph{Choosing what to optimize:} Once a hot spot is detected, the VM launches the runtime optimizer. The optimizer tries to find what function is the best to optimize. It walks a few frames in the stack from the active stack frame and based on simple heuristic (mostly, it tries to find a stack frame where as many closure activation as possible available on the current stack can be inlined), it determines a function to optimize.
\item \emph{Decompilation:} The optimizer then decompiles the selected function to an IR (intermediate representation) to start the optimization process. During decompilation, the virtual machine extracts runtime information from the machine code version of the function if available. The decompiler annotates all the virtual calls and branches in the IR with type and branch information.
\item \emph{Overall optimization:} The optimizer then performs several optimization passes.  We detail that part in \secref{Optimizer}.
\item \emph{Generating the optimized function:} Once the function is optimized, the optimizer outputs an optimized bytecoded function, that is encoded thanks to an extended bytecode set. A specific object is kept in the literal frame of the optimized method to remember all the deoptimization metadata needed for dynamic deoptimization. This optimized bytecoded function looks like any unoptimized bytecoded function, so it can be saved as part of snapshots.
\item \emph{Installation:} The optimized function is installed, either in the method dictionary of a class if this is a method, or in a method if it's a closure.
\item \emph{Dependency management:} All the dependencies of the optimized functions are recorded. This is important as if the programmer installs new methods or changes the superclass hierarchy while the program is the running, the dependency manager knows which optimized functions needs to be discarded.
\end{enumerate}

\subsection {Deoptimisation process}

The dynamic deoptimization process, again, is very similar to other virtual machines \cite{Fin03a, Holz92a}. The main difference is that it is split in two parts: firstly the baseline JIT maps  machine state to a state as if the bytecode interpreter would execute the function, second the deoptimizer maps the interpreter state to the deoptimized interpreter frames.

During dynamic deoptimization, we deal only with the recovery of the stack from its optimized state using optimized functions to the  unoptimized state using unoptimized functions. The unoptimized code itself is always present, as the bytecode version of the  unoptimized function is quite compact. As far as we know, modern VM such as V8~\cite{V8} always keep the machine code representation of unoptimized functions, which is less compact than the bytecode version, so we believe keeping the unoptimized bytecode function is not a problem in term of memory footprint.

\begin{enumerate}
\item \emph{Deoptimization trigger:} Deoptimization can happen in two main cases. First, a guard inserted during the optimization phases of the compiler has failed. Second, the language requests the stack to be deoptimized, typically for debugging.
\item \emph{JIT map:} The first step, done by the baseline JIT compiler is to map the machine code state of the stack frame to the bytecode interpreter state, as it would do for an unoptimized method. This mapping is a one-to-one mapping: a machine code stack frame maps to a single interpreter stack frame. In this step, the baseline JIT maps the machine code program counter to the bytecode program counter, boxes unboxed values present and spills values in registers on stack.
\item \emph{Deoptimizer map:} The JIT then requests the deoptimizer to map the stack frame of the optimized bytecoded function to multiple stack frames of unoptimized functions. In this step, it can also rematerialize objects from values on stack and constants, whose allocations have been removed by the optimizer. The stack with all the unoptimized functions at the correct bytecode interpreter state is recovered.
\item \emph{Stack edition:}
The deoptimizer edits the bottom of the stack to use the deoptimized stack frames instead of the optimized ones, and resumes execution in the unoptimized stack.
\end{enumerate}

\subsection{Recursion problem}

the recursive problem

Doc: blog post Sista chronicles 1, an update on the sista, talks at ESUG and Smalltalks + arch paper

\section{Virtual Machine interface extension}

\subsection{Runtime information access}

new primitive operation

To extract information from the machine code version of a method, we added a new primitive operation \emph{sendAndBranchData}. This operation can be performed only on compiled methods. If the method has currently a machine code version, the primitive answers the types met at each inline cache and the values of the counters at each branch. This information can be then used by the runtime optimizer to type variables and to detect the usage of each basic block. The primitive answers the runtime information relative to the compiled method and all the closures defined in the compiled method.

\subsection{New call-backs}

As in our implementation the runtime optimizer and deoptimizer are implemented in Smalltalk run and not in the virtual machine itself, we needed to introduce callbacks activated by the virtual machine to activate the optimizer and the deoptimizer. 

These callbacks use the reification of stack frame available for the debugger to inform the language which frame had its method detected as a hot spot and which frame has to be deoptimized.

\paragraph{Extending the bytecode set.}

(our work not blind)
To support unsafe operations, the bytecode set needed to be extended. In our previous work, we describe the extended bytecode set used \cite{Bera14a}. The extended bytecode set design relies on the assumption that only a small number of new bytecode instructions are needed for the baseline JIT to produce efficient machine code. Three main kind of instructions were added into the bytecode set:
\begin{itemize}
\item \textbf{Guards}: guards are used to ensure a specific object has a given type, else they trigger dynamic deoptimization.
\item \textbf{Object unchecked accesses}: normally variable-sized objects such as arrays or byte arrays require type and bounds checks to allow a program to access their fields. Unchecked access directly reads the field of an object without any checks.
\item \textbf{Unchecked arithmetics}: Arithmetic operations needs to check for the operand types to know what arithmetic operation to call (on integers, double, etc.). Unchecked operations are typed and do not need these check. In addition, unchecked operations do not do an overflow check and are converted efficiently to machine code conditional branches if followed by a conditional jump.
\end{itemize}

We are considering adding other unchecked operations in the future, for example those related to object creation or stores without the garbage collector write barrier.

The extended bytecode set was designed to use a baseline JIT as a back-end and not an interpreter. It is important to note as we considered that as there are a few optimized methods, we can encode unchecked operations in multiple bytecodes to save other bytecodes for more frequent instructions and lower the overall memory footprint of the runtime. In the case of a bytecode set targetting the bytecode interpreter, we would need to encode unchecked instruction in the smallest number of byte possible as these intructions are on performance critical code and bytecode fetching has a noticeable impact on a bytecode interpreter performance.

%Currently, the extended bytecode set is generated by the runtime optimizing compiler only. If someone adds simple syntaxic extensions to Smalltalk, it is possible to extend slightly the regular bytecode compiler to use unchecked instructions. We did not go in that direction as the unchecked instructions are unsafe, leading potentially in virtual machine crashes if they are used incorrectly, which doesn't work with the protection expected by high-level language programmers. \md{i would move this to the discussion}


Need bytecode set paper in addition + description of new instructions. The precise list come in implementation section.

\section{Advantages}

\subsection{Persistance through snapshots}

optimization and processes

\subsection{Runtime information available}

is it good ? May change discussion

\subsection{High-level language and engineering cost}

start with efficiency ratio from known books.

why it is not really high level language.

they conclude that we believe (but maybe not) that we are more efficient. Maybe other solution better like DSL

\subsection{Modification at runtime}

disable, change, re-enable, etc.
Or is it the same as engineering cost.

\section{Issues}

\subsection{Towards highest performance}
-- opt info lost, waste time going to bc, etc.

\subsection{High-level virtual machine and programmer awareness}
-- problem of having it in the language: hard to keep the programmer unaware

\subsection{New optimizations}
-- new opt may be difficult if need to extend too much the interface, it has to be small
Up until now it's fine.

\ifx\wholebook\relax\else
    \end{document}
\fi