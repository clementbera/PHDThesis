\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Sista Architecture}
\label{chap:architecture}
\minitoc

\section{General approach}

nutshell

existing VM, works with interpreter baseline JIt. Add only hot spot detection. Once detected, in language opt and method installed. Then the VM keeps running and now can use new method installed. + extended bytecode set.

Our architecture, \emph{Sista} (Speculative Inlining SmallTalk Architecture) works as follows. Our optimizing compiler, after doing language-specific optimizations such as speculative inlining or array bounds check elimination, generates an optimized version of the function using a bytecode representation and does not directly generate machine code. This optimized version has access to an extended bytecode set to encode unchecked operations such as array access without bounds checks similar to the work of B\'era et al.\cite{Bera14a}. Optimized bytecoded functions are reifed as objects the same as normal bytecoded functions, hence they can be saved without any additional work as part of the snapshot. Then, the VM uses the baseline Just-in-Time compiler (JIT) as a back-end to generate machine code from the optimized bytecoded function. The optimized functions are marked, so the VM can decide to handle differently optimized bytecoded functions. 

Dynamic deoptimization is also split in two steps. Firstly, Sista asks the baseline JIT to reify the stack frame from machine state to bytecode interpreter state of the optimized bytecoded function, mapping correctly the register to stack entries and converting object representations from unboxed versions to boxed versions, as it would do for any unoptimized version of the function. Secondly, a separate deoptimizer maps the bytecode interpreter state of the optimized bytecoded function to multiple stack frame corresponding to the bytecode interpreter state of multiple unoptimized functions, rematerializing objects from constants and stack values.

\subsection{Overview}



%split architecture, what is where. 

%in theory, optimised bc method could be interpreted. In practice it does not really make sense. But in some cases it does.

%NEVER EVER USE THE WORD PROCESS please. Is a processus, a green thread a n thread ?

\subsection {Optimisation process}

The runtime optimization process has overall the same behavior than other virtual machines: in the unoptimized machine code, a portion of code frequently used is detected, recompiled on-the-fly using information relative to the previous runs. Then the virtual machine uses the optimized portion of code. The difference lies in the generation of an optimized bytecoded function in the middle. The full runtime optimization process is as follows:
\begin{enumerate}
\item \emph{Hot spot detection:} When the baseline JIT compiler  generates unoptimized version of functions in machine code, it  inserts counters on specific locations detailled later in the paper. Each time the execution flow reaches a counter, it increments it by one, and when the counter reaches a threshold, the portion of code is detected as frequently used, \ie as being a hot spot.
\item \emph{Choosing what to optimize:} Once a hot spot is detected, the VM launches the runtime optimizer. The optimizer tries to find what function is the best to optimize. It walks a few frames in the stack from the active stack frame and based on simple heuristic (mostly, it tries to find a stack frame where as many closure activation as possible available on the current stack can be inlined), it determines a function to optimize.
\item \emph{Decompilation:} The optimizer then decompiles the selected function to an IR (intermediate representation) to start the optimization process. During decompilation, the virtual machine extracts runtime information from the machine code version of the function if available. The decompiler annotates all the virtual calls and branches in the IR with type and branch information.
\item \emph{Overall optimization:} The optimizer then performs several optimization passes.  We detail that part in \secref{Optimizer}.
\item \emph{Generating the optimized function:} Once the function is optimized, the optimizer outputs an optimized bytecoded function, that is encoded thanks to an extended bytecode set. A specific object is kept in the literal frame of the optimized method to remember all the deoptimization metadata needed for dynamic deoptimization. This optimized bytecoded function looks like any unoptimized bytecoded function, so it can be saved as part of snapshots.
\item \emph{Installation:} The optimized function is installed, either in the method dictionary of a class if this is a method, or in a method if it's a closure.
\item \emph{Dependency management:} All the dependencies of the optimized functions are recorded. This is important as if the programmer installs new methods or changes the superclass hierarchy while the program is the running, the dependency manager knows which optimized functions needs to be discarded.
\end{enumerate}

\subsection {Deoptimisation process}

The dynamic deoptimization process, again, is very similar to other virtual machines \cite{Fin03a, Holz92a}. The main difference is that it is split in two parts: firstly the baseline JIT maps  machine state to a state as if the bytecode interpreter would execute the function, second the deoptimizer maps the interpreter state to the deoptimized interpreter frames.

During dynamic deoptimization, we deal only with the recovery of the stack from its optimized state using optimized functions to the  unoptimized state using unoptimized functions. The unoptimized code itself is always present, as the bytecode version of the  unoptimized function is quite compact. As far as we know, modern VM such as V8~\cite{V8} always keep the machine code representation of unoptimized functions, which is less compact than the bytecode version, so we believe keeping the unoptimized bytecode function is not a problem in term of memory footprint.

\begin{enumerate}
\item \emph{Deoptimization trigger:} Deoptimization can happen in two main cases. First, a guard inserted during the optimization phases of the compiler has failed. Second, the language requests the stack to be deoptimized, typically for debugging.
\item \emph{JIT map:} The first step, done by the baseline JIT compiler is to map the machine code state of the stack frame to the bytecode interpreter state, as it would do for an unoptimized method. This mapping is a one-to-one mapping: a machine code stack frame maps to a single interpreter stack frame. In this step, the baseline JIT maps the machine code program counter to the bytecode program counter, boxes unboxed values present and spills values in registers on stack.
\item \emph{Deoptimizer map:} The JIT then requests the deoptimizer to map the stack frame of the optimized bytecoded function to multiple stack frames of unoptimized functions. In this step, it can also rematerialize objects from values on stack and constants, whose allocations have been removed by the optimizer. The stack with all the unoptimized functions at the correct bytecode interpreter state is recovered.
\item \emph{Stack edition:}
The deoptimizer edits the bottom of the stack to use the deoptimized stack frames instead of the optimized ones, and resumes execution in the unoptimized stack.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Paper: sista arch + bytecode set paper + read-only paper

small intro. 

\section{Virtual machine interface extension}

\subsection{Extending bytecode set.}

New bytecode set needs to be discussed (not only extensions).

To support unsafe operations, the bytecode set needed to be extended. B\'era and Miranda describes the extended bytecode set used \cite{Bera14a}. The extended bytecode set design relies on the assumption that only a small number of new bytecode instructions are needed for the baseline JIT to produce efficient machine code. Three main kind of instructions were added into the bytecode set:
\begin{itemize}
\item \textbf{Guards}: guards are used to ensure a specific object has a given type, else they trigger dynamic deoptimization.
\item \textbf{Object unchecked accesses}: normally variable-sized objects such as arrays or byte arrays require type and bounds checks to allow a program to access their fields. Unchecked access directly reads the field of an object without any checks.
\item \textbf{Unchecked arithmetics}: Arithmetic operations needs to check for the operand types to know what arithmetic operation to call (on integers, double, etc.). Unchecked operations are typed and do not need these check. In addition, unchecked operations do not do an overflow check and are converted efficiently to machine code conditional branches if followed by a conditional jump.
\end{itemize}

We are considering adding other unchecked operations in the future. For example, we believe instructions related to object creation or stores without the garbage collector write barrier could make sense.

As the optimized methods are represented as bytecodes, one could consider executing them using the bytecode interpreter. This is indeed possible and we explain later in section \ref{interpreter} that in very uncommon cases it can happen in our runtime. However, improving the performance to speed-up the bytecode interpreter or to speed-up the machine code generated using the baseline JIT as a back-end are two different tasks that may conflict with one another. We designed the bytecode set so the machine code generated by the baseline JIT as a back-end is as efficient as possible, not really considering the speed of the interpretation of those methods as they are almost never interpreted in our runtime.

\subsection{New primitive operation.}

To extract information from the machine code version of a method, we added a new primitive operation \emph{sendAndBranchData}. This operation can be performed only on compiled methods. If the method has currently a machine code version, the primitive answers the types met at each inline cache and the values of the counters at each branch. This information can be then used by the runtime optimizer to type variables and to detect the usage of each basic block. The primitive answers the runtime information relative to the compiled method and all the closures defined in the compiled method.

\subsection{New callbacks.}

As in our implementation the runtime optimizer and deoptimizer are implemented in Smalltalk and not in the virtual machine itself, we needed to introduce callbacks activated by the virtual machine to activate the optimizer and the deoptimizer. 

These callbacks use the reification of stack frames available for the debugger to inform the language which frame had its method detected as a hot spot and which frame has to be deoptimized.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Virtual machine evolution}

\subsection{Memory management}

The first problem met was related to the complexity of the existing memory manager (called V3) in the virtual machine. The memory manager is responsible for the representation of object in memory, as well as object allocation and garbage collection. The main issue lied with memory representation of objects: V3 was designed for an interpreter-based VM running heaps of several Mbs at most and it did not allow the JIT to generate efficient machine code nor scaling to large heap of objects.

\paragraph{Efficient JIT compilation.} Let's take a simple example. With V3, an object can access to its class in three different ways. Firstly, immediate objects\footnote{An immediate object is an object directly encoded in the object pointer, typically tagged integers.} access their classes through a special table, a list of 16 common classes have their instances access their class through another special table, while the rest of the objects have a pointer to their class in their header. In machine code, efficient class access becomes critical for efficient type tests (typically deoptimisation guards) and for inline caches, used in the baseline JIT to collect virtual calls receiver types. With V3, each class access in machine code requires to compile the three different paths. 

\paragraph{Heap scaling.} The second main issue was related to heap scaling. Let's take another simple example. V3 memory manager requests 1 Gb of memory to the operating system at start-up to keep all the heap contiguous, instead of segmenting its heap in smaller portions. This is very problematic in multiple OS such as Windows, where the OS may refuse to provide such a large amount of memory. 

(CITE Spur paper)

Spur solutions.

pinned objects

Doc: spur paper

\subsection{Register allocation}

As

Linear scan and reg alloc, plus branches
Doc: TODO

\subsection{Profiling counters}

To detect frequently used method, counters were introduced in code compiled by the baseline JIT, triggering a call-back when the counter reaches a specific threshold. Profiling counters add some overhead. As the baseline JIT is used both to compile non optimised and optimised code, compilation became conditional to introduce counters only in non optimised code. For this purpose, each bytecoded function is marked as being optimized or not thanks to a bit in the header. If the function has not yet been optimized, the baseline JIT generates counters in the machine code that are incremented each time they are reached by the flow of execution. Once a counter reaches a threshold, a call-back requests the runtime optimizer to generate optimized code.

Based on \cite{Arn02}, we added counters by extending the way the baseline JIT generates conditional jumps to add counters just before and just after the branch. In several other VMs, the counters are added at the beginning of each function. The technique we used allowed us to reduce the counter overhead as branches are 6 times less frequent that virtual calls in the Smalltalk code we observe on production application. In addition, the counters provide information about basic block usage. Every finite loop requires a branch to stop the loop iteration and most recursive code requires a branch to stop the recursion, so the main cases for which we wanted to detect hot spots for are covered.

TODO why pinned array and not normal object like symbol literals ? => because direct access to object slot 
Using an immovable object results in 
- not having to add new method map entries
- not having to add new map/reference scanning machinery to update derived pointers (pointers into objects)
- the most direct access to the counter and hence the simplest possible instruction sequence (remember the AbstractInstrcution set is somewhat RISC like)
ENd TODO

To make the processor instruction-cache happy, counter values could not be next the machine code of each function. Indeed, doing so reuire the processor to flush the instruction-cache each time the counter is modified, slowing down massively code execution. To keep it efficient, at machine code compilation time a function is associated with a pinned array holding the counter values. The pinned property of the array allows the machine code to refer directly to the counter addresses, without having to do any garbage collection extension.

\subsection{Runtime information}

A primitive operation was added in the language to extract the \emph{send and branch data} of the associated machine code of each function. It works as follows:
\begin{itemize}
\item If the function is present in the machine code zone, then it answers the \emph{send data}, which means the types met at each virtual call site based on the state of the inline caches, and the \emph{branch data}, which means the number of times each branch was taken at each branch.
\item If the function is not present in the machine code zone, then this primitive fails.
\end{itemize}

The data is answered as an array of array, with each entry being composed of:
\begin{itemize}
\item the bytecode program counter of the instruction.
\item either the types met and the function founds for virtual calls or the number of time each branch was taken for branches.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Language evolution}

\subsection{Read-only objects}

One of the main problem encountered while trying to improve the performance of the Pharo runtime was literal mutability. In most programming languages, if the program executes a simple double addition between two double constants the compiler can compute at compile-time the result. In Pharo, as literals are mutable, one of the double constant may be accessed through reflective APIs and mutated into another boxed double, invalidating the compile-time result. 

To solve the problem, I introduced a feature, called read-only objects. With this feature, a program can mark any object as read-only. Such read-only objects cannot be mutated unless the program explicitly revert them to a writable state. This feature was introduced with little to no overhead on the production VM (CITE). 

Literals can now all be read-only objects, and any attempt to mutate a read-only literal is caught and, if the literal was used for compile-time computation, dependant optimised code is discarded if the mutation happens. This way, traditional compiler optimisation can be applied to the Pharo runtime.

\subsection{Closure implementation}

The second major issue encountered was related to the closure implementation. 

FullBlock
Doc: ? well... the FullBlock talk

\subsection{Runtime optimisation and deoptimisation}

Explain simple behavior.

Maybe it makes sense to say Context are the same as objects.

deoptimizer 
Doc: my blog post + sista arch


\ifx\wholebook\relax\else
    \end{document}
\fi