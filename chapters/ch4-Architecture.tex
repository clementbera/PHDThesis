\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Sista Architecture}
\label{chap:architecture}
\minitoc

%Generic intro + overview intro
The overall thesis focuses on the design and implementation of an optimising JIT for Pharo, written in Pharo itself, running in the same runtime than the optimised application on top of the existing runtime environment. This chapter explains the overall designed and implemented architecture, called Sista. The first section gives an overview of the architecture. Section \ref{sec:functionOptimisation} details how functions are optimised. Section \ref{sec:functionDeoptimisation} focuses on the deoptimisation of optimised stack frames when an assumption taken at optimisation time is not valid at runtime. Section \ref{sec:relatedWorkArch} compares Sista against related work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview}

This section starts by describing briefly the existing runtime and evolutions required to introduce an optimising JIT. Section \ref{sec:splitDesign} explains the overall design. Lastly, Section \ref{sec:further} briefly describes how functions are optimised and deoptimised. 

\subsection{Missing components}

The existing Pharo runtime relies on a v-function interpreter and a baseline JIT named Cogit to execute functions. Cogit is able to compile a single v-function to a single n-function with a limited number of optimisations such as inline caches~\cite{Deut84a,Holz91a}. Cogit does not perform optimisations requiring speculations based on runtime information. To load new code in Pharo, it is possible to install new v-functions at runtime. In this case, the program requests the VM to flush caches matching the new v-functions installed.

Compared to the function-based architecture described in the previous chapter, the Pharo runtime is missing entirely an optimising JIT performing speculative optimisations based on runtime information. We note two main missing features:
\begin{itemize}
	\item \textbf{Hot spot detection and n-function introspection:} The first missing feature is that the baseline JIT is not able to detect Hotspots to trigger the optimising JIT nor to introspect the n-function it generates to provide runtime information. 
	\item \textbf{Optimising JIT:} The second missing feature is the optimising JIT itelf, which should be able to generate an optimised n-function from v-functions and the corresponding runtime information. The optimising JIT needs to include specific components in addition to the optimisation pipeline. A deoptimiser is needed to resume execution with unoptimised code when a speculation made at optimisation time is incorrect at runtime. A dependency manager is required to discard dependant optimised functions when new code is loaded.
\end{itemize}

\subsection{Split design: Two parts for the optimising JIT}
\label{sec:splitDesign}

The architecture requires both baseline JIT extensions and the addition of the optimising JIT.

\paragraph{Baseline JIT extensions.}
The existing baseline JIT, Cogit, had to be extended to detect hot spots and to provide runtime information to direct the optimising JIT decisions. 

To detect hot spots, Cogit was extended to be able to generate profiling counters in generated n-functions. When a profiling counter reaches a specific threshold, a specific routine is triggered and may activate an optimising JIT. More details on the profiling counters are present in Section \ref{sec:hotSpot}.

Cogit was already able to introspect the n-function it generates for multiple purposes, such as debugging, inline cache relinking or literals garbage collection. The introspection logic was extended with a new primitive method, which answers for a given function both the values present in inline caches and the values of profiling counters.

\paragraph{Optimising JIT.}
The optimising JIT is designed in two different parts as shown in Figure \ref{fig:OptArchitecture}. The high-level part is a unoptimised v-functions to optimised v-function compiler called \emph{Scorch}. The second part is a v-function to a n-function compiler and an extended version of Cogit, the baseline JIT, is used. The compilation of unoptimised v-functions to an optimised n-function through Scorch followed by Cogit forms an optimising JIT.

Scorch is written in Pharo and runs in the same runtime as the optimised application in a metacircular style. Scorch deals with Smalltalk-specific optimisations. Hence, any work performed on Scorch is done in Pharo dealing with Smalltalk-specific optimisations. Such work can be performed with little knowledge on low-level or machine-specific details. The optimised v-functions generated by Scorch may use unsafe instructions in addition to the unoptimised v-function instructions. Unsafe instructions are faster to execute but requires the compiler to guarantee specific invariants. For example, an unsafe array access is faster to execute than a normal array access as it does not perform bound checks, but Scorch needs to guarantee that the array access is always in-bounds.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\linewidth]{OptArchitecture}
        \caption{Scorch critical and background modes}
        \label{fig:OptArchitecture}
    \end{center}
\end{figure}

For the low-level part, the existing baseline JIT Cogit is reused. Cogit may perform machine-specific optimisations. To be used as a back-end for the optimised v-functions, Cogit was extended to support new unsafe instructions, including a specific VM call-back to trigger deoptimisation when an assumption speculated at optimisation time is invalid at runtime.

\subsection{Optimisation and deoptimisation}
\label{sec:further}

%opt intro
Cogit was extended to detect hot spots through profiling counters in unoptimised n-functions. When a hot spot is detected, Cogit immediately calls Scorch in Pharo. Scorch then looks for the best v-function to optimise based on the current stack, optimises it and installs the optimised version. To perform optimisations, Scorch asks Cogit to introspect specific n-functions to extract type information and basic block usage from previous runs. Once installed, the VM can execute the optimised v-function at the next call to the function. 

As the VM runtime uses both an interpreter and Cogit, the optimised v-function may conceptually be interpreted or compiled by Cogit and then executed as an optimised n-function. In practice, new heuristics were introduced for optimised v-functions to execute them as optimised n-functions from the first run. The details of the function optimisation logic is written in Section \ref{sec:functionOptimisation}.

%deopt intro
Due to speculative optimisations, optimised v-functions may contain guards to ensure optimisation-time assumptions are valid at runtime. When a guard fails, the execution stack needs to be deoptimised to resume execution with unoptimised code. When an optimised n-frame needs to be deoptimised, Cogit maps the optimised n-frame to a single optimised v-frame. Cogit then provides the optimised v-frame to Scorch, which maps the optimised v-frame to multiple unoptimised v-frames. Scorch may discard the optimised v-function if guards are failing too often in it. The execution can then resume using unoptimised v-functions. The deoptimisation logic briefly described here is explained in detail in Section \ref{sec:functionDeoptimisation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Function optimisation}
\label{sec:functionOptimisation}

%Intro
Cogit was extended to detect hot spots based on profiling counters. When a hot spot is detected, Cogit triggers a call-back to request Scorch to optimise a v-function-based on the current stack. As Pharo is currently single-threaded, the application green thread has to be interrupted to optimise a function. The overall design is then the following: after interrupting the application, Scorch finds a v-function to optimise based on the current stack, optimises it, installs the optimised version, and resumes the application. The installed optimised v-function will be executed at the next call of the function.

\subsection{Optimiser critical and background modes}
\label{sec:optModes}

%TimeBeforePostPoning: problem so it's required
Scorch optimiser may however require a significant amount of time to optimise a v-function. Optimising a v-function can take a long time in slow machines or when a pathological function~\footnote{Many compiler algorithms have a good average complexity based on heurisitics but poor worst complexity. A pathological function is a function not matching any heuristic leading to long optimisation time.} is optimised. This can lead to the interruption of the application for an amount time long enough to be noticed by the user. To experiment with Sista, we worked with the development environment of Pharo (which is written in Pharo). In the case of a user-interface application, it is \emph{very} annoying to see the application interrupted during half a second or more when multiple v-functions long to optimise are optimised in a row. The user interface feels slow, lagging and unresponsive even though the overall code takes less time to execute.

%First solution: timeBeforePostponing, with constraint for long methods
To avoid the problem, we limited the optimisation time to a fixed small time period, configured from the language. For user interface application, we limit it to 40 ms. The limitation is enforced by a high-priority green thread, set to stop the optimiser after the given time period. As the current user interface is refreshing at 50Hz, the optimiser, in the worst case, forces the system not to refresh the screen twice. In practice, most v-functions are optimised in less than 40ms. However, some v-functions are still too long to optimise, so an alternative solution is required to optimise them.
%Didn't use frame before not to confuse graphic frames with stack frames.

%idle and postpone
Upon profiling the development tools, as one would expect, we noticed that the application spends a significant amount of time idle\footnote{An application in idle means it has nothing to do, it is typically waiting for an event to do anything.}. We show for example in Figure \ref{fig:ApplicationIdle} that the application is successfully executing code, then idle, then executing code again, etc. In this case, each time an event happen (key stroke, mouse click, etc.), some code is executed, but when no event happens, for example when the developer is reading code, the application is in idle.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.95\linewidth]{ApplicationIdle}
        \caption{User interface application idle times}
        \label{fig:ApplicationIdle}
    \end{center}
\end{figure}

%background thread
Based on the profiling result, we introduced a background green thread responsible for optimising the functions too long to optimise in the limited time period allowed. This way, when the application would normally become idle, it starts by optimising such functions and becomes idle when no functions to optimise are remaining. As the background green thread is running at low priority, if the application restarts while an optimisation is being performed, the application green thread preempts the optimisation green thread and no pauses are seen by the user.

%Critical vs background
The optimiser can therefore be run in two modes. When a hot spot is detected, the optimiser is started in \emph{critical mode}. It has a limited time period to optimise a function-based on the current stack. If the optimisation takes too long, the function to optimise is added to the background compilation queue. When the application becomes idle, if the background compilation queue is not empty, the optimiser is started in \emph{background mode}. In background mode, the optimiser is run in a low-priority green thread and is preempted by any application green thread. When the optimiser has optimised all the functions in the compilation queue, it stops and the application becomes idle. Scorch optimiser critical and background modes are represented on Figure \ref{fig:ScorchModes}.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.95\linewidth]{ScorchModes}
        \caption{Scorch critical and background modes}
        \label{fig:ScorchModes}
    \end{center}
\end{figure}

%Merged with previous paragraph.  Fig explanation, critical vs background mode
%As shown on Figure \ref{fig:ScorchModes}, when the application is running, it can be interrupted for a small time window at worst. When the application normally becomes idle, the background green thread starts optimising the functions long to optimise. When no more functions are queued for optimisations, the application becomes idle. In any case, after a while, all critical portions of code have been optimised and the application is usually not interrupted any more while the background green thread has usually no more v-functions to optimise.

\paragraph{Conclusion.}Scorch optimiser can be run in two modes. In critical mode, it interrupts the application green thread and has a limited time period to optimise a function. If the time period is not enough, the function's optimisation is postponed to the background mode. In background mode, Scorch optimises code only when the application is idle but has no time limit. This design allows all the application's code to be optimised in a single-threaded environment without the system loosing too much responsiveness.

\subsection{Hot spot management}
\label{sec:hotSpot}

%generic profiling counters
Cogit was extended to be able to generate n-functions with profiling counters. Profiling counters allow one to detect hot spots and provide information about basic block usage at the cost of a small overhead, detailed in the validation chapter of the thesis. Because of the overhead, Cogit was extended to support conditional compilation. Based on a specific bit in the v-function's header, Cogit compiles the v-function with or without profiling counters. Typically, unoptimised v-functions, produced by the source code to v-function bytecode compiler~\cite{Bera13a}, are by default compiled to n-functions with profiling counters, while optimised v-functions are compiled without profiling counters. Profiling counters are generated so that the counter is increased by one when the execution flow reaches it and a specific hot spot detection routine is called when the counter reaches a threshold.

%SHOW v-function header format ?

%Counters on branch
Based on \cite{Arn02}, we added counters by extending the way the baseline JIT generates conditional jumps. Counters are added just before and just after a branch. In several other VMs, the counters are added at the beginning of each function. The technique we used allows us to reduce the counter overhead as branches are 6 times less frequent that virtual calls in Smalltalk. In addition, the counters provide information about basic block usage. Every finite loop requires a branch to stop the loop iteration and most recursive code requires a branch to stop the recursion, so the main cases where hot spots are present are detected.

%hot spot detection - call-back
When a hot spot is detected, a specific Slang routine is called. The routine makes sure the n-frame where the hot spot is detected is reified so it can be introspected from Pharo. Then, the routine performs a virtual call with a selector registered from Pharo, the reified stack frame as the receiver and the boolean the conditional jump was branching on as a parameter. The method activated by the call-back, in Pharo, calls Scorch optimiser.

During optimisation, the bottom frames of the execution stack are used by Scorch optimiser. The frame above is the call-back frame, followed by the application frame holding the n-function with the hot spot, as shown on Figure \ref{fig:HotSpotCallBackStack}.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.35\linewidth]{HotSpotCallBackStack}
        \caption{Stack state during critical mode optimisation}
        \label{fig:HotSpotCallBackStack}
    \end{center}
\end{figure}

%Flag: FRAMEREIFICATION.

%returning from call-back
%The profiling counter machine code is generated so that upon return to the n-frame with the profiling counter, the conditional branch is performed again on the value returned. This way, if the Smalltalk code resets the profiling counters and returns the boolean passed by argument to the call-back, the application execution is resumed by performing one more time the branch with the same boolean.

%WARNING ! What happen when conditionalBranchTrip and reification.

\subsection{Scorch optimiser}

Scorch optimiser is activated by the call-back and has access to the reified stack. Scorch firstly analyses the stack and finds the best function to optimise. Then, it generates, either directly in critical mode or indirectly through background mode an optimised v-function and installs it for further uses.

%TO REMOVE
%We call the \emph{tripping function} the function where the hot spot is detected, because a counter has "tripped" in this function.

\paragraph{Stack search.}
\label{ss:stackSearch}

%Function selection, bottom is bad
When a hot spot is detected, Scorch is activated and has access to the reified stack. A naive approach would be to always optimise the function where the hot spot is detected and not to search the stack at all. Unfortunately, this heuristic would be terrible for a Smalltalk program. An important part of the execution time is due to the extensive use of closures. More specifically, most loops in the executed code, assuming the code respects standard Smalltalk coding conventions, are using closures. To efficiently remove the closure overhead, the closure needs to be inlined up to its enclosing environment to remove both the closure creation and the closure activation. If the function where the hot spot is detected is either activating closures or a closure activation itself, then optimising it won't gain that much performance because the closure creation and activation execution time will remain.

%Function selection, bottom or home is bad
Another approach, a bit less naive, would be to optimise the function where the hot spot is detected if it is a method, and the enclosing environment's function if it is a closure, in an attempt to remove closure overhead. Yet, this heuristic still does not solve the most common case of the problem. To illustrate the problem, let's look at a simple example with a loop over an array.

%array loop example. descr exampleArrayLoop
In the code sample in Figure \ref{fig:ExampleCode}, \ct{exampleArrayLoop} is a method installed in the class \ct{ApplicationClass}. Its method body consists of a loop over an array, the array being an instance variable. To loop over an array, Smalltalk provides high-level iterator methods such as \ct{do:}. In this case, \ct{do:} is very similar to \ct{foreach} in other languages and allows one to iterate over the array while providing at each iteration of the loop the array's element in the variable \ct{element}. The \ct{do:} method, installed in \ct{Array}, takes a single argument, a closure, which is evaluated using \ct{value:} at each iteration of the loop. The parameter of the closure activation is \ct{self at: i}, which represents the access to the element \ct{i} of the array. During the closure evaluation, the bottom three frames are the closure activation, the frame for \ct{Array >> do:} and the frame for \ct{ApplicationClass >> exampleArrayLoop} as shown on Figure \ref{fig:ExampleLoopStack}.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.42\linewidth]{ExampleLoopStack}
        \caption{Example stack during closure execution}
        \label{fig:ExampleLoopStack}
    \end{center}
\end{figure}

\begin{figure}
	\small
    \begin{center}
		\noindent \begin{tabular}{@{}c|l|l@{}}
		Method & ApplicationClass >> & Array >> \\
		 & exampleArrayLoop & do: aClosure \\
		\midrule
		Source &  array do: [ :elem | & 1 to: self size do: [:index | \\
		Code &\hspace{0.3cm}FileStream stdout nextPutAll: &\hspace{0.3cm}aClosure value: (self at: index)]. \\
		 &\hspace{0.6cm}elem printString ]. &  \\
		\midrule
		Bytecode &  17 <00> pushRcvr: 0 & 17 <4C> self \\
		& 18 <F9 00 00> createClosure & 18 <72> send: size \\
		& 	        25 <10> pushLit: FileStream & 19 <D1> popIntoTemp: 1 \\
		& 	        26 <81> send: stdout & 20 <51> pushConstant: 1 \\
		& 	        27 <40> pushTemp: 0 & 21 <D2> popIntoTemp: 2 \\
		& 	        28 <82> send: printString & 22 <42> pushTemp: 2 \\
		& 	        29 <93> send: nextPutAll: & 23 <41> pushTemp: 1 \\
		& 	        30 <5E> blockReturn & 24 <64> send: <= \\
		& 	21 <7B> send: do: & 25 <EF 0E> jumpFalse: 41 \\
		& 	22 <D8> pop & 27 <40> pushTemp: 0 \\
		& 	23 <58> returnSelf &  28 <4C> self \\
		&  &	29 <42> pushTemp: 2 \\
		&  & 	30 <70> send: at: \\
		&  & 	31 <7A> send: value: \\
		&  & 	32 <D8> pop \\
		&  & 	33 <42> pushTemp: 2 \\
		&  & 	34 <51> pushConstant: 1 \\
		&  & 	35 <60> send: + \\
		&  & 	36 <D2> popIntoTemp: 2 \\
		&  & 	37 <E1 FF ED ED> jumpTo: 22 \\
		&  & 	41 <58> returnSelf \\
		\end{tabular}
	\caption{Example code}
    \label{fig:ExampleCode}
    \end{center}
\end{figure}

%descr do:
The method \ct{Array >> do:} is using a special selector, \ct{to:do:}, which is compiled by the bytecode compiler to a loop, in a similar way to \ct{for} constructs in other programming languages. In fact, the \ct{Array >> do:} method body is a loop from 1 to the size of the array, evaluating the closure at each iteration for each element in the array. At each iteration, the current value of index is tested against the size of the array, and when that value is reached the loop is exited.

%profiling counter location
As discussed in the previous section, profiling counters detect frequent portion of code on branches. Each finite loop has a branch to either keep iterating over the loop or exit the loop. In the example, it means that the method \ct{Array >> do:} has a profiling counter on the branch testing the value of the index against the size of the array. The rest of the code, in the two methods and in the closure, have no other profiling counters.

%closure issue and why to pick next function
The hot spot is going to be detected on the profiling counter, hence in the method \ct{Array>>do:}. If Scorch optimised the \ct{Array>>do:} method, it cannot know what closure will be executed as the closure is an argument, while an important part of the execution time is spent creating and evaluating the closure. However, if \ct{Array>>do:} gets inlined into \ct{ApplicationClass>>exampleArrayLoop}, the closure evaluated would be known to be the closure \ct{[ :element | FileStream stdout nextPutAll: element printString ]}. Hence, to gain maximum performance, the optimiser should decide to optimise \ct{ApplicationClass>>exampleArrayLoop} and inline both the \ct{Array>>do:} method and the closure evaluation (\ct{value:}). 

%detail of the problem
In this case, the hot spot is detected in \ct{Array>>do:}. The hot spot is therefore detected in a method, not a closure. Naive heuritics would have chosen to optimise \ct{Array>>do:}, while it is better to select the caller stack frame's function.

%closure last explanation
Overall, because of the extensive use of closures, the optimiser almost never chooses to optimise the function where the hot spot is detected. It usually walks up a few frames to find the best function to optimise. 

\paragraph{Optimised v-function generation.}
Once Scorch has selected the v-function to optimise, it generates an optimised v-function. It attempts to do it immediately, within a limited amount of time. If it fails to do it, it postpones the optimisation to background mode. The function's optimisation and installation is using the same code in both cases, so we will discuss only the critical mode optimisation in the following paragraphs.

Scorch optimiser is implemented with traditional compiler optimisation techniques. It decompiles the v-function to optimise into a single static assignment intermediate representation, represented in the form of a control flow graph. Specific instructions that may require deoptimisation of the optimised frame have deoptimisation metadata attached which is updated during the optimisation passes to still refer to the correct values. During decompilation, Scorch asks Cogit to introspect the n-function corresponding to the decompiled v-function. If such a n-function exists, Cogit provides type information for each virtual call based on the data present in each inline cache and provide basic block usage based on the profiling counter values. The intermediate representation is annotated with this runtime information.

%The same intermediate representation is used during the whole optimisation process.

Scorch optimiser performs Smalltalk specific optimisations, very similar to the object-oriented specific optimisations present in other optimising JITs (speculative inlining, array bounds check elimination, etc.). Guided by the information provided by Cogit, Scorch speculates on receiver types for each virtual call to determine what v-function to inline. Each inlined v-function is decompiled to the same intermediate representation, annotated with runtime information the same way and merged into the same control flow graph. Each inlined function requires the insertion of a deoptimisation guard, to stop using the optimised code at runtime if the receiver type assumptions are not valid anymore. 

Once the inlining phase is finished, Scorch optimiser performs standard optimisations such as array-bounds check elimination with the ABCD algorithm~\cite{Bodi00a} or global value numbering. Scorch optimiser also postpones the allocation of objects not escaping the optimised v-function from runtime to deoptimisation time (or completely removes the allocation if the object is never required for deoptimisation)~\footnote{Objects used only inside optimised functions are not allocated unless deoptimisation is triggered. The state of such objects is moved from a heap location to the stack to allow one to optimise read-write using the single-static assignment property.}.

The back-end is the only non-conventional part of the optimiser. Scorch generates an optimised v-function and not an optimised n-function. Most intermediate representation instructions are translated to a single bytecode instruction from our extended bytecode set. However, as the bytecode set is stack-based, Scorch back-end needs to map each used intermediate representation instruction value either to a value on stack or a temporary variable. The deoptimisation metadata needs to be updated accordingly.

Lastly, the back-end generates an optimised v-function. For each point where deoptimisation could be requested (typically, failing guards, but also each virtual call for debugger support), the optimised v-function has metadata attached to reconstruct the stack with unoptimised v-functions.

\paragraph{Installation.}

If the optimisation has been done, the optimised v-function is installed. It is installed in the method dictionary of a class if this is a method, or in a method if it's a closure. If an optimised method is installed, the installation explicitely requests the VM to flush the caches dependent of this installation so it can be used at next call (the global look-up cache for the interpreter and the inline caches).

In addition, the dependencies are installed in the dependency manager so that if new code is loaded, code that may be dependent is discarded. Indeed, if a new version of a v-function is installed while the previous version was inlined in optimised v-functions, all optimised v-functions having inlined the previous version of the function need to be discarded. Frames on stack using discarded functions are lazily deoptimised when the execution flow returns to them.

Once installed, conceptually, the VM runs the optimised v-function as a normal v-function. The first few runs can be interpreted and the subsequent runs use the n-function produced by Cogit. The only difference is that optimised v-functions have access to additional operations, but those operations are supported both by the interpreter and by Cogit. Our first version worked exactly this way.

We then added a cheap heuristic to encourage the execution of optimised v-functions as n-functions. Optimised v-functions have a bit set in their header to tell Cogit not to compile profiling counters when generating their corresponding n-function. If this bit is set, the interpreter asks Cogit to compile it at the first run and immediately uses the n-function instead of doing so after a couple of interpretations.

In any case, the VM still needs to support the interpretation of optimised v-functions. Indeed, in very rare cases, Cogit cannot temporarily generate a n-function for the given v-function. For example, as the native code zone for n-functions has a fixed size of a few megabytes, it can happen that Cogit tries to compile a v-function while relinking a polymorphic inline cache~\cite{Holz91a} of another n-function. If there is not enough room in the machine code zone, a compaction of the machine code zone has to happen while relinking. It is not easy to compact the machine code zone at this point as it can happen that the polymorphic inline cache or the n-function holding it is discarded. To keep things simple, in this situation, we postpone the machine code zone compaction to the next interrupt point and interpret the v-function once. The interpretation of optimised v-functions, even if uncommon, is required for the VM to execute code correctly.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section {Function deoptimisation}
\label{sec:functionDeoptimisation}

%intro
The deoptimisation of the execution stack is similar to other VMs \cite{Fin03a, Holz92a}. An optimised frame is on stack and is in most cases a n-frame~\footnote{In very uncommon cases, the VM may decide to interpret an optimised v-function, leading to the presence of an optimised v-frame.}. The optimised frame cannot be used any more because an optimisation time assumption is invalid at runtime (a deoptimisation guard has failed) or the optimised function was discarded (by the debugger or because of new code loading). Deoptimising the stack requires the mapping of the optimised frame to multiple unoptimised v-frames. 

%Main difference
In our architecture, deoptimisation is done in two steps as shown on Figure \ref{fig:SFDeoptimisation}. Firstly, Cogit deoptimises the optimised n-frame to a single optimised v-frame. This step is not performed in the uncommon case where deoptimisation happens already from an optimised v-frame. For the rest of the section, we assume that the optimised frame is a n-frame, the other case being uncommon and being implemented simply by ignoring this first step. Secondly, Scorch deoptimises the optimised v-frames to multiple unoptimised v-frames.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\linewidth]{SFDeoptimisation}
        \caption{Stack frame deoptimisation in two steps}
        \label{fig:SFDeoptimisation}
    \end{center}
\end{figure}

%only stack deopt
When discussing deoptimisation, we deal only with stack recovery (deoptimisation of the optimised frame to the unoptimised v-frames). The unoptimised v-function is always present and never discarded, so the deoptimiser does not need to recreate it when restoring v-frames. There is no such thing in our design and implementation as reconstructing unoptimised v-function from optimised v-function-based on deoptimisation metadata. As far as we know, modern VMs such as V8~\cite{V8} always keep an unoptimised version of each function, so we believe the memory footprint impact of keeping them is acceptable.

\subsection{Deoptimisation of a frame}

%Intro + 2 cases.
Deoptimisation can happen in two main cases. On the one hand, Scorch optimiser inserts guards~\cite{Holz92a} to ensure assumption speculated at optimisation time are valid at runtime, such as the speculation on types. Such guards can fail, requiring deoptimisation of the execution stack to keep executing the application correctly. On the other hand, Smalltalk code can request deoptimisation of the code when manipulating the stack (typically the debugger's code does it).

\paragraph{Guard failure.} When a guard fails, the VM reifies the optimised frame so it can be introspected from Smalltalk. It then calls Scorch deoptimiser, switching from the VM executable code to Smalltalk, to restore the unoptimised stack
%the native code generated by Cogit calls a specific routine in Slang to perform the deoptimisation of the stack. The routine makes sure the frame with the tripping counter is reified so it can be introspected from Smalltalk. Then, the routine performs a virtual call with a selector registered from Smalltalk and the reified stack frame as the receiver. The Smalltalk code lastly calls Scorch deoptimiser's to restore the unoptimised stack.

During deoptimisation, the bottom frames are used by the deoptimiser. Just above is a frame for the activation of the virtual call performed by the routine followed by the optimised frame requesting deoptimisation, as shown on Figure \ref{fig:DeoptCallBackStack}.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.4\linewidth]{DeoptCallBackStack}
        \caption{Stack state during guard deoptimisation}
        \label{fig:DeoptCallBackStack}
    \end{center}
\end{figure}

%low-level stack banging issue, in Slang ?
%Flag: FRAMEREIFICATION.

\paragraph{Smalltalk code deoptimisation.}The Smalltalk code can request deoptimisation of specific frames to perform specific operations such as debugging. In this case, the situation is different because:
\begin{enumerate}
	\item The frame to deoptimise is in the middle of the application frames. Instead of having the call-back and deoptimiser frames below the frame to deoptimise on stack, other application frames are present.
	\item The instruction pointer is not on a guard instruction.
\end{enumerate}

\paragraph{Deoptimisation metadata.} To restore unoptimised frames, each instruction where deoptimisation can happen in optimised v-functions are annotated with deoptimisation metadata. The deoptimisation metadata is composed of the description of objects to recreate at deoptimisation time to resume execution with unoptimised functions at the corresponding annotated instruction. 

Some objects to recreate are unoptimised v-frames (reified as objects as detailed in Section \ref{par:frameToContext}). These unoptimised v-frames, once recreated, will replace the optimised v-frame requesting deoptimisation on stack. Other objects to recreate are objects which allocation has been postponed from runtime to deoptimisation time, because the allocation was not required in the optimised function and ignoring the allocation speeds-up code execution. 

The description of objects to recreate specifies what value the deoptimiser has to set in each field of each object recreated. The value can be a constant, a value to fetch from the optimised frame or another object to recreate.

\subsection{Scorch deoptimiser}

%deopt through metadata 
Cogit has reified the optimised n-frame to an optimised v-frame, hence, Scorch deoptimiser accesses the optimised v-frame. The frame is necessarily activated on a virtual instruction which is annotated with deoptimisation metadata. The metadata consists of a list of objects to materialize, which allocation has been postponed from runtime to deoptimisation time. As v-frames and closures are reified as objects in Smalltalk, part of those objects to rematerialize are unoptimised v-frames and closures. For each field of each object, the metadata specifies if the value is a specific constant, a value to fetch in the optimised v-frame or a reference to one of the other rematerialized object.

\paragraph{Stack edition.}

Once all the unoptimised v-frames are reconstructed, the execution stack needs to be edited to use them. This is done using the stack manipulation APIs. Basically, the VM splits the current stack page in two, copying one part on another stack page. Deoptimised frames are present in the middle in their single context form, in a similar way to frame divorces described in the previous chapter and as shown in Figure \ref{fig:DeoptStacks}.

\begin{figure}[h!]
    \begin{center}
		\includegraphics[width=0.80\linewidth]{DeoptStacks}
        \caption{Stack edition}
        \label{fig:DeoptStacks}
    \end{center}
\end{figure}

\paragraph{Discard functions.} When new unoptimised functions are installed (for example when a library is loaded), optimisation assumptions may be invalidated because some look-up results cannot be guaranteed anymore. In this case, we cannot really iterate over the whole stack and deoptimise all the frames holding invalidated code as it would take a long time (especially multiple stack manipulations may take a while). Instead, Scorch mutates the discarded optimised v-function to hold only guard failures virtual instructions, so optimised v-frames would always trigger deoptimisation upon return. In addition, Scorch requests Cogit to patch all return native instruction pointers of all optimised n-frames representing discarded function activations to a pointer to a specific routine which will trigger deoptimisation upon return.

\paragraph{Execution restart.}

If the deoptimisation happened due to a guard failure (or due to a return to a discarded function), the application needs to resume execution once deoptimisation is performed. In this case, the application frame is just above the call-back frame so returning resumes execution. %There is one little detail, a return instruction in Smalltalk pushes the returned value on the caller stack. The deoptimisation call-back returns the top value of the bottom deoptimised frame to resume execution. The bottom deoptimised frame is guaranteed not to be empty as deoptimisation can be only in position in the code where there is necessarily something on stack.

%In those cases, the first deoptimiser frame is just below the bottom restored unoptimised v-frame. Returning from the deoptimiser frame to the application is not as easy as for the optimiser call-back. The optimiser is always triggered on a conditional branch and conditional branches require a value to test to be on stack. Hence, the optimiser could simply return the value to test and the execution resumes just before the branch. 

%In the case of deoptimisation, the application stack frame could be restored on many different instructions. Especially, it could be restored at an instruction where no value is on stack. Hence to return to the application, the deoptimiser has to perform a special return instruction, which resumes the caller frame execution without pushing any value on its frame.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}
\label{sec:relatedWorkArch}

This section compares our architecture against existing work. The first section compares the overall architecture versus the meta-tracing and function-based architectures. Section \ref{sec:interface} compares our optimised v-function representation against representations existing in other VMs. Section \ref{sec:codeSharing} discusses how different JIT tiers may or may not share portions of code.

The two research subproblems and their related work are detailed later in the thesis. Chapter \ref{chap:metacircular} discusses how Scorch is able, under specific constraints, to optimise its own code. Chapter \ref{chap:persistence} details how the runtime state, including optimised v-functions, can be persisted across multiple start-ups.

\subsection{Architecture}
\label{sec:relWArch}

Compared to the classical three tier function-based architecture described in Chapter \ref{chap:stateOfTheArt}, the existing Pharo runtime featured only the first two tiers. Sista is an implementation of the third tier, the optimising JIT tier. The implementation is however quite different from other VMs. One of the main difference is the split between the high and low-level part of the optimising JIT as well as the optimised v-function representation used to communicate between both parts. As far as we know, no other VM splits its optimising JIT this way. The other main difference is Cogit, which in one code base, can be used both as a baseline JIT and as a back-end for the optimising JIT.

Sista was not designed as a meta-tracing JIT. The main reason is that improving the performance of loop bodies did not seem the right thing to do in the case of Pharo. As explained in the example of Section \ref{ss:stackSearch}, if the Smalltalk code executed respects standard Smalltalk coding conventions, most loops just activate a closure. The optimiser needs to optimise code outside of the loop body to be able to inline the closure activation and improve performance. Hence, it is not clear how to design an efficient meta-tracing JIT for Pharo: the naive strategy of focusing the optimisations on loop bodies is not good enough in our case. The project RSqueak\cite{Felg16f} implements a Squeak VM using the RPython toolchain~\cite{Rigo06a} and the problem described is present.

\subsection{Optimised virtual function representation}
\label{sec:interface}

One important difference in Sista compared to most optimising JITs lies with the optimised v-function representation. Most optimising JIT do not generate optimised v-function but generate only optimised n-functions. As far as we know, no existing baseline JIT is used to compile v-functions optimised using runtime information to optimised n-functions.

\paragraph{Threaded code.}
%Maybe add ref later if relevant...
One of the first works in the late 80s to speed up object-oriented language VMs was in the direction of threaded code interpreters. Such virtual machine would feature a threaded code interpreter instead of a v-function interpreter. A threaded code interpreter is faster to execute code, but requires a small compilation time to translate the v-function to its threaded code representation. The threaded code representation is platform-independent and can be considered as an optimised v-function. The main advantage of the threaded code interpreter over a JIT is that it provides speed-up while being platform-independent. Indeed, a JIT requires a different back-end for each native code supported. 

Work in this direction has mostly vanished for two reasons. Firstly, the execution of v-function through JITs is more efficient than the execution through a threaded interpreter. Secondly, the use of threaded jumps in v-function interpreters allowed to massively reduce the performance difference between threaded code and v-function interpreters.

Recently, one problem for VM implementors was to implement JITs for iOS. Apple's policy was forbidding, until very recently, to have a memory page both writable and executable. Such a page is required by a JIT to store the native code it generates so it was not possible to build a JIT running in iOS. One experiment in the Dart VM was to keep the three tier strategy of the function-based architecture, but the baseline and optimising JIT would generate an abstract assembly, similar to threaded code. The abstract assembly was then executed quickly by a low-level interpreter, mapping almost one-to-one abstract assembly instructions to native instructions. This solution was not as fast as regular JITs, but it allowed the VM to perform decently under Apple's policy constraints.

\paragraph{WebAssembly.}
Recent Javascript VMs have support for WebAssembly~\cite{WebAssembly}, an abstract assembly code representation for functions. In this case, the VM can take as input two different representations of v-functions. One form is the v-function representation for the language supported, in the case of Javascript the source code of v-functions. The other form, the WebAssembly form, allows the VM to execute v-functions optimised ahead-of-time.

It would be interesting for Scorch to target an optimised v-function representation such as WebAssembly instead of our extended bytecode set. We implemented Scorch to target the extended bytecode set as we could make the architecture work with a limited number of evolutions in Cogit. Being able to compile efficiently a representation such as WebAssembly would require us to implement larger extensions to Cogit. One of the goal of our architecture was to limit evolutions on low-level parts of the VM, so the direction we took looked more interesting. Recently, an abstract assembly started to be supported in the Pharo VM, called LowCode~\cite{Salg16a}. It would be interesting, as future work, to investigate if Scorch targeting LowCode is a valuable option.

\paragraph{Hotspot Graal runtime.}
The VM design the most similar to ours is certainly the Hotspot VM using the Graal compiler~\cite{Oracle13,Dubo13c} as an alternative optimising JIT. In both cases, the baseline JIT and the interpreter are compiled to machine code ahead-of-time (in our case, from Slang to native code, in Hotspot, from C++ to native code). The optimising JITs, Scorch and Graal, are however written in the language run by the VM and run in the same runtime as the optimised application. 

The main difference still lies with the optimised function representation. Graal generates optimised n-functions in the form of a data structure including the native code and metadata. Graal provides these data structures to the Hotspot VM so it can install them~\cite{Grim13a}. Scorch generates however optimised v-functions, which requires Cogit to compile them. 

With the Graal strategy, it may be possible to produce optimised n-functions slightly faster to execute as the Graal back-end may perform low-level optimisations more aggressively than Cogit. The compilation time may also be slightly better as no optimised v-function representation needs to be created. However, our optimised v-function representation allows us to have a single back-end to maintain for the two JIT tiers in the form of Cogit and as discussed later in the thesis to persist optimised n-functions across multiple start-ups.

\subsection{Sharing code between compiler tiers}
\label{sec:codeSharing}

Most optimising JITs have no code in common with the baseline JIT they extract runtime information from. In our context, Cogit is used both as the baseline JIT and as a back-end for the optimising JIT. Most VM teams keep the back-ends of different JIT tiers independent as each back-end has different constraints. On the one hand, the optimising JIT back-end needs to generate high-performance code in a reasonnable amount of time. On the other hand, the baseline JIT back-end needs to generate code that can be instrospected later as quickly as possible. 

We believe that with Cogit performing a limited number of low-level optimisations while providing n-function instrospection, we can reduce the maintenance cost of the low-level parts of our VM compared to other architecture with a reasonnable performance loss. 

There are two main cases in the litterature where JIT tiers are sharing portions of code.

\paragraph{Webkit VM.}The Javascript Webkit VM~\cite{Webkit15} is one of the only VMs where important parts of code are shared between multiple JIT tiers. The Webkit VM is different from other VMs as it features four tiers, including two optimising compiler tiers. In the webkit VM, the two optimising JIT tiers are sharing the high-level part of the compiler, but not the back-end. There is some code shared for n-function introspection, but each back-end of each JIT tier is mostly independent from the other tiers.

In our case, our two compiler tiers share the same back-end is used. In addition, the portion of code is shared between a baseline and an optimising JIT and not two optimising JIT tiers.

\paragraph{WebAssembly.}Modern Javascript VMs support WebAssembly~\cite{WebAssembly}, an abstract assembly representation for functions. In the V8 Javascript engine~\cite{V8}, the back-end of the optimising compiler Turbofan is shared between the WebAssembly compiler and the optimising JIT. There are some similarities with our work as WebAssembly is a representation that could be used to represent optimised v-functions. 

In this case, the back-end is shared between two optimising runtime compilers. There is no code shared between the optimising JIT and the baseline JIT. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Conclusion}

The chapter provided an overview of our architecture and detailed the optimisation of functions and deoptimisation of stack frames. The following chapter discusses the evolutions of the Pharo runtime implemented in the context of the thesis.

\ifx\wholebook\relax\else
    \end{document}
\fi