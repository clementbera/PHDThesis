\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Validation}
\label{chap:validation}
\minitoc


To validate our architecture, we evaluated in Section \ref{sec:bench} the execution time of a set of benchmarks on the Pharo VM with and without Sista. With Sista, the VM is up to five times faster than the current production VM. The VM was also evaluated in different configurations to show the overhead of profiling counters and the persistance of optimisations across multiple start-ups. Section \ref{sec:otherValid} discusses other strategies we implemented to validate our architecture.

\section{Benchmarks}
\label{sec:bench}

This section first details the methodology used for benchmarking, then describes each benchmark used, analyses the results and concludes.

%TODO use a citation instead of footnote
\subsection{Benchmark methodology}

We evaluate our architecture on a variety of benchmarks from the Squeak/Smalltalk speed center~\cite{Felg16a} that is used to monitor the performance of the Cog VM and other compatible virtual machines for Squeak and Pharo. The benchmarks are adapted from the Computer Language Benchmarks Game suite \cite{GameBenchs} and contributed by the Smalltalk community. We have selected these benchmarks to give an indication of how certain combinations of operations are optimised with our architecture. Although they tend to over-emphasize the effectiveness of certain aspects of a VM, they are widely used by VM authors to give an indication of performance.

We consider the results on the Pharo VM with four different configurations:
\begin{enumerate}
	\item \emph{Cog} is the existing VM (interpreter and Cogit as the baseline JIT). Cog represents the baseline performance.
	\item \emph{Cog+Counters} is the existing VM with profiling counters without any additional optimisation. Cog+Counters is used to evaluate the profiling counters overhead.
	\item \emph{Sista Cold} is the Sista VM started on a snapshot without any optimised v-function.
	\item \emph{Sista Warm} is the Sista VM started on a snapshot that already contains optimised v-functions.
\end{enumerate}

We iterated each benchmark for 100 iterations or 60 seconds, whichever came last, and measured the last 10 iterations. For Sista Warm, we start with an already optimised snapshot. For Sista Cold, we only enable the optimising compiler before the last 10 iterations (this way, the warm-up from Cog's baseline JIT is not included in measuring the warm-up of Sista Cold). The benchmarks were run on an otherwise idle Mac mini 7,1 with a Dual-Core Intel Core i7 running at 3GHz and 16 GB of RAM. We report the average milliseconds taken per iteration, with the confidence interval given for the 90$^{th}$ percentile. For these measurements, we configured the VM to detect hot spots when a profiling counter reaches 65535 iterations (they are encoded as \emph{int16}, so this is currently the maximum) and we allow the optimiser up to 0.4 seconds to produce an optimised method in critical mode (the benchmarks are run consecutively without any idle time so optimisation in background mode is not considered). We use a high counter value and allow for a long optimisation time, because as the optimisations are saved across start-ups we believe it does not matter that much if the VM takes a long time to reach peak performance. 

We have found these values to produce good performance across a variety of benchmarks. Because Scorch is written in Smalltalk itself, it is possible to configure various other optimisation options depending on the application, for example, to emphasize inlining, to produce larger or smaller methods, or to spend more or less time in various optimisation steps. 

\subsection{Benchmark descriptions}

This section briefly describes for each benchmark what the benchmark does and what operations are used the most.

	\paragraph{A*.}The A* benchmark is a good approximation for applications where many objects collaborate. It measures parsing of large strings that define the layout of the tree of nodes, message sending between each node, arithmetic to calculate costs, and collection operations. In the benchmark, we alternately parse and traverse two different graphs with 2,500 and 10,000 nodes, respectively. It is also a good benchmark for inlining block closures that are used in iterations.

	\paragraph{Binary tree.}The binary tree benchmark allocates, walks and deallocates binary trees. The benchmark is parameterized with the maximum tree depth, which we have set to 10. The benchmark is focused on object allocation, tree traversal and object deallocation.

	\paragraph{JSON parsing.}We test a JSON parser written in Smalltalk as it parses a constant, minified, well-formed JSON string of 25 Kilobytes. This benchmark is heavy on nested loops and string operations, as well as a lot of parsing rules that call each other.

	\paragraph{Richards.}Richards is an OS kernel simulation benchmark that focuses on message sends between objects and block invocations. We ran this benchmark with the customary idle task, two devices, two handler tasks, and a worker, and filled the work queue of the latter three.

	\paragraph{K-Nucleotide.}This benchmark reads a 2.4 MB DNA sequence string and counts all occurrences of nucleotides of lengths 1 and 2, as well as a number of specific sequences. It is a benchmark meant to test the performance of dictionaries in different languages, but serves well to test our inlining of small methods into loops. The benchmark runs much slower than the others due to the large input, taking over 4 minutes to complete.

	\paragraph{Thread ring.}The Thread ring benchmark switches from thread to thread (green threads) passing one token between threads. Each iteration, 503 green threads are created and the token is passed around 5,000,000 times.

	\paragraph{N-body.}N-body models the orbits of Jovian planets, using a symplectic integrator. Each iteration simulates 200,000 interactions between the Jovian planets. The n-body benchmark is heavy on float operations, and ideal benchmark to highlight the inlining that Sista performs.

	\paragraph{DeltaBlue.}DeltaBlue is a constraint solver, it tests polymorphic message sends and graph traversal. Each iteration tests updating a chain of 5000 connected variables once with equality constraints and once with a simple arithmetic scale constraint.

	\paragraph{Spectral Norm.}Calculating the spectral norm of a matrix is heavy on floating point and integer arithmetic as well as large arrays. The arithmetic is expected to inline well, but since large allocations take place throughout this benchmark, the performance benefit for Sista is expected to be smaller.

	\paragraph{Mandelbrot.}This benchmark calculates the Mandelbrot set of on a 1000x1000 bitmap. It is implemented in only one method with nested loops that almost exclusively calls primitive float methods and thus is a good candidate for Sista optimisations.

	\paragraph{Meteor.}This benchmark solves the meteor puzzle by recursively trying to fit puzzle pieces together using an exhaustive search algorithm.

\subsection{Results}

Figure \ref{fig:measure} shows all the measurements in the form of graphs. The ordinate of each graph is the average time in milliseconds to run one iteration of the corresponding benchmark, so the smaller the column is, the fastest the benchmark is run. The exact values of the measurements are reported in Figure \ref{tbl:benchmark}. The following paragraphs describe each a benchmark and its corresponding performance measurements. We distinguish three categories of benchmarks.

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=\linewidth]{Bench}
        \caption{Benchmark measurements}
        \label{fig:measure}
    \end{center}
\end{figure}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=\linewidth]{BenchTable}
        \caption{Benchmark results (standard errors in avg ms, 90\% confidence interval)}
        \label{tbl:benchmark}
	\end{center}
\end{figure}

	\paragraph{Quick start-ups.} A*, Binary tree, JSON parsing, Richards, and K-nucleotide reach quickly peak performance. The difference between Sista Cold and Sista Warm is minimal, as even from a cold state, the VM is able to reach peak performance during the first few runs out of the ten runs. We can however see that the error margin in the Sista Cold is greater, as the first few runs have lower performance.

	\paragraph{Slow start-ups.} Thread ring, N-body, Delta blue and Meteor require multiple runs to reach peak performance. The average performance of the ten first runs is clearly not as good in Sista Cold that in Sista Warm, as a significant amount of these runs are not done at peak performance. In fact, in the case of N-body, ten runs is not even enough to reach peak performance. The error margin in Sista Cold is very important.

	\paragraph{Very slow start-ups.} In the case of Mandelbrot and Spectral Norm, ten runs is far from enough to reach peak performance. An important part of the execution time in the ten first runs is spent in compilation, leading the benchmark to be slower than the base VM. Once peak performance has been reached, Spectral Norm is 10\% faster than Cog. The peak performance of Mandelbrot is similar to Cog performance, only removing the overhead of profiling counter, because Mandelbrot is a floating-pointer intensive benchmark and we have not yet implemented floating-pointer optimisations in Sista.

\subsection{Conclusion}

For all benchmarks our approach shows significant performance improvements on the scales that we would expect given the various benchmark's properties. For these benchmarks, Sista is up to 80\% faster. Since the baseline JIT compiles almost every method on second invocation, this is also the only warmup when a snapshot that was warmed up using our approach is launched. Thus, these benchmarks indicate that Sista can provide significant performance benefits without any additional warm-up time compared to the baseline compiler.

	We ran our VM profiler to profile the VM C code, but as for real world application, the time spent in the baseline JIT compiler generating machine code from bytecode is less than 1\% of the total execution time. As the runtime switches from interpreted code to machine code at second invocation for most functions and at first invocation for optimised functions, the time lost here is too small to be shown on our graphics. In fact, the time lost here is not significant compared to the variation so it is difficult to evaluate in our current setting. We believe that using a back-end doing many more machine low-level optimisations would increase the machine code compilation time and in this case we would be able to see a difference between the first run of pre-heated snapshot and second run as the VM still needs to produce the machine code for the optimised bytecoded functions.

	Our optimiser is controlled by a number of variables that have been heuristically chosen to give good performance in a variety of cases. These include, among others, global settings for inlining depth, the allowed maximum size of optimised methods as well as methods to be inlined, as well as the time allowed for the optimiser to create an optimised method before it is aborted. We have found that for certain benchmarks, these variables can have a great impact. We are working on fine-tuning these default values, as well as enabling heuristics to dynamically adapt these values depending on the application.
	
\section{Other validations}
\label{sec:otherValid}

To evaluate our infrastructure, we tried two other innovative techniques in addition to measuring benchmarks. On the one hand, we built an experimental technique to validate runtime deoptimisation using partial evaluation. On the other hand, we built a type inferencer using the runtime information extracted from inline caches. The promising results of the type inferencer confirm that the runtime information is quite precise and should give Scorch valuable hints to direct compiler optimisations.

\subsection{Experimental validation of the deoptimiser}

The speculative optimisations in the Sista VM enable many performance optimisations. However, they also introduce significant complexity. The compiler optimisations themselves, as well as the deoptimisation mechanism are complex and error prone. To stabilize Scorch, we designed a new approach to validate the correctness of dynamic deoptimisation. The approach~\cite{Bera16a} consists of the symbolic execution of an optimised and a non optimised v-function side by side, deoptimising the abstract stack at each point where dynamic deoptimisation is possible and comparing the deoptimised and non optimised abstract stack to detect bugs. 

Although this approach is interesting, the complexity required to maintain a good symbolic executor is significant compared to the time available for the maintenance of the overall VM. In other VMs such as V8~\cite{V8}, dynamic deoptimisation is stabilised using a "deopt-every-n-time" approach: the program run is forced to deoptimise the stack regularly (every n deoptimisation point met). This approach is way simpler to maintain and finds in practice a similar number of bugs than the approach built. We are now using a "deopt-every-n-time" approach to validate the deoptimisation of functions.

\subsection{Assessment of the runtime information quality}

Thanks to the \emph{sendAndBranchData} primitive method, any Pharo program, including Scorch, may request Cogit to provide the runtime information of a specific function. This runtime information is composed of the types met and functions called at each virtual call and the profiling counter values. To assess the quality of the runtime information provided for each virtual call, we built an approach called \emph{inline-cache type inference} (ICTI) to augment the precision of fast and simple type inference algorithms~\cite{Milo16a}. 

ICTI uses type information available in the inline caches during multiple software runs, to provide a ranked-list of possible classes that most likely represent a variable's type. We evaluated ICTI through a proof-of-concept that we implemented in Pharo Smalltalk. Analyzing the top-$n$+2 inferred types (where $n$ is the number of recorded runtime types for a variable) for 5486 variables from four different software systems (Glamour~\cite{Bung09a}, Roassal2~\cite{Pena13a}, Morphic~\cite{Fern07y} and Moose~\cite{Girb10a, Duca05a, Duca00b}) show that ICTI produces promising results for about 75\% of the variables. For more than 90\% of variables, the correct runtime type was present among the first ten inferred types. Our ordering shows a twofold improvement when compared with the unordered base approach~\cite{Pluq09a}, \ie for a significant number of variables for which the base approach offered ambiguous results, ICTI was able to promote the correct type to the top of the list.

Based on these results, we believe the runtime information extracted from the first runs to be quite reliable. In any case, this information is used only to direct compiler optimisation: if the runtime information is not correct, the code is executed slower but correctly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Conclusion}

This chapter validates Sista by showing on a range of benchmarks the performance gain of the architecture (up to 5x). The next Chapter discusses the future works that could be considered based on this thesis.

\ifx\wholebook\relax\else
    \end{document}
\fi