\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Metacircular optimiser and deoptimiser}
\label{chap:metacircular}
\minitoc

%Intro
By design, the runtime optimiser and deoptimiser are written in Smalltalk and are running in the same runtime than the optimised application. This design leads to multiple problems similar to problems existing in metacircular virtual machine. 

%single-threaded -> for comparison with Graal and co later
As Pharo is currently single-threaded, it is not possible to run Scorch in a concurrent native thread. To optimise code, Scorch requires either to interrupt the application green thread temporarily or to postpone the optimisation to a background-priority green thread. The deoptimiser cannot however postpone its tasks as it would block completely the running application, hence it has to interrupt the application green thread until the deoptimisation is done.

%introduction of the main issue
We call the \emph{infinite recursion} problem the main issue. The infinite recursion happens in both the optimiser and the deoptimiser. When a frequent portion of code is detected in the optimiser code, it interrupts itself and starts to optimise one of its own v-function. While doing so, the same frequent portion of code is detected and the optimiser interrupts itself again to optimise the same v-function, until it interrupts itself again, completely freezing the application. When the deoptimiser requires to deoptimise a function to be able to keep deoptimising an optimised frame, it calls itself, which may require to deoptimise the same function, leading to the deoptimiser calling itself again and again, also completely freezing the application.

%Different constraint so different solutions
The optimiser and deoptimiser have different constraints. For example, it is possible to disable temporarily the optimiser, which in the worst case leads to some functions not to be optimised, but it is not possible to disable temporarily the deoptimiser as it is required to execute code. As they have different constraints, they need different solutions for the infinite recursion problem. In both cases, the management of the dependencies of the framework is critical to ensure that the infinite recursion issue is solved.

%Outline and solution
This chapter explains the design used to avoid the infinite recursion issue in both the deoptimiser and the deoptimiser. For each of them, we specify what constraints we enforce on package dependencies and on the runtime to make sure the problem is entirely solved. The problem is solved in the optimiser by defining a critical portion of code in the optimiser where it disables itself temporarily, ensuring that it cannot be triggered on this part of its own code. The deoptimiser solves the problem by using code completely independent from the rest of the system that cannot be optimised, hence never requires to be deoptimised. The last section discuss similar design issues in other VMs and when relevant, compares our solution to other solutions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Scorch optimiser}

%Intro
Cogit was extended to detect frequently used portion of code based on profiling counters. When such a portion of code is detected, Cogit triggers a call-back to request Scorch to optimise a v-function based on the current state. The overall design is then the following: the optimiser interrupts temporarily the application green thread, finds a v-function to optimise based on the current stack, optimises and installs the optimised version, then resumes the application. The optimised v-function installed will be executed at the next call of the function.

\subsection{Optimiser critical and background modes}

%TimeBeforePostPoning: problem so it's required
This design is too naive because the optimiser may interrupt the application for a long time. Indeed, optimising a function can take a long time in slow machines or when a pathological function is optimised. To experiment with the optimiser, we worked mainly using one application, the development environment of Pharo. In the case of a user-interface application, it is \emph{very} annoying to see the application interrupted during half a second or more, it feels like the user interface is slow, lagging and unresponsive.

%First solution: timeBeforePostponing, with constraint for long methods
To avoid the problem, we limited the time window of the optimiser to a small amount of time, configurable from the language. For the development tools, we limited it to 40 ms. The limitation is enforced by a high-priority green thread, set to stop the optimiser after a given amount of time. As the current tools are refreshing at 50Hz, it means that the optimiser, at worst, force the system to drop two frames. In practice, most v-functions are optimised in less than 40ms. However, we now have a significant problem: v-functions too long to optimise are not optimised at all.

%idle and postpone
Upon profiling, we noticed, as expected, that this user interface application spends a significant amount of time in idle\footnote{An application in idle means it has nothing to do, it is typically waiting for an event to do anything.}. We show for example in figure \ref{fig:ApplicationIdle} that the application is successfully executing code, then idle, then executing code again, etc. 

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.9\linewidth]{fig:ApplicationIdle}
        \caption{User interface application idle times}
        \label{fig:ApplicationIdle}
    \end{center}
\end{figure}

%background thread
Based on this result, we introduced a background green thread responsible for optimising the v-function too long to optimise in the small time window. Hence, when the application would normally become idle, it starts by optimising such v-functions until and becomes idle when no v-functions to optimise are remaining.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.9\linewidth]{fig:ScorchModes}
        \caption{Scorch critical and background modes}
        \label{fig:ApplicationIdle}
    \end{center}
\end{figure}

%Fig explanation, critical vs background mode
As shown on figure \ref{fig:ScorchModes}, when the application is running, it can be interrupted for a small time window at worst. When the application normally becomes idle, the background green thread starts optimising the v-functions long to optimise. When no more v-functions are queued for optimisations, the application becomes idle. In any case, after a while, all critical portion of code have been optimised and the application is usually not interrupted any more while the background green thread has usually no more v-functions to optimise.

\paragraph{Conclusion.} The Scorch optimiser can be run in two modes. On critical mode, it interrupts the application green thread and has a limited time-window to optimise a function. On background mode, it optimises code only when the application is idle but as no time limit. 

\subsection{Infinite recursion}

%repeat problem for optimiser
As Scorch is written in Smalltalk, it can theoretically optimise its own code. In practice, if Scorch is triggered on itself, it may lead to an infinite recursion. Indeed, each time Scorch tries to optimise anything, before reaching the point where it can install the optimised function, it may interrupt itself to start optimising one of its own function. If a frequent portion of code is detected at each optimisation attempt, then Scorch never reaches the point where it can install optimised function because it keeps restarting new optimisation on its own code.

%slow down - nothing happen in critical and slow but succeed in background mode
In critical mode, Scorch has a limited time window. Hence, if the infinite recursion happens, the optimiser gets stuck until the time window ends, then the application resumes without any optimised function installed. In background mode, the optimisation is slow because the optimiser gets stuck in critical mode multiple times, but at some point the code gets optimised and installed.

%TODO
%explain here why we cannot add it in thread safe queue, we need function from stack. Take the example of do:, tripping counter and function

\paragraph{Naive solution.} The first solution we tried to implement is to disable to optimiser when it is running. We believed it would solve the problem at the cost that the optimiser would not be able to optimise itself, and we were willing to to accept this consequence.

%call-back removal / addition implementation
To do so, we changed the VM call-back activating the optimiser to uninstall itself upon activation. Then, we changed the optimiser so that when it decides to resume the application, after postponing the optimisation or installing optimised code, it installs back the call-back. This way, we thought the optimiser would never end up in a situation where it optimises itself, solving entirely the problem.

%can optimise itself through back process
Then, we run our benchmarks and see that the problem was solved but the optimiser could still optimise its own code. Our implementation effectively disabled the optimiser, but only while it was on critical mode. Hence, when frequent portions of code of the application were detected, they were optimised or postponed without any issue as the optimiser disabled itself during this phasis, and the application resumed just fine. However, when the optimiser was run in background mode, it was not disabled. Hence, in this case, the optimiser in background was sometimes interrupted by itself in critical mode to get optimised.






Pb : all the code to optimise in critical mode counter reset but not opt.
Further solution is to block only stack search in critical mode.

%When in backprocess it can optimise itself.



\paragraph{Further work.} disable when stack searching in critical mode.


\paragraph{Future work and discussion.} Alternatively to the flag approach, the optimiser could decide to add any function requiring runtime optimisation to the existing background green thread, so that Scorch could be optimised when the application is in idle. We investigated in that direction, but when the VM triggers the call-back to start optimising code, Scorch needs to figure out based on the current stack what function to optimise. In many case, the frequently used function is inside a loop, and most loops in Smalltalk are run using high level constructs using closures, hence Scorch needs to optimise the function of another frame than the bottom frame. The code searching the stack for the function to optimise is also written in Smalltalk as part of Scorch, hence adding a function to the background green thread while avoiding infinite recursion is not that obvious. Further investigation in this direction is required in the future.

Another alternative to the flag approach would be to optimise the runtime compiler statically, using it on itself either without performing optimisations requiring type speculation or with runtime type information statically computed from warm-up runs. 







%old
%In the current environment used for benchmarking, a global flag exists to know it exists one green thread currently performing runtime optimisation by blocking an application thread. The flag is set when the call-back to activate Scorch is executed. When set, the call-back is disabled so that no infinite recursion can happen. As the Scorch compilation process is limited to a fixed amount of time, the flag cannot be continuously set for more than this amount of time.

%Old
%The flag approach has a significant advantage: it is quite simple both conceptually and implementation-wise, while it completely avoids the infinite recursion problem. It has however a major drawback: Scorch cannot optimise its own code any more. Of course, Scorch uses core libraries that can be optimised. The main case are the core collections, if the application optimised is also using core collections, they may get optimised, then Scorch ends up using an optimised collection library. For this reason, it is possible that while optimising code Scorch triggers the deoptimiser, and it works perfectly fine.


%Old
%An interesting property of the optimiser is that it can fail. If anything happens so that Scorch cannot generate optimised code within its time limit, Scorch can simply ignore the given code and the execution keeps going with non optimised code. Currently the optimisation is postponed in the background green thread if the optimisation takes too long, but even then, sometimes the background green thread compilation queue may be discarded because new code is load ed or installed. Lastly, as the optimiser grows in maturity the optimisation passes grow in complexity, and it can be that very narrow cases are not correctly handled in the optimiser raising exception. In this case, the optimiser is configured so that any exception escaping the framework leads to the code to be marked as non optimiseable, but the execution can resume just fine with non optimised code.

\subsection{Dependencies.} 

The first constraint to note when programming Scorch, which may be obvious to the Kernel programmer, is that Scorch cannot depends on any framework or library but the Kernel and Core librairies. Each framework or library in the system relies on the execution engine to perform its code. Scorch is part of the execution engine. Hence, if Scorch relies on an external librairy and that someone modifies the library, the execution engine may not be stable any more and the runtime completely crashes. In fact, all the Kernel code and Core librairies have similar constraints, they cannot rely on anything to keep the system modular. 

While writting Scorch, we needed a tool to compress the deoptimisation metadata generated aside from the optimised code. We wanted to use the standard Pharo serializer, Fuel (CITE), but we were not able to do it or further modification on Fuel would break the execution engine.

In the end, we limited the dependencies of Scorch to the Pharo Kernel and the core collections (exactly: Set, OrderedCollection, Array, ByteArray and Dictionary in addition to the kernel). Any change on one of this dependency may require to change something in Scorch to keep the system running.

\subsection{Debugging and runtime modification}

%Should I talk about that at all ? I was thinking over a restricted compiling to C but maybe we don't care.

%Maybe rewrite so that formally does not work but in practice it does.

As any Smalltalk program, it is possible to modify the optimiser while it is running, for example in the debugger. If the modifications leads to incorrect optimiser behavior, then the runtime may crash. To avoid crashes, it may be wise to disable the optimiser while editing it. In practice, this feature is used only by the optimiser implementors. It is very useful to debug the optimiser to understand specific bugs or compiler decisions. With careful understanding of the infrastructure, it is possible in practice to debug the optimiser while it is running and modify its code. The optimiser is set by default to catch all exceptions, failing the optimisation of a specific v-function if an exception was raised. Hence, if the code modification triggers a compile-time exception, the system shall not crash. Unfortunately, in some cases, the optimiser may have silent errors, generating incorrect code without raising exceptions and completely crashing the system.

The only part of Scorch that cannot really be edited is the deoptimisation metadata generation. Indeed, deoptimisation metadata is also used by the deoptimiser which, as detailled in the following section, has stronger constraints on its code. If one modifies the deoptimisation metadata generated, the deoptimiser may not be able to deoptimise correctly optimised code any more, leading to crashes.

\section{In-language deoptimiser}

The trap instruction is present in the extended bytecode set and when reached through execution flow, Cogit triggers a call-back to the deoptimiser. In addition, multiple tools in the language, such as the debugging tools, have been changed to call the deoptimiser when the programmer attempts to introspect the stack.

On the contrary to the optimiser, deoptimisation cannot be postponed to a background green thread, or the application green thread may be blocked until the application reaches idle state. In addition, deoptimisation cannot fail or the runtime crashes. If the optimiser failed to create an optimised function, then the execution could simply fall back to non optimised code. If the deoptimiser fails to recreate a non optimised version of the stack, then the system crashes or the application green thread has to be terminated.

%TODO I think I mean it cannot be disabled.

As the deoptimiser cannot fail, it is not possible to solve the infinite recursion problem in the same way than the optimiser. If the deoptimiser request deoptimisation, as for the optimiser requesting optimisation, the programs ends up in an infinite recursion where it keeps deoptimising / optimising itself. In the case of the optimiser, we could simply disable it while it is running, and the entire problem was solved. In the case of the deoptimiser, if we disable it and the deoptimiser request deoptimisation, the program cannot resume using non optimised code and crashes. 

To solve this infinite recursion problem, we implemented two solutions. The first solution attempts to restore the system in a recovery mode when recursive deoptimisation happens. It was used for the first benchmark, but it did not work correctly with benchmarks creating multiple green threads, and making it thread-safe had too many constraints. Then a second solution, used now, was designed and consists in keeping all the deoptimiser code in a library completely independent from the rest of the system that cannot be optimised.

\subsection{Recovery mode}

In this first solution to the infinite recursion problem for deoptimisation, we attempted to keep a recovery state of the whole system. More precisely, Scorch kept a copy of the method dictionaries

 Attempt to recovery state with limited portion that cannot be optimised, issues with green threads and complexity, made it independent.

cannot optimie until flag set / reset.

- recovery state (not working with threads)

too difficult to make thread safe because then up until the flag is set we could not optimise hence can't optimise code in process scheduler.

\subsection{Independent library}

- completely independent and forbid deopt / opt

We could opt it statically.

Strong constraints: no dependency at all (duplicate array and dict), debugging is hardly possible.

Example of debugging -> can't log from deoptimiser. 

\subsection{Debugging and runtime modification}

Modifying the deoptimiser while it is running is also more complex than the optimiser. Optimised code is present in the runtime, and any modification of the deoptimiser is

- any change imply existing code cannot be deopt and hard crashes.

\section{Related work}

\subsection{DSL compiled to machine code}

Modification at runtime of the optimiser in our case (Smalltalk + same runtime), though crashes.
Restrictive.

\subsection{Metacircular VMs}

In general this problem is solved, for example with magic or threads (Jalapeno).

\subsection{Graal}

Stack search and deoptimisation in the VM


In chapter \ref{chap:stateOfTheArt}, we introduced multiple metacircular VMs. 

\section{Conclusion}

In this chapter we discussed the main issues existing because the optimiser and the deoptimiser are implemented in Smalltalk and are running in the same runtime and the same native thread than the application they optimise and deoptimise respectively. The main issues are related to infinite recursion. If a frequently used portion of code is detected inside the optimiser code, the optimiser may call itself indefinitely by trying to optimise the portion of code, and while doing so, calling itself. The deoptimiser has the same issue when it needs to deoptimise its own code. In both cases, the program may get stuck.

The optimiser solves this issue by disabling itself when it runs by interrupting the application green thread. In most case, this means that the optimiser cannot optimise its own code. In practice, v-functions too long to optimise within a small time window are postponed in a background green thread, running when the application is in idle. When the optimiser is started through the background green thread, it can optimise itself.

The deoptimiser cannot solve the problem the same way.

 to optimise may  or if the deoptimiser calls itself to deoptimise its code, the program may get stuck. As an optimisation can be cancelled, the optimiser can simply be disabled while it is optimising something. 

%repeat again.

\ifx\wholebook\relax\else
    \end{document}
\fi