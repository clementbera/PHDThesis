\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Metacircular optimising JIT}
\label{chap:metacircular}
\minitoc

%Intro
By design, Scorch's optimiser and deoptimiser are written in Smalltalk and are running in the same runtime than the optimised application. This design leads to multiple problems similar to the ones existing in metacircular virtual machines. 

%single-threaded -> for comparison with Graal and co later
As Pharo is currently single-threaded, it is not possible to run Scorch in a concurrent native thread. To optimise code, Scorch requires either to temporarily interrupt the application green thread or to postpone the optimisation to a background-priority green thread as described in section \ref{sec:optModes}. The deoptimiser cannot however postpone the deoptimisation of a frame as it would block completely the running application. The deoptimiser has necessarily to interrupt the application green thread to deoptimise the stack.

%introduction of the main issue
Hot spots can be detected in any Smalltalk code using conditionnal branches, including Scorch optimiser code itself (as Scorch is written in Smalltalk). When a hot spot is detected in the optimiser code, the optimiser interrupts itself and starts to optimise one of its own function. While doing so, the same hot spot may be detected again before the optimised function is installed, leading the optimiser to interrupt itself repeatedly. 

As Scorch deoptimiser is written in Smalltalk, its code base may get optimised. One of the optimisation-time speculation may be incorrect at runtime, leading the deoptimiser to require the deoptimisation of one of its own frame. In this case, the deoptimiser calls itself on one of its own frame, which may require to deoptimise another frame for the same function, leading the deoptimiser to call itself repeatedly. 

We call the \emph{infinite recursion} problem this issue where the optimiser or the deoptimiser calls itself repeatedly.

%Different constraint so different solutions
The optimiser and deoptimiser have different constraints. It is possible to disable temporarily the optimiser while the application is running. In the worst case, a disabled optimiser leads some functions not to be optimised, but the application keeps running correctly. However, the deoptimiser cannot be disabled temporarily while the application is running as an application requiring deoptimisation cannot continue to execute code until the deoptimisation is performed. As the optimiser and the deoptimiser have different constraints, they need different solutions for the infinite recursion problem.

%Outline and solution
This chapter explains the design used to avoid the infinite recursion issue in both the optimiser and the deoptimiser. The problem is solved in the optimiser by temporarily disabling it in specific circumstances. The deoptimiser solves the problem by using a code base completely independent from the rest of the system that cannot be optimised, hence it never requires to be deoptimised. The last section discuss similar designs in other VMs and compares our solution to other solutions when relevant.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Scorch optimiser}

%Intro, what Scorch optimiser does and critical mode.
Scorch optimiser is activated by the VM when a hot spot is detected. As Pharo is single-threaded, the optimiser is activated by interrupting the application green thread. The optimiser chooses, based on the current stack, a v-function to optimise.  Once the v-function to optimise is chosen, the optimiser gets started in critical mode: it attempts to generate an optimised v-function in a limited time period. If it succeeds, the optimised v-function is installed and used by further calls on the function. If the optimiser fails to generate the optimised v-function in the limited time period, it adds the v-function to a background compilation queue. In any case, the application is then resumed. When the application becomes idle, if the background compilation queue is not empty, Scorch gets activated in background mode. It produces and installs optimised v-functions for each function in the compilation queue without any time limit. 

\subsection{Infinite recursion issue}

%repeat problem for optimiser
As Scorch optimiser is written in Smalltalk, it can theoretically optimise its own code. In practice, if it happens, it may lead to an infinite recursion. Indeed, each time Scorch tries to optimise a function, before reaching the point where it can install the optimised function, it may interrupt itself to start optimising one of its own function. If a hot spot is detected in the optimiser code each time it attempts to optimise anything, then the optimiser never reaches the point where it can install an optimised function.

Figure \ref{fig:InfiniteRecursionOptPb} shows the problem. On the left, in the normal optimisation flow, the application is interrupted when a hot spot is detected. The optimiser generates an optimised v-function, installs it and the application resumes. On the right, in the infinite recursion issue, the application is also interrupted when a hot spot is detected. While the optimiser is generating an optimised function, a hot spot is detected in the optimiser code. The optimiser then restarts to optimise one of its own function, but another hot spot is detected in the optimiser code. The optimiser keeps restarting the optimisation of one of its own function.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.65\linewidth]{InfiniteRecursionOptPb}
        \caption{Infinite recursion problem during optimisation}
        \label{fig:InfiniteRecursionOptPb}
    \end{center}
\end{figure}

This problem has different consequences depending if the optimiser is started in critical or in background mode. In practice, the infinite recursion issue leads to a massive performance loss.

%slow down - critical
\paragraph{Critical mode.} In critical mode, the optimiser has a limited time period to optimise code. If the infinite recursion issue happens, the optimiser spins until the time period ends as shown in figure \ref{fig:InfiniteRecursionOptPbCritical}. The application is then resumed without any optimised function installed. The application gets drastically slower as it gets interrupted for the full critical mode time period without gaining any performance from those interruptions.

\begin{figure}[h!]
    \begin{center}
		\subfigure[Critical mode]{\label{fig:InfiniteRecursionOptPbCritical}\includegraphics[width=0.3\linewidth]{InfiniteRecursionOptPbCritical}}
		\hspace{1cm}
		\subfigure[Background mode]{\label{fig:InfiniteRecursionOptPbBackground}\includegraphics[width=0.3\linewidth]{InfiniteRecursionOptPbBackground}}
		
		\subfigure{\includegraphics[width=0.3\linewidth]{InfiniteRecursionOptPbLegend}}
		\caption{Infinite recursion problem in the two optimisation modes}
    \end{center}
\end{figure}

%slow down - background
%It can conceptually happen that the optimiser starts spinning while searching the stack for a function to optimise. In this case, no function can be added to the background compilation queue as the optimiser has not been able to find a function to optimise in the limited time period. In practice, in our case, the stack search code is quite simple and in practice no hot spot can be detected repeatedly in this code.

\paragraph{Background mode.} When the application becomes idle, the optimiser is started in background mode to optimise functions in the background compilation queue. In this case, the optimiser always successfully generates and installs optimised functions. However, the optimisation of a function is very slow. Indeed, while the optimiser is running in background mode, it activates itself multiple times in critical mode when detecting hot spots in its own code. Each time it happens, the optimiser spins in critical mode for the time period allowed as shown on figure \ref{fig:InfiniteRecursionOptPbBackground}. If the optimiser is started many times on itself during background optimisation, the optimisation of the function may take a significant amount of time. 

Eventually, the optimiser optimise most of its own code correctly through the background mode. Once done, both modes can work correctly as the optimiser cannot be triggered on already optimised code.

The problem is therefore that the application executed gets really slow at start-up because of time wasted spinning in critical mode. Peak performance takes a long time to reach because the optimiser successfully installs code only in background mode or when the infinite recursion issue does not happen in critical mode. 

\subsection{Current solution}

The first solution we implemented is to disable the optimiser when it is running. This way, the optimiser cannot optimise its own code any more and no infinite recursion can happen. This first design has a significant advantage: it is quite simple both conceptually and implementation-wise, while it completely avoids the infinite recursion problem. It has however a major drawback: the optimiser code cannot be optimised at all any more. Of course, the optimiser may use core libraries that can be optimised. For example, the optimiser uses collections such as arrays. If the application optimised is also using the same collections, and it is very likely that an application would use arrays, the array code base may get optimised. Then, the optimiser ends up using an optimised version of arrays. However, the code specific to the optimiser is not optimised.

%call-back removal / addition implementation
To implement our first solution, we changed the VM call-back activating the optimiser to uninstall itself upon activation. When a hot spot is detected but the call-back is not installed, the VM resets the profiling counters which has reached the hot spot threshold and restarts immediately the execution of the application. Then, we changed the optimiser to install back the call-back when it resumes the application, after installing optimised code or adding a function to the background queue. This way, we believed the optimiser would never end up in a situation where it optimises itself, solving entirely the problem.

%can optimise itself through back process
Then, we ran our benchmarks and saw that the problem was solved but the optimiser could still optimise its own code. Our first implementation effectively disabled the optimiser, but only when it was running on critical mode. When hot spots were detected, they were optimised or postponed without any issue as the optimiser disabled itself in critical mode, and the application resumed just fine. When the optimiser was started in background mode, it was not disabled. Hence, in this case, hot spots were detected in the optimiser code and the optimiser was sometimes interrupted by itself in critical mode to optimise its own code.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.4\linewidth]{Disabling}
        \caption{Hot spot detection disabled in critical mode.}
        \label{fig:Disabling}
    \end{center}
\end{figure}

Figure \ref{fig:Disabling} shows the first solution. The application code can be optimised by Scorch, but Scorch cannot optimise its own code when run in critical mode as hot spot detection is disabled. When Scorch is started in background mode, then hot spots are detected (as in any application) and the optimiser code can get optimised.

%1st sol work and good enough 
This first solution is implemented, stable and works fine. Multiple benchmarks run with significant speed-up over the normal VM (This will be detailed in chapter \ref{chap:validation}). In general, in our production VM, simplicity is really important to keep the code base relatively easy to maintain. For each added complexity in the VM we evaluate if the complexity is worth the benefit. This first solution is very valuable to us because it is fairly simple to understand and to maintain. Hence, the optimising JIT may move to production with this design. The next section discusses alternative solutions, which are more complex but allow, at least partially, the optimiser to optimiser its own code.

\subsection{Discussion and advanced solutions}

%but cannot optimise itself in critical mode
The first solution is working but has one major drawback: Scorch optimiser cannot optimise itself in critical mode. Indeed, hot spots detected inside the optimiser in critical mode are completely ignored and the corresponding profiling counters are reset. If the optimiser attempts to optimise code later, it may get confused by some counter values which were reset (basic block usage is incorrectly inferred in this case, in the worst case, a branch may be speculated as unused whereas it is frequently used). 

\paragraph{Decay strategy.} Instead of resetting entirely the counters, we could implement a decay strategy, by for example dividing the current counter values by two. We did not go in this direction because the counters are currently encoded in 16 bits while the hot spot threshold is set to the maximum value. Due to the 16 bits encoding limitation, not completely resetting the counters leads to the detection of many hot spots, repeatedly, without any optimisation happening slowing down the optimiser at start-up. Further analysis in this direction are required to conclude anything.

%the postpone pb while in critical mode - Saving current stack impossible
\paragraph{Partial disabling.} Another naive approach is to postpone the optimisation of to background mode when the infinite recursion issue happens in critical mode. In our design, it is quite difficult to do so. Indeed, when the VM call-back starts the optimiser, it provides only a reification of the current stack as detailled in section \ref{ss:stackSearch}. The optimiser then needs to search the stack to select a function to optimise, and only then it can add a function to optimise to the background compilation queue. 

The stack is modified upon execution and may reference a very large graph of objects, so it is very difficult to save it efficiently for the optimiser to search it later in the background green thread. In addition, as discussed in section \ref{ss:stackSearch}, there is no obvious cheap heuristic to figure out what function is the best to optimise based on the current stack. 

It is however possible, once the optimiser has found what function to optimise, to add it to the background compilation queue. Therefore, we believe that instead of disabling the entire optimiser while it is running in critical mode, we could instead disable it only during the stack searching phase in critical mode and postpone the optimisation to the background green thread instead if the function to optimise has already been found. This way, only hot spots detected during the stack search would be ignored, while the rest of the optimiser would be optimised at the next idle pause. As the stack search phase represents less than 1\% of the optimiser execution time, this approach looks very interesting.

Figure \ref{fig:PartialDisabling} shows the solution proposed. When a hot spot is detected, Scorch is activated on a stack to optimise and starts by searching a function to optimise. During this phasis, the optimiser is disabled to avoid the infinite recursion issue. Once the function to optimise is found, Scorch optimises it. During this phasis, if a hot spot is detected, the optimiser searches a function to optimise and directly adds it to the background compilation queue. Once the function is optimised, the optimised v-function is installed and the application can resume.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.55\linewidth]{PartialDisabling}
        \caption{Partial disabling of the optimiser.}
        \label{fig:PartialDisabling}
    \end{center}
\end{figure}

\paragraph{Ahead-of-time optimisation.} 
Alternatively, we could consider optimising the Scorch optimiser code ahead-of-time.

As the Sista architecture allows to persist optimised code (this is discussed in details in the following chapter), the optimiser code could be optimised ahead of time. The optimised optimiser code would be shipped to production, and no runtime optimisation would happen on the optimiser code in production.

To generate the optimised code ahead-of-time, one way is to preheat the optimiser through warm-up runs. For example, the optimiser can be given a list of well-chosen functions to optimise. This way, all hot spots inside the optimiser would be detected ahead of time and optimised. 

Alternatively, the optimiser's code could be optimised statically by calling itself on its own code, using types inferred from a static type inferencer instead of types inferred from the runtime. This solution has a significant cost in our case as we have to implement and maintain a library to infer types.

%\subsection{Dependencies and optimisations} 

% from 1 old part
%Rephrase -  solution does not forbig Scorch to use optimise libs

% maybe only one sentence ? -> be careful about dependencies
%The first constraint to note when programming Scorch, which may be obvious to the Kernel programmer, is that Scorch cannot depends on any framework or library but the Kernel and Core libraries. Each framework or library in the system relies on the execution engine to perform its code. Scorch is part of the execution engine. Hence, if Scorch relies on an external library and that someone modifies the library, the execution engine may not be stable any more and the runtime completely crashes. In fact, all the Kernel code and Core librairies have similar constraints, they cannot rely on anything to keep the system modular. 

%While writting Scorch, we needed a tool to compress the deoptimisation metadata generated aside from the optimised code. We wanted to use the standard Pharo serializer, Fuel (CITE), but we were not able to do it or further modification on Fuel would break the execution engine.

%In the end, we limited the dependencies of Scorch to the Pharo Kernel and the core collections (exactly: Set, OrderedCollection, Array, ByteArray and Dictionary in addition to the kernel). Any change on one of this dependency may require to change something in Scorch to keep the system running.

%\subsection{Debugging and runtime modification}

%Should I talk about that at all ? I was thinking over a restricted compiling to C but maybe we don't care.

%Maybe rewrite so that formally does not work but in practice it does.

%As any Smalltalk program, it is possible to modify the optimiser while it is running, for example in the debugger. If the modifications leads to incorrect optimiser behavior, then the runtime may crash. To avoid crashes, it may be wise to disable the optimiser while editing it. In practice, this feature is used only by the optimiser implementors. It is very useful to debug the optimiser to understand specific bugs or compiler decisions. With careful understanding of the infrastructure, it is possible in practice to debug the optimiser while it is running and modify its code. The optimiser is set by default to catch all exceptions, failing the optimisation of a specific v-function if an exception was raised. Hence, if the code modification triggers a compile-time exception, the system shall not crash. Unfortunately, in some cases, the optimiser may have silent errors, generating incorrect code without raising exceptions and completely crashing the system.

%The only part of Scorch that cannot really be edited is the deoptimisation metadata generation. Indeed, deoptimisation metadata is also used by the deoptimiser which, as detailled in the following section, has stronger constraints on its code. If one modifies the deoptimisation metadata generated, the deoptimiser may not be able to deoptimise correctly optimised code any more, leading to crashes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%KEEP WRITING FROM HERE

\section{Scorch deoptimiser}

The deoptimiser can be activated in multiple situations. If an optimisation time assumption is invalid at runtime, a deoptimisation guard fails and Cogit triggers a call-back to deoptimise the stack. In addition, multiple development tools in the language, such as the debugging tools, may call the deoptimiser to introspect the stack.

\subsection{Infinite recursion issue}

% repeat problem for deopt
As Scorch deoptimiser is written in Smalltalk, its code base may get optimised. If one of the optimisation-time speculation is incorrect at runtime, the optimised frame requires the deoptimiser to restore the non-optimised stack frames to continue the execution of the program. 

In the case where the deoptimiser functions are optimised, the deoptimiser may call itself on one of its own frame to be able to continue deoptimising code. If at each deoptimisation the deoptimiser calls itself on one of its own frame, the deoptimisation cannot terminate because the deoptimiser needs to call itself again. The application then gets stuck in an infinite loop.

Figure \ref{fig:InfiniteRecursionDeoptPb} shows the problem in the case where the deoptimiser is triggered by a guard failure. On the left, in the normal deoptimisation flow, the application is interrupted when a guard fails. The deoptimiser recreates the non optimised stack frames from the optimised stack frame and edits the stack. The application can then resume with non optimised code. On the right, in the infinite recursion issue, the application is also interrupted when a guard fails. However, while the deoptimiser is deoptimising the stack, another guard fails in the deoptimiser code. The deoptimiser then restarts to deoptimise one of its own frame, but another guard fails in its own code. The deoptimiser keeps restarting the deoptimisation of one of its own frame and the application gets stuck in an infinite loop.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.65\linewidth]{InfiniteRecursionDeoptPb}
        \caption{Infinite recursion problem during deoptimisation}
        \label{fig:InfiniteRecursionDeoptPb}
    \end{center}
\end{figure}

This problem was solved in the optimiser by disabling it temporarily in specific circumstances. The optimiser could be disabled as the execution could simply fall back to non optimised code. The deoptimiser cannot however be disabled or any application green thread requiring deoptimisation would not be able to keep executing code. As the deoptimiser cannot be disabled, it is not possible to solve the infinite recursion problem in the same way than the optimiser. 

To solve this infinite recursion problem, we tried to implement two solutions. The first solution attempts to restore the runtime in a "recovery mode" when recursive deoptimisation happens. In recovery mode, no optimised function can be used (the runtime relies entirely on the v-function interpreter and the baseline JIT). This solution was used successfully for a subset of the current set of benchmarks. However, this solution was not very good for applications and benchmarks using multiple green threads. We then designed and implemented a second solution, that is still in use now. The second solution consists in keeping all the deoptimiser code in a library completely independent from the rest of the system that cannot be optimised.

\subsection{Recovery mode}

As a first attempt to solve the infinite recursion issue for the deoptimiser, Scorch was modified to keep a recovery copy of each method dictionary where optimised v-functions are installed. The recovery copies include only non optimised v-functions. We added a global flag, marking if a deoptimisation is in progress. If the deoptimiser is activated while a deoptimisation is in progress (this can be known thanks to the global flag), the deoptimiser falls back to recovery mode. To do so, the deoptimiser uses the primitive \ct{become:} (described in Section \ref{par:become}) to swap all method dictionaries with their recovery copy and disables the optimiser not to optimise anything in the recovery copies. The deoptimiser can then deoptimise the stack without calling itself repeatedly as it now uses only non optimised functions. Once the stack is deoptimised, the deoptimiser restores the method dictionaries with the optimised v-functions and re-enables the optimiser. 

Figure \ref{fig:InfiniteRecursionDeoptPbRecovery} shows how the recovery mode solves the infinite recursion issue in the deoptimiser. The application is interrupted when a guard fails, and the deoptimiser recreates the non optimised stack frames from the optimised stack frame. If another guard fails in the deoptimiser code, Scorch falls back to recovery mode, and deoptimises its stack frame using only non optimised code. Once done, Scorch deactivates the recovery mode to resume the deoptimisation of the application optimised frame. In the worst case, the deoptimiser may switch multiple times to recovery mode to deoptimise the application frame. One the non optimised stack frames are recreated, the application can resume with non-optimised code.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.6\linewidth]{InfiniteRecursionDeoptPbRecovery}
        \caption{Infinite recursion problem solved with recovery mode}
        \label{fig:InfiniteRecursionDeoptPbRecovery}
    \end{center}
\end{figure}

With this solution, most of the deoptimiser code can be optimised. Indeed, if an infinite recursion happens, Scorch is able to switch to recovery mode and executes correctly the code. Although most of the deoptimiser code can be optimised, not all the code can be. The Smalltalk code executed from the guard failure call-back to the point where recovery mode is activated cannot be optimised. Such code is not protected by the recovery mode and may suffer from the infinite recursion issue. To avoid the problem, we marked a very small list of functions so they cannot be optimised.

\paragraph{Issues.} We were able to run most benchmarks with this solution. However, some benchmarks showed significant slow-down during deoptimisation. Moreover, other benchmarks (the ones using multiple green threads) were crashing. With this solution, we had two major problems. 

The first problem is that switching to recovery mode requires to edit many look-up caches in the VM to use non optimised functions. Each look-up cache entry referencing an optimised function needs to be edited to refer back to the non optimised function. Once the deoptimisation in recovery mode is terminated, the caches need to be updated again to reference the optimised functions. Updating all the caches can take a significant amount of time. In addition, in our implementation, the inline caches are directly written in the native code of each n-function. Each update in the native code requires the processor to partially flush its instruction cache. The recovery mode therefore slows down the application for a short while due to the time spent for the VM to update the caches but also to the cpu instruction cache flush and miss (due to the flush).

The second and main problem is that several of our benchmarks use multiple green threads. In this case, the global flag approach to amrk if a deoptimisation is in progress does not work as multiple deoptimisations may happen concurrently. In a normal application, when such a problem happen, a developer uses semaphores or other green thread management features present in the language to make the code green thread safe. In the case of Pharo, most of the code related to green thread management and scheduling is written in Pharo itself. Using this code to mark if a deoptimisation is in progress in a process would require it to be marked in the list of functions that cannot be optimised, as this is used in-between the guard failure call-back and the activation of the recovery mode. We did not want to disable optimisations on code present in the semaphores and the process scheduler as such code may be performance critical in some applications. Forbidding the optimisation of such code seemed to be too restrictive. We concluded that this approach could not work for our production environment.

\subsection{Independent library}

As a second solution, we designed the whole deoptimiser as a completely independent Smalltalk library. The deoptimiser may use primitives but is not allowed to use any external functions. All the deoptimiser classes are marked: their functions do not have profiling counters and the Scorch optimiser is aware that the optimisation of such functions is not allowed. The deoptimiser code is therefore not optimised at runtime, it is running using only the v-function interpreter and the baseline JIT. As the deoptimiser code cannot be optimised and cannot use any external function that could be optimised, the infinite recursion issue cannot happen.

This solution has multiple important constraints. 

Firstly, the deoptimiser code cannot be optimised at runtime. This constraint is the least important as there are potential solutions. As deoptimisation is uncommon, arguably, not optimising the deoptimiser code is not a problem. In addition, this problem can be partially solved by optimising the deoptimiser code ahead-of-time using Scorch only with optimisations that do not require deoptimisation guards and type information inferred statically. As the library is quite small (500 LoC) and has a very strong invariant (it cannot call any external function), a type inferencer can be very precise and efficient.

Secondly, the deoptimiser code needs to be completely independent. Only primitives can be used directly because any external function may be optimised, potentially leading to the infinite recursion problem. We analysed what other libraires the deoptimiser depended on and remove most of the dependencies by duplicating a bit of code. However, the deoptimiser was relying both on arrays and dictionaries to deoptimise the stack. We create a minimal array and a minimal dictionary as part of the deoptimiser library. The two collection code have now to be maintained in parallel to the core collections.

Lastly, working of the deoptimiser is very tedious. A simple thing such as logging a string in the deoptimiser code require to call a function external to the deoptimiser and may lead to the infinite recursion problem.

Although the constraints are important, we were able to run all our benchmarks with this design. We believe we are going to keep this implementation to move to production, at least on the short term.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}

To have an optimising JIT optimising its own code and encounter the infinite recursion problem we discussed in this chapter, the optimising JIT has to be written in one of the languages it can optimise and run in the same runtime than the optimised application. Such an optimising JIT is not common.

Many production VMs are entirely written in a low-level language such as C++~\cite{V8,Webkit15}. The optimising JIT cannot optimise its own code in such VMs. Other VMs such as the ones written with the RPython toolchain ~\cite{Rigo06a} are written in a language that the optimising JIT could optimise, but the production VMs are compiled ahead-of-time to native code, hence the optimising JIT does not optimise its own code at runtime in the production runtime. Some VMs are metacircular~\cite{Unga05b}(CITE JIKES RVM), which means they are entirely written in a language they can run. However, many metacircular VMs, such as Klein~\cite{Unga05b}, do not feature an optimising JIT. 

Overall, there are two main VMs where the optimising JIT optimises its own code at runtime. On the one hand, there is the Graal compiler~\cite{Oracle13,Dubo13c} which can be used both in the context of the Maxine VM~\cite{Wimm13a} and the Java hotspot VM (CITE). The Graal compiler effectively optimises its own code at runtime as it would optimise the application code. On the other hand, the Jalape\~no VM~\cite{Alp99a}, now called Jikes RVM, features a runtime compiler that can optimise its own code at runtime. 

\subsection{Graal optimising JIT}

The Graal runtime compiler~\cite{Oracle13,Dubo13c} was initially designed and implemented as part of the Maxine VM~\cite{Wimm13a}, a metacircular Java VM. Graal was then extracted from Maxine. It was then integrated in the Java hotspot VM (CITE). Graal can be used in two different ways on top of the hotspot VM. It can be used as an alternative optimising JIT, replacing the Java hotspot optimising JIT written in C++ or it can be used as a special purpose optimising JIT, optimising only specific libraries or application while the rest of the Java runtime is optimised with hotspot optimising JIT. VMs built with Truffle~\cite{Wur13a}, a framework allowing to build efficient VMs by simply writting an AST interpreter in Java, are now running using the Java hotspot VM and the Graal compiler as the optimising JIT.

The most relevant use-case, in our context, is when the Graal compiler is used as an alternative optimising JIT on top of the Java hotspot VM. The interpreter and baseline JIT tiers are in this case present in Java hotspot VM, written in a low level language (C++) and compiled ahead-of-time to native code. In our case, the Pharo interpreter and baseline JIT are also compiled ahead-of-time to native code. The optimising JIT are in both cases written in the language run by the VM (Graal in Java and Scorch in Smalltalk), they can optimise their own code and they need to interface with the existing VM to trigger runtime compilation or to install optimised code.

In the Graal-hotspot runtime, when a hotspot is detected, code in the hotspot VM (written in C++) searches the stack for a function to optimise. Once the function is chosen, Hotspot adds it to a thread-safe compilation queue. The Graal compiler is run in different native threads concurrently to the application native threads. Graal takes functions to optimise from the compilation queue, generates concurrently optimised n-function and hands them over to the hotspot VM for installation. The optimised n-functions handed by Graal to hotspot respect the Graal Java native interface. They include deoptimisation metadata that hot spot is able to understand. When dynamic deoptimisation happens, code written in the hotspot VM (in C++) is responsible for the deoptimisation of the stack.

In our work, Scorch optimiser is able to optimise most of its own code unconditionnally, but the stack search code can be optimised only if the a hot spot is detected while the optimiser is running in background mode. Scorch deoptimiser code cannot be optimised at all. In the case of the Graal-hotspot runtime, the stack search and deoptimisation code is written in C++ and cannot be optimised by Graal. Having the stack search and deoptimisation code in Smalltalk, even if they cannot be optimised normally, allow us to change part of the design such as the deoptimisation metadata without having to recompile the VM. We therefore believe that our approach has an advantage over the Graal-hotspot runtime.

\subsection{Jikes RVM}

Jikes RVM~\cite{Alp99a,Arn00} optimising runtime compiler is written entirely in Java and can optimise Java code. However, it is not currently able to use runtime information to direct its optimisations and do not generate deoptimisation guards, making it not that relevant in our context. The runtime compiler uses however an interesting technique~\cite{Arn00} to choose what function to optimise. Jikes RVM uses an external sampling profiling native thread. Based on the profiling samples, the profiling thread detect what function should be optimised and adds it to a thread-safe compilation queue. The optimising runtime compiler can then start other native threads which take functions to optimise from the compilation queue, optimise and install them. 

The n-function generated by the Jikes RVM baseline JIT do not need profiling counters as profiling is performed using the samples taken by the profiling native thread. The functions to optimise are chosen entirely concurrently. The application is not interrupted, at any time, to search the stack for a function to optimise or to detect a hot spot.

We did not investigate in this direction because our VM is currently single threaded. We do not think it is possible to implement something similar in a single-threaded environment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion.}

In this chapter we discussed the main issue existing because the optimiser and the deoptimiser are implemented in Smalltalk and are running in the same runtime and the same native thread than the application they optimise and deoptimise respectively. The main issue is related to infinite recursion. If a hot spot is detected inside the optimiser code, the optimiser may call itself indefinitely to try to optimise it. The deoptimiser has a similar issue when it needs to deoptimise its own code. In both cases, the program may get slow or gets completely stuck.

The optimiser solves this issue by disabling itself when it runs in critical mode (when it interrupts temporarily the application green thread to perform the optimisation). The optimiser cannot optimise itself directly while running in critical mode, it can only optimise the  application, which may include libraries used both by the application and the optimiser itself. For functions taking a long time to optimise, the optimiser cannot stop the application for too long or the application becomes unresponsive, hence it postpone the optimisation to a background compilation queue where functions are optimised when the application is in idle. When performing optimisations in the background, the optimiser can optimise itself entirely.

The deoptimiser cannot solve the problem the same way as it cannot be disabled at any time or Smalltalk code cannot be executed any more. The deoptimiser avoids the problem by being written using a small number of classes that cannot be optimised nor call any other libraries.

The next chapter explains how the runtime state is persisted across multiple VM start-ups, including the running green threads and the optimised functions.

\ifx\wholebook\relax\else
    \end{document}
\fi