\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Introduction}
\label{chap:intro}
\minitoc

\section{Context}

Object-oriented languages have been one of the most popular programming languages for the past decades. Many high-level object-oriented programming languages run on top of a virtual machine (VM) which provides certain advantages from running directly on the underlying hardware. The main advantage is platform-independence: a program running on top of a VM can run on any processor and operating system supported by the VM without any changes in the program. In this thesis, the term VM is used to discuss about virtual machines for high-level programming languages, as opposed to operating system VMs which are not discussed at all.

\paragraph{High-performance virtual machines.}
%Most high-level languages pursue a strict separation between language-side and VM-side. VMs for instance provide automatic memory management or use platform independent instructions such as bytecodes. These properties allow a programming language to develop independently from the underlying hardware.
High performance VMs, such as Java HotSpot~\cite{Pale01a} or Javascript VMs like V8~\cite{V8} achieve high performance through just-in-time compilation techniques: once the VM has detected that a portion of code is frequently used (called \emph{hot spot}), it recompiles it on-the-fly with speculative optimisations based on previous runs of the code. If usage patterns change and the code is not executed as previously speculated anymore, the VM dynamically deoptimises the execution stack and resumes execution with the unoptimised code.

Such performance techniques allow object-oriented languages to greatly improve their peak performance. However, a warm-up time is required for the VM to correctly speculate about frequently used patterns. This warm-up time can be problematic for different use-cases (distributed applications with short-lived slaves, code in web pages, etc.).

Originally VMs were built in performance oriented low-level programming languages such as C. However, as the VMs were reaching higher and higher performance, the complexity of their code base increased and some VMs started to be written in higher-level languages as an attempt to control complexity. Such VMs were written either in the language run by the VM itself~\cite{Unga05b,Wimm13a,Alp99a} or in domain specific languages compiled to machine code through C~\cite{Rigo06a,Inga97a}.

\paragraph{Existing design: Aosta.}

An optimising JIT design~\cite{Mira02c} for Smalltalk~\cite{Gold83a}, called Aosta (\textbf{A}daptive \textbf{O}ptimisations \textbf{S}mall\textbf{T}alk \textbf{A}rchitecture), emerged in the early 2000s. Aosta was designed by Eliot Miranda with contributions from Paolo Bonzini, Steve Dahl, David Griswold, Urs H\"olzle, Ian Piumarta and David Simmons. The design is the convergence of multiple ideas to ease the development of the optimising JIT, to ensure that the maintainance and evolution cost of the resulting implementation is reasonnable and to attract contributors from the community.

One of the key ideas of Aosta is to split the optimising JIT in two parts, as shown on Figure \ref{fig:ExistingDesign}. The first part, the high-level part, may deal with Smalltalk-specific optimisation and compiles to well-specified platform independent instructions (bytecodes). The second part, the low-level part, can translate such instructions into machine code, performing machine-specific optimisations.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\linewidth]{ExistingDesign}
        \caption{JIT compilation model design}
        \label{fig:ExistingDesign}
    \end{center}
\end{figure}


The high-level part can be written in Smalltalk entirely, in a metacircular style. As most of the Smalltalk community has high skills in Smalltalk but little skills in low-level programming, the design aims here to allow the community to contribute to a project in Smalltalk doing Smalltalk-specific optimisations, improving performance while staying away from low-level details and machine-specific optimisations. 

In addition, the high-level part of the JIT generates platform-independent optimised bytecode methods. Bytecode methods can already be persisted across multiple start-ups in Smalltalk using snapshots. This design allows therefore to persist optimised code, in the form of optimised bytecode methods, to avoid most of the warm-up time present in many modern VMs.

According to other VM implementors\footnote{We discussed with developers from the V8 team.}, it seems that language-specific optimisations (in this case Smalltalk-specific optimisations) are more important than machine-specific optimisations for performance. The Aosta design allows therefore to push most of the complexity from a low-level language, in which the existing Smalltalk VMs are implemented, to Smalltalk.

Another idea is then to reuse the existing baseline JIT, already present in the existing Smalltalk VMs, as the low-level part. Reusing the existing baseline JIT as a back-end for the optimising JIT means there is only one code-base to maintain and evolve for all the low-level aspects of both JITs. To ease the implementation of this design, the interface between the two parts of the optimising JIT is conceived as an extended bytecode set (the existing bytecode set with the addition of new operations used only by optimised code). This way, the existing baseline JIT already supporting the existing bytecode set would "just" needs to be slightly extended to support the new operations.

Some aspects of the design were considered, analyzed and discussed very carefully by several VM experts, making the design attractive and interesting. However, the overall design was incomplete so it was unclear how multiple parts of the system would work, especially, as no one really knew how to design and implement the high-level part of the JIT nor if the design could work.

The work done during the Ph.D started from the Aosta proposal: the goal was to complete the design and propose an implementation. The resulting architecture and implementation, presented in the thesis, is Sista (\textbf{S}peculative \textbf{I}nlining \textbf{S}mall\textbf{T}alk \textbf{A}rchitecture). The design of Sista is largely inspired from Aosta, but a working implementation to validate different aspects of the design and able to run a large suite of benchmarks is provided. Multiple aspects of Sista are different from existing VMs, such as the split in the optimising JIT or the ability to persist optimised bytecode methods across start-ups. The advantages and issues of these differences are discussed in the thesis.

\paragraph{Pharo programming language and community.}

The Aosta proposal was written for Smalltalk. One major feature of the design is the ability to persist bytecode methods across multiple start-ups. In the proposal, bytecode methods are persisted through snapshots: Snapshots allow the program to save the heap (including bytecode methods, which are normal objects in Smalltalk) in a given state, and the virtual machine can resume execution from this snapshot later. We wanted to retain this aspect of the design when working on Sista.

Snapshots are available in multiple object-oriented languages, such as Smalltalk and later Dart \cite{Anna13a}. However, they are widely used mainly in Smalltalk: in the normal development workflow, a Smalltalk programmer uses snapshots to save his code and deployment of production applications is typically done from a snapshot. For this reason, we used Smalltalk for the implementation of the Sista architecture.

As of today, multiple Smalltalk dialects are available, from commercial Smalltalk with expensive licences to open-source versions. In the thesis we focus on the Smalltalk dialect named Pharo~\cite{Blac09a}, a fork of another Smalltalk dialect named Squeak~\cite{Blac07a} made by the original Smalltalk-80 implementors. We picked this dialect for two main reasons. First, both the VM and the language are under the MIT licence, allowing to read, edit and use the code base without any licence cost. Second, the community around Pharo is very active and eager to test and use new features.

In Pharo, everything is an object, including classes, bytecoded versions of methods or processes. It is dynamically-typed and every call is a virtual call. The VM relies on a bytecode interpreter and a baseline just-in-time compiler (JIT) to gain performance. Modern Smalltalk dialects directly inherit from Smalltalk-80~\cite{Gold83a} but have evolved during the past 35 years. For example, real closures and exceptions were added.

As Pharo is evolving, its VM, the Cog VM~\cite{Mira08a}, is improving. For example, a modern memory manager was added over the past few years, improving performance and allowing the VM to use a larger amount of memory. The open-source community is now looking for new directions for VM evolutions, including better VM performance. Compared to many high performance VMs, the Pharo VM is not as efficient because it lacks an optimising JIT with speculative optimisations. The optimising JIT is usually one of the most complex part of high performance VMs. As the Pharo community has a limited amount of ressources to the maintain and evolve the VM, the idea is to design the optimising JIT in a way where open-source contributors can get involved in the maintainance and evolution tasks.

Many people in the community have high skills in object-oriented programming, especially Pharo development, while few people have skills in low-level programming such as assembly code or C. Hence, the community on average understands much more Smalltalk programs than low-level programs. Assuming one is more likely to contribute to a program one can understand, the logical choice is to design the optimising JIT in Smalltalk.

The existing production VM is written in a subset of Smalltalk~\cite{Inga97a}, called Slang, compiling through C to machine code to generate the production VM. Hence, two directions could be taken to write the optimising JIT in Smalltalk. On the one hand, the optimising JIT could be written in Slang, the existing subset of Smalltalk, like the existing VM. On the other hand, it could be written in the complete Smalltalk language, with a design similar to the metacircular VMs~\cite{Unga05b,Wimm13a,Alp99a}. Compared to C and assembly code, Slang tends to abstract away machine concepts to leverage the development experience closer to Smalltalk. However, an important part of the community does not contribute to the VM because its code-base is not available in the base system (it has been compiled to an executable ahead-of-time) and because they do not entirely understand the remaining low-level aspects. For this reason, writting the optimising JIT in the complete Smalltalk language seems to be the best option.

%Removed
%However, Slang was designed primarily for a pure interpreter-based VM. Its first version included the minimum set of features required to have a simple interpreter and memory manager running. Then, to increase performance, the interpreter was improved and a baseline JIT was added. Both new components were more complex than the existing code. As they were added by customer demands requiring short-term delivery, multiple features got incrementally added in Slang, each time making sure the VM could still compile but without entirely rethinking the semantics or re-working the Slang-to-C compiler. As a result, Slang is now quite complex and the addition of new features is now difficult. In fact, the current VM maintainers believe that the VM complexity has reached the limit that Slang can handle without an important redesign of its features. Adding an optimising JIT, more complex than existing code, did not seem to be the right thing to do.

%Removed
%The community considered falling into a modern trend which consists of reusing existing VM for other programming languages to run Pharo to drastically decrease the ressources spent in VM maintenance. For example, the RPython toolchain(CITE) and the Truffle framework(CITE) allows teams to easily build high-performance VM with a limited investment of ressources. This was not the direction taken for four main reasons:
%\begin{itemize}
%	\item Doing so would weaken the VM knowledge in the Pharo community as there will be no VM maintainer in the it (all VM maintainers would be in the used framework community).
%	\item Few people in the community would have the skills to understand and contribute to such a VM due to the language barrier, as the common skills in the community are Smalltalk skills, and none of those frameworks are written in Smalltalk.
%	\item Part of the system would potentially not be understood by anyone understanding both Pharo and the VM, leading to uncommon crashes that no one can understand or solve.
%	\item Pharo have exotic features that may not be supported by such framework. Even if such features are supported, they may be supported without the required performance or they may not be supported in the future as the Pharo community has no major influence on such framework communities.
%\end{itemize}
	
To conclude, the Pharo community is looking for better VM performance and the next step to improve the performance of the existing VM is to add an optimising JIT. %To attract open-source contributors from the community, the optimising JIT has to be written in Smalltalk.



\section{Problem}

The overall direction of the thesis is to prove that Sista, derived from Aosta, is a viable and relevant architecture to write an optimising JIT. Two specific aspects of Sista, the metacircular high-level part of the optimising JIT and the persistence of optimised code across VM start-ups are then analysed carefully.

%Metacircular pb
In the Sista design, the optimising compiler is running in the same runtime as the running application. As the optimising JIT is written in the optimised language, it may be able to optimise its own code. This behavior may lead to strange interactions between multiple parts of the runtime, leading to performance loss or crashes. The Graal compiler~\cite{Dubo13c} has a similar design to what we are trying to build. It can run on top of the Java Hotspot VM as an alternative optimising JIT. However, the development team avoids most of these problems by keeping part of the deoptimisation logic and the stack analysis to determine what method to optimise in the Hotspot VM and not in the Java runtime. Others problems are avoided by running Graal in different native threads than the running application.

%persistence
In most existing VMs, the optimised code is not persisted across multiple start-ups, making difficult the persistence of green threads unless all stack frames present in their execution stack are deoptimised. As we implemented Sista and code was starting to get optimised and executed, we analysed the interaction between optimising JITs and Smalltalk-style \emph{snapshots}. In Smalltalk, a normal programmer regularly takes a snapshot, a memory dump of all the existing objects, to save the running system state. By default, the Smalltalk VM starts-up by resuming execution from a snapshot, restoring all the object states and resuming all running green threads. Each green thread has its own execution stack, which may refer to optimised code. With the bytecode to bytecode optimisation design, the persistence of running green threads, including the persistence of optimised code they refer to, is possible across multiple start-ups.

%VM-language interface
%Based on the existing design, the optimising JIT was implemented in the thesis mostly as a bytecode-to-bytecode optimiser. In this context, an expressive interface between the high-level part of the optimising JIT, doing language-specific optimisations in Smalltalk and the low-level part, doing machine-specific optimisations and written in Slang had to be designed. To ease the development and get results quicker, the interface was designed as an extended bytecode set. This interface is good enough to show performance improvements and very convenient to demonstrate the persistence of optimised code across start-ups. A different interface, more specific for the optimising JIT can be considered to bypass some limitations and may be more expressive. In the Graal compiler, which is as far as we know the project with the closest design, machine code is generated from the language-side optimiser hence the interface between the VM and the language had to be extended very differently.

\paragraph{Research problems.}The thesis focuses on three aspects:
\begin{itemize}
	\item \emph{Optimising JIT architecture:} {What is a good architecture for an optimising JIT running in the same runtime than the optimised application on top of a non-optimising VM?}
	\item \emph{Metacircular optimising JIT:} In the context of an optimising JIT written in the single-threaded language it optimises, can the JIT optimise its own code at runtime and if so, under which constraints?
	\item \emph{Runtime state persistence:} How to persist the runtime state across multiple VM start-ups, including the running green threads and the optimised code?
	%\item \emph{Virtual function instruction set:} How to design an expressive instruction set between to have the high-level part and the low-level part of the optimising JIT, both completely independent and written in different programming languages, communcating efficiently?
\end{itemize}

\paragraph{Supervisors.} In the thesis, I use the term "we" to discuss about my supervisors, the people I worked with and myself. This includes mainly St\'ephane Ducasse, Marcus Denker and Eliot Miranda, but also different persons on specific aspects of Sista.

Implementation-wise, Eliot Miranda\footnote{Eliot Miranda is the maintainer and main implementor of the current production VM.} and I did over 99\% of the work to get Sista working. Section \ref{sec:WorkDistrib} details for each evolution done to the Pharo runtime which one of us did the work. The most complex component of Sista by far is the optimising compiler, which I wrote myself entirely.

Publication-wise, in addition to the authors mentioned in each paper, my research supervisors, St\'ephane Ducasse and Marcus Denker, helped me consistently and reliably to improve the quality of each paper.

\section{Contributions}

The thesis introduces Sista (\textbf{S}peculative \textbf{I}nlining \textbf{S}mall\textbf{T}alk \textbf{A}rchitecture). Sista features an optimising JIT written in Smalltalk running on top of the existing Pharo VM. The optimising JIT is running in the same runtime than the optimised application. Sista is able to persist the runtime state of the program across multiple start-ups. %The communication between the rest of the VM and the optimising JIT is done through a minimal interface.

The main contributions of this thesis are:
\begin{itemize}
	\item An optimising JIT running on top of the existing production virtual machine, showing 1.5x to 5x speed-up in execution time.
	\item A bytecode set solving multiple existing encoding limitations.
	\item A language extension: each object can now be marked as read-only.
	\item A new implementation of closures, both allowing simplifications in existing code and enabling new optimisation possibilities.
\end{itemize}

\section{Outline}

\begin{itemize}
	\item Chapter \ref{chap:stateOfTheArt} defines the terminology and presents existing production and research virtual machines relevant in the context of the thesis. 
	\item Chapter \ref{chap:existing} discusses the existing Pharo runtime as Sista is built on top of it.
	\item Chapter \ref{chap:architecture} details Sista and Chapter \ref{chap:runtimeEvolution} discuss the evolutions done on the Pharo runtime to have the Sista architecture working.
	\item Chapters \ref{chap:metacircular} and \ref{chap:persistence} discuss the architecture in the context of metacircular optimising JITs and the runtime state persistence. 
	\item Chapter \ref{chap:validation} evaluates Sista by comparing the performance of the runtime in multiple contexts, showing that the Sista runtime is going up to 80\% faster than the current production Pharo VM.
	\item Chapter \ref{chap:futureWork} details the future work that could be relevant based on this dissertation.
\end{itemize}

\section{Thesis and published papers}

In the thesis, the focus is on Sista. However, during the Ph.D I worked on other topics, always related to VMs but not necessarily to Sista, leading to publications. I did not detail such work to keep the thesis concise and structured. This section lists all my publications (the publications in parentheses are waiting for approval):

\paragraph{Conferences and journals:}
\begin{enumerate}
	\item Eliot Miranda and Cl\'ement B\'era. A Partial Read Barrier for Efficient Support of Live Object-oriented Programming. In International Symposium on Memory Management, ISMM'15, 2015.
	\item Cl\'ement B\'era, Eliot Miranda, Marcus Denker and St\'ephane Ducasse. Practical Validation of Bytecode to Bytecode JIT Compiler Dynamic Deoptimization. Journal of Object Technology, JOT'16, 2016.
\end{enumerate}	
\begin{enumerate}[label=(\arabic*)]
	\setcounter{enumi}{2}
	\item Nevena Milojkovi\'c, Cl\'ement B\'era, Mohammad Ghafari and Oscar Nierstrasz. Mining Inline Cache Data to Order Inferred Types in Dynamic Languages. Accepted with minor revisions in Science of Computer programming, SCP'17, 2017.
	\item Cl\'ement B\'era, Eliot Miranda, Tim Felgentreff, Marcus Denker and St\'ephane Ducasse. Sista: Saving Optimized Code in Snapshots for Fast Start-Up. Submitted to International Conference on Managed Languages \& Runtimes (ManLang, formerly PPPJ), ManLang'17, 2017.
\end{enumerate}	
\paragraph{Workshops:}
\begin{enumerate}
	\setcounter{enumi}{4}
	\item Cl\'ement B\'era and Marcus Denker. Towards a flexible Pharo Compiler. In International Workshop on Smalltalk Technologies, IWST'13, 2013.
	\item Cl\'ement B\'era and Eliot Miranda. A bytecode set for adaptive optimizations. In International Workshop on Smalltalk Technologies, IWST'14, 2014.
	\item Nevena Milojkovi\'c, Cl\'ement B\'era, Mohammad Ghafari and Oscar Nierstrasz. Inferring Types by Mining Class Usage Frequency from Inline Caches. In International Workshop on Smalltalk Technologies, IWST'16, 2016.
	\item Cl\'ement B\'era. A low Overhead Per Object Write Barrier for the Cog VM. In International Workshop on Smalltalk Technologies, IWST'16, 2016.
\end{enumerate}	
\begin{enumerate}[label=(\arabic*)]
	\setcounter{enumi}{8}
	\item Sophie Kaleba, Cl\'ement B\'era, Alexandre Bergel, and St\'ephane Ducasse. Accurate VM profiler for the Cog VM. Submitted to International Workshop on Smalltalk Technologies, IWST'17, 2017.
\end{enumerate}
%\end{itemize}

\ifx\wholebook\relax\else
    \end{document}
\fi