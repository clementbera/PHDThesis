\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Introduction}
\label{chap:intro}
\minitoc

\section{Context}

One of the most popular family of programming languages in the 21st century is the object-oriented languages family. Many high-level object-oriented programming languages run on top of a virtual machine (VM)~\footnote{In the whole thesis, VM refers to a virtual machine for high-level languages, by opposition to Operating System VMs which are not discussed.} which provides certain advantages from running directly on the underlying hardware. 

\subsection{Virtual machines for high-level programming languages}

Most high-level languages pursue a strict separation between language-side and VM-side. VMs for instance provide automatic memory management or use platform agnostic instructions such as bytecodes. These properties allow a programming language to develop independently from the underlying hardware.

High performance VMs, such as Java HotSpot or current Javascript VMs achieve high performance through just-in-time compilation techniques: once the VM has detected that a portion of code is frequently used, it recompiles it on-the-fly with speculative optimizations based on previous runs of the code. If usage patterns change and the code is not executed as previously speculated anymore, the VM dynamically deoptimizes the execution stack and resumes execution with the unoptimized code.

Such performance techniques allow object-oriented languages to greatly improve their peak performance. However, a warm-up time is required for the VM to speculate correctly about frequently used patterns. This warm-up time can be problematic for multiple different use-cases.

Originally VMs were built in performance oriented low-level programming languages such as C. However, as the VMs were reaching higher and higher performance, the complexity of their code base increased and some VMs started to get written in higher-level languages as an attempt to ease develpment. Such VMs got written either in the language run by the VM itself (cite Klein, jikes, Maxine) or in domain specific languages. (CITE RPYTHON paper, Back to the future)

\subsection{Pharo programming language}

In this thesis the focus is on a specific high-level object-oriented programming language, the Smalltalk dialect named Pharo (CITE PHARO by example). In Pharo, everything is an object, including classes, bytecoded versions of methods or processes. It is dynamically-typed and every call is a virtual call. The VM relies on a bytecode interpreter and a baseline just-in-time compiler (JIT) to gain performance. Modern Smalltalk dialects directly inherit from Smalltalk-80 specified in (CITE Goldberg  Robson 1983) but have evolved during the past 35 years. For example, real closures and exceptions were added.

As Pharo is evolving, the community is looking for better VM performance. Compared to many high performance VMs, the Pharo VM is lacking an optimising JIT with speculative optimisations. In addition, the optimising JIT is one of the most complex part of existing high performance VMs. As the Pharo community has just enough ressources to fund the maintenance and minor evolutions of the existing VM, the optimising JIT had to be designed under two main constraints:
\begin{itemize}
\item The maintenance of the resulting VM has to remain affordable.
\item The resulting VM should be built on top of the existing runtime to reuse as much of existing code as possible.
\end{itemize}

A first design emerged in the early 2000s according to those constraints. The main ideas were:
\begin{itemize}
	\item \emph{Constraint 1:} To build the optimising JIT in the Pharo runtime itself. The existing VM is written in a domain specific language (DSL) compiling to native code through C. As the DSL semantics are very close to C and that most people in the Smalltalk community are either not familiar with C or more productive with Smalltalk than C, this design was seen as a way to reduce the maintenance cost.
	\item \emph{Constraint 2:} To build the optimising JIT as a bytecode to bytecode optimiser and reuse the baseline JIT as a back-end of the optimising JIT to produce efficent machine code. This design would avoid implementing and maintaining two native code back-ends while reusing and extending the existing language-VM interface with the bytecodes, overall lowering the development cost.
\end{itemize}

The design looked interesting but was incomplete so it was unclear how multiple part of the system would work. The thesis started from this proposal and explore multiple aspects of the design that are different from existing VMs, especially their advantages and issues.

\section{Problem}

According to \emph{Constraint 1}, the optimising compiler compiler has to run in the same runtime as the running application. This design cause multiple metacircular issues, for example, if the optimising compiler attempts to optimise itself, the runtime may get stuck in an infinite loop. The Graal compiler (CITE) has a similar design as it runs on top of the Java hotspot VM as an alternative optimising JIT. In this case, as Java is multithreaded, it runs in the same runtime than the running application but in different threads. In Graal though, the development team chose to keep part of the optimising logic, such as the deoptimisation process and the stack analysis to determine what method to optimise in the hotspot VM and not in the Java runtime. 

Due to \emph{Constraint 2}, the optimising JIT was designed in the thesis mostly as a bytecode-to-bytecode optimiser. In this context, a clear interface between the in-language optimising JIT and the rest of the VM had to be designed. One of the goal was to build a small but expressive interface between the two elements. In the Graal compiler, which is one of the closest project, machine code is generated from the Java runtime hence the interface between the VM and the language had to be extended very differently.

As the attempt to implement the proposal was started to work, we analysed the interaction between optimising JITs and Smalltalk-style \emph{snapshots}. In Smalltalk, a normal programmer regularly takes a snapshot, a memory dump of all the existing objects, to save the running system state. The Smalltalk VM normally starts-up by resuming execution from a snapshot, restoring all the object states and resuming all running processes. Each process has its own execution stack, which may refer to optimised code, which is not kept across multiple start-ups in most existing production VMs. With the bytecode to bytecode optimisation design, the persistance of running processes, including the persistance of optimised code refered by such processes, across multiple start-ups became possible.

The thesis focuses on these three problems:

\begin{itemize}
	\item \emph{Problem 1:} How to build an optimising JIT, including its deoptimisation process, in the same runtime as the running application ?
	\item \emph{Problem 2:} How to design a minimal interface between the VM and the language in the context of a language-side bytecode-to-bytecode optimising JIT ?
	\item \emph{Problem 3:} How to persist the runtime state across multiple VM start-up, including the running processes state and the optimised code state ?
\end{itemize}

\section{Contributions}

The main contributions of this thesis are, in the context of the Pharo programming language:
\begin{itemize}
	\item An optimising JIT running on top of the existing production virtual-machine, showing improved performance in execution time.
	\item An alternative bytecode set solving multiple existing encoding limitations.
	\item A language extension: each object can now be marked as a read-only object.
	\item An alternative implementation of closures, both allowing simplifications in existing code and enabling optimisation possibilities.
\end{itemize}

\section{Overview}

The thesis present the \emph{Sista architecture} (\textbf{S}peculative \textbf{I}nlining \textbf{S}mall\textbf{T}alk \textbf{A}rchitecture). The architecture features an optimising JIT running in the same runtime than the optimised application. The communication between the rest of the VM and the optimising JIT is done through a minimal interface. Sista is able to persist the runtime state of the program across multiple start-up.

\section{Terminology}

\subsection{Functions}

In the languages supporting snapshots we refer to, non-tracing JITs are available, we call the compilation unit for the JIT compilers a \emph{function}, which corresponds in practice to a method or a closure. More specifically, we distinguish \emph{virtual functions}, or v-functions, which are understood by a virtual machine (in our case, bytecode version of functions) and \emph{native function}, or n-function, the machine code version of a function understood by a specific processor.

\subsection{Tiered architecture}

A popular architecture for high performance is a tiered architecture. In this context, the first few executions of v-functions are performed by an interpreter. Subsequent executions falls into the JIT infrastructure, composed of multiple tiers. Each tier require more time to compile the v-function than the previous tier, but the resulting n-function is more efficient. In most VMs, there are two JIT compiler tiers. The first tier is called the \emph{baseline JIT}. It translates quickly v-functions to n-functions with a limited number of optimisations. The baseline JIT typically generates n-functions with inline caches to collect type information. The last tier is caled the \emph{optimising JIT}. It translates v-functions to highly optimised n-functions with speculative optimisations.

\subsection{Sista}

\emph{Sista} is the named of the architecture detailled in the thesis. As the architecture has notable difference from the standard tiered architecture, the two runtime compilers are not really a baseline JIT and an optimising JIT. We call them by their project name in the thesis. The first runtime compiler is \emph{Scorch}, which compiles v-functions to v-functions using speculative optimisations. The second one is \emph{Cogit}, the v-function to n-function JIT compiler. Cogit can be used alone as the baseline JIT, or as a back-end for Scorch. In the later case, the pair of Scorch and Cogit form an optimising JIT.

\section{Outline}

In the chapter \ref{chap:stateOfTheArt}, existing production and research virtual machines, relevant in the context of the thesis are presented. Chapter \ref{chap:existing} detail the existing Pharo runtime as Sista is built on top of it. Chapter \ref{chap:architecture} details the Sista architecture. Chapters \ref{chap:metacircular}, \ref{chap:persistance} and \ref{chap:interface} evaluates the architecture in the context of the three problems of the thesis.






\ifx\wholebook\relax\else
    \end{document}
\fi