\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Runtime evolutions}
\label{chap:runtimeEvolution}
\minitoc

%Generic intro
To support the architecture described in the previous chapter, the Pharo runtime had to evolve. We distinguish two kind of evolutions. Some evolutions were required to support the architecture, the Sista runtime could not work without those features. Other evolutions were not mandatory, Sista could have worked without these features, but each of them were important to improve the overall performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Required language evolutions}

Five major evolutions were required to have Sista up and running:
\begin{enumerate}
	\item Cogit was extended to detect hot spots through profiling counters.
	\item The interpreter and Cogit were extended to be able to execute and compile the additional instructions of the extended bytecode set.
	\item Two VM call-backs were added to trigger Scorch when a hot spot is detected or a guard fails.
	\item A new primitive was introduced to provide runtime information in Smalltalk for v-functions having a corresponding n-function generated by Cogit.
	\item Scorch, including the optimiser, the deoptimiser and the dependency manager were introduced.
\end{enumerate}

\subsection{Profiling counters}

To detect hot spots, Cogit was extended to be able to generate profiling counters when generating a n-function. When the execution flow reaches a counter, it increases its value by one. If the counter reaches a threshold, the VM triggers a special call-back to activate Scorch. Profiling counters induce overhead, which can be significant enough to be seen in some benchmarks (the overhead is detailed on a set of benchmarks in the validation chapter, Chapter \ref{chap:validation}). To avoid the overhead in optimised code, Cogit was extended to support conditional compilation. Based on a bit in a v-function's header, Cogit generates a n-function with or without profiling counters.

Based on \cite{Arn02}, we added profiling counters on conditional branches, with one counter just before and one counter just after the conditional branch. This strategy allows the VM to provide basic block usage information in addition to the detection of hot spots. Every finite loop requires a branch to stop the loop iteration and most recursive code requires a branch to stop the recursion, so the main cases for which we wanted to detect hot spots for are covered. Each time the execution flow reaches a conditional branch in a n-function, it increases the profiling counter by one, compares the counter value to a threshold and jumps to the hot spot detection routine if the threshold is reached. If the threshold is not reached, the conditional branch is performed. If the branch is not taken, a second counter is incremented by one to provide later basic block usage information.

The main issue we had to deal with when implementing profiling counters is the location of the counters and the access to the counters. Indeed, in our first naive implementation, the counter values were directly inlined in the native code. That was a terrible idea as every write near executable code flushes part of the processor instruction cache, leading to horrible performance. In the end, we changed the logic to allocate a pinned unsigned 32-bits integer array\footnote{A pinned object is an object that can never be moved in memory. For example, the garbage collector cannot move it.} for each n-function requiring counters. The pinned array is on heap, far from executable code, and contains all the counter values. As the array is pinned, the native code can access the array and each of its fields (each counter) through a constant address. This is very nice as the native code can be efficient by using constant addresses and the n-function does not require any metadata\footnote{References to non pinned objects from n-function normally require metadata to update the reference when the object is moved in memory, typically by the garbage collector.}.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\linewidth]{ProfilingCounters}
        \caption{Unoptimised n-function with two profiling counters}
        \label{fig:ProfilingCounters}
    \end{center}
\end{figure}

Figure \ref{fig:ProfilingCounters} shows a n-function with two profiling counters. The n-function is present in the n-function zone, which is readable, writable and executable. The n-function zone is exclusively modified by Cogit. The pinned array is allocated on heap, where all objects are present, which is a readable and writable (but not executable) zone. The n-function is composed of a header, which encodes different properties of the n-function such as its size, the n-function native code and metadata to be able to introspect the n-function. 

As the n-function from Figure \ref{fig:ProfilingCounters} requires two counters, an array with two 32 bits wide fields is allocated on heap. Each 32 bits counter field is split in two. The high 16 bits are used for the counter just before the conditional branch, precising how many times the branch has been reached. The low 16 bits are used for the counter just after the branch, precising how many times the branch was not taken. The native code has direct references to the counter addresses. The n-function header has also a reference to the pinned array, not shown on the figure to avoid confusion, which is used to reclaim the pinned array memory when the n-function is garbage collected. In the example, we can see that the first branch is always taken (the counter increased when the branch is not taken is at 0 while the branch has been reached 61285 times).

\subsection{Extended bytecode set}

Our architecture requires an extended bytecode set to support all the new operations permitted only in optimised v-functions. The new operations were described in one of our paper~\cite{Bera14a}. The extended bytecode set design relies on the assumption that only a small number of new instructions are needed for Cogit to produce efficient machine code. Four main kind of instructions were introduced:
\begin{itemize}
\item \textbf{Guards}: Guards are used to ensure an optimisation-time assumption is valid at runtime. If the guard fails at runtime, the dynamic deoptimisation routine is triggered.
\item \textbf{Object unchecked accesses}: Normally variable-sized objects such as arrays or byte arrays require bound checks to allow a program to access their fields. Unchecked access directly reads the field of an object without any checks. Instructions to access the size of a variable-sized object without any checks are included.
\item \textbf{Unchecked arithmetics}: Arithmetic operations need to check for the operand types to know what arithmetic operation to execute (integer operation, double operation, etc.). Unchecked operations are typed and do not need these check. In addition, unchecked operations do not perform any overflow check.
\item \textbf{Unchecked object allocation and stores}: Normal object allocations do many different things in addition to memory allocation, such as the initialization of all fields to \ct{nil} which is not needed if all fields are set immediately after to other values. Normal stores into objects go through a write barrier to make sure that the store does not break any garbage collector invariant. Such a write barrier can be ignored in specific cases, for example when doing a store in an object that has just been allocated, because it is guaranteed to be in the young object space.
\end{itemize}

As discussed in the previous chapter, optimised v-functions may be interpreted but we made sure that their interpretation is very uncommon. We designed the unsafe operations to be efficient when an optimised n-function is generated and executed. We did not design the unsafe operations for efficient interpretation. 

Designing the operations for efficient native code generation is quite different from designing them for efficient interpretation. For efficient native code generation, we encoded the unsafe operations in multiple bytes to be able to provide extra information to Cogit on how to produce good native code for the instruction. This strategy is not very good for the interpreter as it needs additional time to decode the multiple bytes. A good design for efficient interpretation would have been to encode performance critical instructions in a single byte.

%As the optimised methods are represented as bytecodes, they can potentially be executed by the VM interpreter. However, as discussed in the previous chapter, we made sure that it was very uncommon. Improving the performance to speed-up the bytecode interpreter or to speed-up the machine code generated using Cogit are two different tasks that may conflict with one another. We designed the extended bytecode set so the machine code generated by Cogit is as efficient as possible, not really considering the speed of the interpretation of such v-functions.

\subsection{Call-backs}

As Scorch optimiser and deoptimiser are written in Pharo while the rest of the VM is written in Slang, the VM needs a special way to activate them. 

Pharo has an array of registered objects which can be accessed both from the VM and the language, as described in Section \ref{par:regObjects}. We registered two new selectors. One is activated by the VM when a hot spot is detected in an unoptimised n-function. The other one is activated when a guard failed in an optimised n-function. A method is implemented in Smalltalk for each selector in the class used for reified stack frames. In our case, these methods activate respectively Scorch optimiser or Scorch deoptimiser.

\subsection{Machine code introspection}

%what does the primitive + intro
To extract runtime information from a n-function, we added a new primitive method called \emph{sendAndBranchData}. SendAndBranchData is activated with no arguments and fails if the receiver is not a v-function. If the v-function was compiled by Cogit and therefore has an associated n-function, the primitive answers runtime information present for the function, if the v-function was not compiled, the primitive fails. The runtime information includes types and functions met in each inline cache and the profiling counter values. This information can be then used by Scorch to speculate on types and basic block usage. 

%reading the cache/counter through 
Cogit had an introspection API used for multiple features such as inline cache relinking or debugging. The cache and counter values are read by the implementation of a small extension on top of Cogit API for introspection. In unoptimised n-functions, Cogit generates an inline cache for each virtual call~\cite{Deut84a,Holz91a}, relinked at runtime each time a new receiver type is met for the call. The caches can be read using low-level platform-specific code, similar to the code used for relinking. Cogit generates for each profiling counter a call to the hot spot detection Slang routine, used to trigger the hot spot detection routine. This call is annotated with the corresponding virtual instruction pointer, allowing Scorch to know to which conditional branch in the v-function each profiling counters correspond to.

%Result of primitive
The new primitive iterates over the n-function, collecting for each virtual call and each conditional branch the virtual instruction pointer as well as respectively type and profiling information. Because the data is collected from Slang, it's not convenient to build complex data structures using multiple different kind of objects (Each object's class internal representation would need to be specifically known by both the VM and Scorch). To keep things simple, the primitive answers an array of arrays, each inner array containing the virtual program counter of the instruction, and a list of types and v-functions targetted by the inline cache or the number of times each branch has been taken.

\subsection{Scorch}

In the implementation of our architecture, the bulk of the work is the design and the implementation of Scorch optimiser. The thesis is centered around Sista in general and there are no big innovative features in Scorch.

Scorch optimiser is implemented as a traditional optimising compiler. It translates the v-functions to optimise into a single static assignment intermediate representation, represented as a control flow graph. Conditional branch and send instructions are annotated with the runtime information provided by Cogit to speculate on what method to inline and on basic block usage.

The optimisations performed are very similar to the ones performed in other optimising JITs such as V8 Crankshaft optimising JIT~\cite{V8}. Scorch starts by speculating on types to inline other functions. Guards are inserted to ensure speculations are valid at runtime. Once inlining is done, Scorch performs multiple standard optimisations such as array-bounds check elimination with the ABCD algorithm~\cite{Bodi00a} or global value numbering. Scorch also attempts to postpone allocation of objects not escaping the optimised v-function from runtime to deoptimisation time (or completely removes the allocation if the object is never required for deoptimisation).

Scorch back-end generates an optimised v-function from the intermediate representation. The phi instructions from single static assignment are removed, using instead temporary locations assigned multiple times. For each instruction, Scorch determines if the computed value needs to be stored to be reused, and if so, if it can be stored as a spilled value on stack or as a temporary variable. The deoptimisation metadata is attached to the optimised v-function to be able to restore multiple frames.

Scorch deoptimiser is much simpler than the optimiser. It reads the deoptimisation metadata for a given program counter in the optimised v-function. The metadata consists of a list of objects to reconstruct, including closures and reified stack frames. The objects are reconstructed by reading constant values in the metadata or reading the optimised stack frame values. Once the objects are reconstructed, execution can resume in the restored bottom frame.

Scorch dependency manager is also much simpler than the optimiser. It keeps track for each optimised function of the list of selectors it depends on. If a method with one of these selectors is installed, all the optimised functions dependant are discarded. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optional language evolutions}


Five major evolutions were introduced in the language in addition to the required evolutions to allow Scorch to produce more efficient optimised v-functions:
\begin{enumerate}
	\item A new memory manager for efficient n-function generation.
	\item A new bytecode set to leverage encoding limitations.
	\item A register allocation algorithm in Cogit.
	\item A write barrier feature to be able to mark objects as read-only.
	\item A new closure implementation to be able to optimise closure more efficiently.
\end{enumerate}

\subsection{New memory manager}

%pb with existing representation
The first version of Sista was built on the existing VM with a minimum number of modifications. One of the main issues met was related to the memory representation of objects. The existing memory manager was designed and implemented before the implementation of Cogit for a pure interpreter VM~\cite{Inga97a}. The representation of objects did not allow Cogit to produce efficient accesses to object fields in native code.

%existing class access
One problem for example was the encoding of the class field in an object. It could be encoded in three different ways:
\begin{itemize}
	\item \emph{Immediate classes:} A very limited subset of classes, included \ct{SmallInteger}, have their instances encoded in the pointer to the object itself. As all objects are aligned in memory for efficient access to their fields, the last few bits (the exact number depends on the alignment) of a pointer to an object are never set. By setting some of these last few bits, the memory manager can encode a class identifier. \ct{SmallInteger} for example are encoded by setting the last bit of the pointer.
	\item \emph{Compact classes:} A limited set of classes, up to 15 classes, had their instances encoding their classes as an index in a 4-bits field in the first word of the object's header. The memory manager had access to an array mapping the indexes to the actual classes.
	\item \emph{Other classes:} All the other instances encoded their class as a pointer to the class object, encoded in an extra pointer-sized field in the header of the object.
\end{itemize}

%existing class access pb in n-functions
In practice, Cogit compiles many type-checks. In unoptimised n-functions, type-checks are generated mainly in inline caches. In optimised functions, type-checks are generated for deoptimisation guards. For each type-check, the native code generated needed three paths to find out which one of the three encodings was used for the instance which was type-checked, to finally compare it against the expected type. In addition, as many instances encoded their class as a pointer to the class object while class objects can be moved by the garbage collector in memory, cogit needed to annotate the expected type to correctly update the pointer value during garbage collection. Overall both the generated n-function and the garbage collector were slowed by the memory representation.

%Spur and problem solving
To solve this problem, a new memory manager was implemented and deployed in production~\cite{Mir15a}. The new representation of objects in memory allows the generation of very efficient n-functions. For example, there is now only two ways for an instance to access its class, the class is either immediate or compact. Compact class indexes are stored in the instances in a 22-bit fields, allowing over four millions different concrete classes. Cogit does not need any more to annotate type-checks when generating the n-function as an indirection index is referred instead of the class object.

%other pbs soled
In addition to the generation of efficient n-functions, other problems non directly related to the thesis were present in the existing memory manager (poor support for large heaps, slow scavenges, etc.) which were solved with the new memory manager.

% pinned objects ??? I don't know if it's worth going into details there.

\subsection{New bytecode set}

The existing Pharo bytecode set had multiple encoding limitations~\cite{Bera14a}. For example, jumps (forward, backward and conditonnal) were able to jump over 1024 bytes at most. Such limitations are very rarely a problem while compiling normal Smalltalk code due to coding convention encouraging developers to write small functions. However, the optimised function produced by Scorch includes many inlined functions and in some case the limitations were a problem. As the bytecode set already needed to be extended to support the new unsafe operations, we designed a complete new bytecode set instead of just adding the new operations to leverage encoding limitations.

\subsection{Register allocation}

To allocate registers, Cogit simulates the stack state during compilation. When reaching an instruction using values on stack, Cogit uses a dynamic template scheme to generate native instructions. The simulated stack provides information such as which values are constants or already in registers. Based on this information, Cogit picks one of the available templates for the instruction, uses a linear scan algorithm to allocate registers that do not need to be fixed into specific concrete registers and generates native instructions.

The existing linear scan algorithm was very naive and limited. It was very efficient because registers are not live across certain instructions that are very common in unoptimised code. Specifically, registers cannot live across these three instructions:
\begin{enumerate}
	\item \emph{virtual calls:} All registers are caller-saved.
	\item \emph{backjumps:} Backjumps are interrupt points.
	\item \emph{Conditional branches:} If the branch is on a non-boolean, a slow path is taken to handle the case requiring to spill the registers.
\end{enumerate}

However, these instructions are not that common in optimised code. Most virtual calls are inlined. Some backjumps are annotated not to require an interrupt check. Some conditional branches are removed because one branch has never been used and others are annotated as branching on a value which is guaranteed to be a boolean. Registers can therefore stay live across many more instructions and the register allocation algorithm has more impact on native code quality.

We wrote a new linear scan register algorithm, performing better under register pressure. The most difficult part is to correctly keep registers live across conditional branches. At each control flow merge point, the register state has to be the same in both branches or Cogit needs to generate additional instructions to spill or move registers.

\subsection{Read-only objects}

One of the main problem encountered while trying to improve the performance of the optimised v-functions generated by Scorch was literal mutability. In most programming languages, if the program executes a simple double\footnote{We use \emph{double} to discuss double-precision floating-point in this paragraph.} addition between two double constants the compiler can compute at compile-time the result. In Pharo, as literals are mutable, one of the double constant may be accessed through reflective APIs and mutated into another double, invalidating the compile-time result. 

To solve the problem, we introduced read-only objects. With this feature, a program can mark any object as read-only. Such read-only objects cannot be modified unless the program explicitly reverts them to a writable state. Any attempt to modify a read-only object triggers a specific call-back in Smalltalk, similarly to the hot spot detection and guard failure call-backs. The modification failure routine can, for example, revert the object to a writable state, perform the modification and notify a list of subscribers that the modification happened. This feature was introduced with limited overhead to the existing runtime~\cite{Bera16b}. 

In Sista, literals are read-only objects by default. Any attempt to modify a read-only literal is caught by the runtime and Scorch is notified. If the literal was used for compile-time computation, corresponding optimised v-functions are discarded. Thanks to this technique, traditional compiler optimisations can be applied to Smalltalk.

\subsection{Closure implementation}

Another important problem encountered when implementing Scorch was related to the implementation of closures. The existing closures were implemented in a way that the closure's v-functions were inlined into their enclosing v-function. This led to multiple problems as it was difficult to optimise a v-function without having to rewrite the v-functions of all the closures that could be instantiated inside the v-function. This was increasing the complexity of the optimiser and required very expensive object manipulation at deoptimisation time to correctly remap all the v-functions of the closures created inside an optimised function. The closure implementation was also complicating the code of Cogit: Cogit could not compile a virtual method without compiling all the virtual functions of the closures the virtual method could compile. This induced significant complexity also to introspect n-functions and to activate closures. 

To solve these issues, we designed a new closure implementation. In the new implementation, closures have v-functions separated from their enclosing environment v-functions. Scorch can optimise independently methods and closures. We were able to reduce the complexity of Cogit, both when it is used as a baseline JIT and as a back-end for Scorch.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.85\linewidth]{CompiledBlock}
        \caption{Old and new closure representation}
        \label{fig:CompiledBlock}
    \end{center}
\end{figure}

Figure \ref{fig:CompiledBlock} shows on the left the old closure function representation and on the right the new representation. In the old representation, the closure function literals and bytecodes are present inside the enclosing function. The creation of a closure in the v-function interpreter requires to jump over the closure instructions. All the function metadata, normally present in the function header, can be computed by disassembling the closure creation instruction or the closure bytecodes. Any change on the closure function impacts the enclosing function and any change on the enclosing function impacts the closure function. When Cogit compiles the v-function to n-function, it compiles both the function and all the inner closure functions at the same time. In the new representation, the two functions are independent. Each function has its own function header. Both functions can be changed independently. Cogit compiles separatedly each v-function to n-function. The closure function has no source pointer as it can be fetched from the enclosing function (the last literal of the compiled block).

\section*{Conclusion}

This chapter described the runtime evolutions required in the Pharo runtime to support Sista. The next chapter discusses how Scorch is able to optimise its own code and under which constraints.

\ifx\wholebook\relax\else
    \end{document}
\fi