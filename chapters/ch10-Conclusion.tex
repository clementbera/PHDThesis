\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Future work and conclusion}
\label{chap:conclusion}
\minitoc

Maybe future work, discussion

Future work
- productize
- make application of hot start-up on a use case (for ex, amazon lambda, android) and evaluate
- Discussion energy cant be evaluated for other reasons (poor event management...) pre-opt as in android, distib app with amazon lambda and co

Other future work:


\subsection{Handling exotic Smalltalk operations}

Smalltalk provides some operations that are not typically available in other object-oriented languages.  We call them \emph{exotic operations}. These operations are problematic for the optimizer. We provide examples for
these exotic operations and discuss how the system can handle them.

\subparagraph{Become.} One operation is called \emph{become}. It allows an object to swap identity with another one, \ie if an object \emph{a} becomes an object \emph{b}, all the references to \emph{a} now refer to \emph{b} and all the references to \emph{b} refer to \emph{a}. This operation was made efficient using different strategies described in \cite{Mir15a}. This feature has some implications in the context of the runtime optimizer. At any interrupt point, there could be a process switch and from the other process, any of the temporary variable of the optimized stack frame could be changed to any object in the heap. This would invalidate all assumptions taken by the optimizer.

\subparagraph{Heavy stack manipulation.} The other exotic operations are related to stack manipulation. Smalltalk reifies the call stack and allows the program not only to reflect on the call stack, but also to manipulate it. We discussed this in in \secref{Context} when we explained that for example the developer can set the caller of any stack frame to another frame.

\subparagraph{Current Solution: Deoptimization for exotic operations.} All these operations are uncommon in a normal Smalltalk program at runtime.  They are usually used for implementing the debugging functionality of the language. Currently, profiling production applications does not show that we would earn noticeable performance if we would optimize such cases. The solution therefore is to to always deoptimize the stack frames involved when such an operation happens. In the case of become, if a temporary variable in a stack frame executing an optimized method is edited, we deoptimize the frame. In the case of the stack manipulation, if the reification of the stack is mutated from the language, we deoptimize the corresponding mutated stack frames.

\subparagraph{Future Work: Optimizing exotic operations.} It could be possible to have the runtime optimizer aware of these features and to handle them specifically. 
In fact, optimizing the stack manipulation would be similar to the optimization of exceptions.  \cite{Ogas01a}.  %It has been done in the Java hotspot virtual machine and has proven to improve the performance of exception intensive libraries by 20\% 

\subsection{Platform-dependency and Snapshots}
In the case of Smalltalk, snapshots are independent of the processor and the OS used. It is proven as the same snapshot can be deployed for example on x86, ARMv5 and Windows or Linux. However, Smalltalk snapshots are dependent on the machine word size: 32 bit or 64 bit snapshots are not compatible. They are not compatible because the size of managed pointer is different, but also because the representation of specific objects, such as numbers, is different. It is however possible to convert offline a 32 bit snapshot to 64 bit and vice-versa. 

As some optimizations related to number arithmetics, such as overflow checks elimination, depends on the number representations, the current optimizing compiler also adds some dependencies to the machine word size. A fully portable solution would either need not to do optimizations on machine word specific number representations or de-optimize the affected code on startup.

\subsection{Limitation of the stack-based IR}

The bytecoded function (optimized or not) are encoded in a stack-based representation. This can be seen as a problem as it is very difficult to do the optimizations passes on a stack-based IR. To avoid this problem, the optimizer decompiles the bytecode to a non stack-based SSA IR. This implies that the optimizer looses time to translate the bytecode to its IR, and then its IR back to the extended bytecode. The latter is questionable as the optimizer IR has more information than the generated bytecode (for example, it knows the liveness of each SSA value). Information lost here could be profitable for low level optimization such as register allocation and instruction selection.

A possible future work  is to design a better representation for bytecoded functions, especially the optimized ones. %Adding all the instructions required to have an hybrid stack and register based bytecode set did not feel like a good idea as for this design to be efficient the number of new instructions in the extended bytecode set has to be limited. 

We have not invested yet in that direction as we believe that low level machine specific optimizations do not earn a lot of performance for high level languages such as Smalltalk compared to language-specific optimizations. Our belief is based on the optimizing compiler Crankshaft, Javascript V8~\cite{V8} previous optimizing compiler, which is doing very little low level optimizations and is performing very well. Our back-end uses only a few simple heuristic for instruction selection and a simple linear scan algorithm for register allocation. 

\subsection{Optimizer}

We chose to implement the runtime compiler from bytecoded functions to optimized bytecoded functions in Smalltalk instead of C as the rest of the VM. We made this decision because our engineering is more productive in high-level language such as Smalltalk compared to low-level languages such as C. The optimizer is running in the same runtime as the application.

\subparagraph{Pros.} There were good points in our experience, as for example we could use all the Smalltalk IDE tools and debug the optimizing compiler while it was optimizing a method in the active runtime. Using Smalltalk allows to ignore all the memory management constraints that we have in C.

\subparagraph{Cons.} However, there are some drawbacks. 

Firstly, the runtime now depends on each library the optimizer uses. For example, if one decides to use a specific collection in the runtime optimizer, then editing the collection implementation may break the optimizer compiler and crash the runtime. Hence, we chose to limit as much as possible the dependencies of the runtime compiler, to a minimal part of the Smalltalk kernel. Programming the optimizer is therefore quite different from normal Smalltalk development as we have to keep as few dependencies as possible.

Secondly, the language has now access to both the optimized and non optimized state of each function activation. When the programmer now accesses the reification of a stack frame, depending on the state of optimization, an optimized function activation might be shown. We are adapting the debugging tools to request function activation to be deoptimized when needed. In fact, we are adding an IDE settings: the developer may or may not want to see the stack internals, depending on what he wants to implement. When programming normal applications, the developer usually does not want to see the optimized code, but when programming the optimizing compiler itself, the developer usually wants to see it.

\subsection{Process and snapshots.}

In the case of Smalltalk, processes are persisted in the snapshot. For example, if a snapshot is taken while some code displays an animation, restarting the VM using the snapshot will resume the animation at the exact same point where it was when the snapshot was taken. To persist a process, the Smalltalk runtime has to persist all the execution stacks. 

In a classical JIT compilation approach, only machine code versions of optimized functions are available and stack frames refer to them. As it is very difficult to save directly the machine code version of the method in the snapshot (because of platform-dependency and position-dependent code for example), persisting stack frames referring to optimized functions is problematic. Optimized functions are generated in a non deterministic way as the optimizing compiler depends on runtime type information, so it is not trivial to recreate them at start-up.

Persisting processes is difficult in classical JIT compiler. Our architecture solves that problem by allowing to persist bytecoded versions of optimized functions. In our case, the VM persists processes by mapping all machine code state of stack frames to bytecode interpreter state, and then persist all the stack frames in their reified form.

\subsection{Memory footprint}

Usually when dealing with speculative optimizations in JIT compilers, one evaluates the memory footprint taken by the deoptimization metadata. That evaluation would be really interesting in our context as the metadata is split in two parts:
\begin{itemize}
\item A part next to the machine code version of the method to map machine state to bytecode interpreter state.
\item A part in the literal frame of the bytecoded optimized function to map the optimized stack frame to non optimized stack frames.
\end{itemize}

Does the split implies a larger memory footprint, and, if so, how much bigger is the memory footprint ? In our implementation, we have kept the metadata almost uncompressed (We used a very naive compression algorithm). Working on an efficient serializer to compress this metadata and an analysis of memory usage is future work.





------------------------------
For the end in future work.----

\ifx\wholebook\relax\else
    \end{document}
\fi