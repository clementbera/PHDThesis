\ifx\wholebook\relax\else

% --------------------------------------------
% Lulu:

    \documentclass[a4paper,12pt,twoside]{../includes/ThesisStyle}

	\input{../includes/macros}
	\input{../includes/formatAndDefs}

	\graphicspath{{.}{../figures/}}
	\begin{document}
\fi

\chapter{Future work}
\label{chap:futureWork}
\minitoc

This Chapter discusses future work related to Sista. The focus is on research-oriented future work, but the first two sections also mention engineering-oriented future work (for instance, moving the architecture to production).

Section \ref{sec:archEvo} describes the evolution planned or considered for the overall architecture. Section \ref{sec:newOpt} discusses specifically the optimisations that could be implemented in Scorch and Cogit. Section \ref{sec:useCase} details how the application of Sista on a real-world application requiring quick start-ups would be very valuable to validate further the architecture. Lastly, Section \ref{sec:energy} discusses briefly the potentially low energy consumption of the Sista VM and how it could be valuable in specific use-cases.

\section{Architecture evolution}
\label{sec:archEvo}

This section details the evolution of the Sista architecture that are worth investigating. Section \ref{ss:FWInterface} describes the potential evolution of the interface VM-language: Sista is currently using an extended bytecode set to communicate between Scorch and Cogit, another representation may be better. Section \ref{ss:FWIDE} explains the on-going work to integrate Sista with the development tools. The evaluation of the memory footprint used by the Sista runtime is precised in Section \ref{ss:FWMemFootprint}. Section \ref{ss:FWPlatDep} discusses the current platform-dependencies of the persistence of the runtime state across start-ups and how these dependencies could be avoided. Section \ref{ss:FWProduct} briefly states the on-going work to move Sista to production.

\subsection{Interface VM-language}
\label{ss:FWInterface}

\paragraph{Stack-based or register-based IR.}The v-functions discussed in the thesis are currently encoded in a stack-based bytecode Intermediate Representation (IR). Stack-based IRs are usually considered as difficult to deal with in optimising compilers, so this can be seen as a problem. For this reason, Scorch decompiles the v-function to a register-based IR (similar to TAC\footnote{Three Address Code.}, but some operations such as virtual calls may have more than three parameters), performs the optimisations and translates it back to the stack-based bytecode. Cogit then takes the stack-based bytecode as input, and translates it to a register-based IR to generate the n-function. 

The stack-based extended bytecode sets has two main issues:
\begin{itemize}
	\item \textbf{Loss of information:} Scorch IR has significant information about the instructions (liveness, uses) than the stack-based bytecode. This information is lost during the translation to the stack-based bytecode, while it may be valuable for Cogit to perform efficiently low-level optimisations.
	\item \textbf{redundant conversion:} When compiling using Sista's optimising JIT, unoptimised v-functions are compiled by Scorch to optimised v-functions and then by Cogit to optimised n-functions. It feels a bit redundant to take a stack-based IR (bytecode of the v-function), translate it to a register-based IR (Scorch IR), then translate it back to a stack-based IR (extended bytecode of the optimised v-function) and lastly translate it to a register-based IR (Cogit's IR), as shown in the left part of Figure \ref{fig:FutureWorkredundant}. One could consider encoding optimised v-functions in a register-based IR, to avoid two translations.
\end{itemize}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.7\linewidth]{FutureWorkredondant}
        \caption{redundant conversion}
        \label{fig:FutureWorkredundant}
    \end{center}
\end{figure}

However, the extended bytecode set was designed stack-based for two relevant reasons:
\begin{itemize}
	\item \textbf{Compatibility:} The existing bytecode set is stack-based, and having the extended bytecode set stack-based allows us to generate optimised functions using existing instructions and not only new ones. This was very convenient to have quickly a working version of Sista (only the new unsafe operations used needed to work to have the architecture working).
	\item \textbf{Machine-independent:} A stack-based representation allows to abstract away from low-level details such as the exact number of registers. To abstract away from the exact number of registers, both a stack-based IR and a register-based IR with an infinite number of registers are possible. Although at first look it seems the register-based solution is easier to deal with, experts such as the ones which designed WebAssembly~\cite{WebAssembly} chose to use a stack-based IR over a register-based IR. It is not clear which solution is better.
\end{itemize}	 

One future work is to design another extended bytecode set, register-based, and to evaluate the complexity of the representation in the back-end of Scorch and the front-end of Cogit, as shown on the right part of Figure \ref{fig:FutureWorkredundant}.

\paragraph{Lower-level representation.}The optimised v-functions are encoded in a quite high-level representation. For example, instructions like array accesses generate multiple native instructions. The extended bytecode set is quite high-level because it abstracts away from:
\begin{itemize}
	\item the memory representation of objects.
	\item the processor used.
\end{itemize}

We believe abstracting away from the processor used was a good idea as it allows to use snapshot to persist the optimised state across multiple start-ups. However, snapshots are already dependent on the memory representation of objects used, so one could implement a new representation of optimised v-functions with a lower-level representation, object representation dependent but still processor independent. This would allow Scorch to perform additional optimisations, such as better constant propagation (Some constants are currently hidden in high-level instructions), which are currently hard to support in Cogit.

Currently, only the VM code is aware of the memory representation of object. Hence, using such a lower-level representation would require to duplicate the knowledge about the memory representation of objects from the VM code-base to Pharo, so Scorch could be aware of it. 

\subparagraph{Summary.}To summarize, one could change Sista so optimised v-functions would be encoded in a lower-level representation instead of the extended bytecode set. Such changes would make Scorch dependent of the memory representation of objects, while keeping it independent from the processor used. Some code would need to be duplicated from the VM code-base to Pharo to make Scorch aware of the internal memory representation of objects. Scorch would be able to produce more optimised code, performing optimisations that are currently difficult to implement in Cogit, yielding hopefully better performance.

\subparagraph{Implementation.}To implement such a solution, one could build a back-end for Scorch targeting the abstract assembly instructions set featured by the Cog VM, called Lowcode~\cite{Salg16a}, similar to WebAssembly~\cite{WebAssembly}. Lowcode features low-level instructions, compiled almost in most processors one-to-one to machine instructions. The future work is to implement such a back-end for Scorch and evaluate the complexity and the performance.

\subsection{Development tools integration}
\label{ss:FWIDE}

In Sista, Scorch is an unoptimised v-functions to optimised v-functions compiler, running in the same runtime as the application. Although this design has several advantages, there is a major draw-back: when the programmer accesses the reification of a stack frame, depending on the optimisation state, an optimised frame might be shown. 

In some cases, for example when the programmer is working on Scorch itself, it is relevant to show in the development tools the optimised frames. In other cases, for example when the programmer is working in an end-user application on top of Pharo, development tools should show only unoptimised frames by transparently deoptimising stack frames.

We are now adapting the debugging tools to request deoptimise stack frames when needed. To do so, we are adding a development tool setting: one may or may not want to see the stack internals, depending on what one wants to implement.

\subsection{Memory footprint evaluation}
\label{ss:FWMemFootprint}

When implementing an optimising JIT, it is relevant to evaluate the memory footprint of the optimised code and the deoptimisation metadata. In our context, such an evaluation would be very interesting as:
\begin{itemize}
	\item \textbf{Split architecture:} Due to the split between Scorch and Cogit, optimised functions are present both as v-functions and n-functions, and each of them has different deoptimisation metadata, potentially increasing the memory footprint. 
	\item \textbf{Persistence:} As optimised v-functions are persisted across start-ups, it is interesting to know the size of the optimised v-functions and the corresponding deoptimisation metadata that is persisted.
\end{itemize}

Two future works are planned in this direction:
\begin{itemize}
	\item Does the split in the optimising JIT induce memory overhead compared to a classical function-based optimising JIT, and how big is the overhead ?
	\item What is the size of the optimised v-functions and the corresponding deoptimisation metadata that is persisted across start-up ?
\end{itemize}

We did not evaluate the memory footprint because currently the deoptimisation metadata of Scorch is kept uncompressed. As Scorch deoptimiser requires to read deoptimisation metadata, compressing the metadata requires to write a decompressor which is, as all the deoptimiser code, independent from the rest of the system. This is possible but requires a certain amount of engineering work, which is the reason why we postponed it.

\subsection{Platform-dependency}
\label{ss:FWPlatDep}

In Pharo, snapshots are independent of the processor and the OS used. It is proven as the same snapshot can be deployed on x86, ARMv6 and MIPSEL, as well as on Windows, Mac OS X, iOS, Linux, Android or RISC OS. However, the snapshots are dependent on the machine word size: 32 bit or 64 bit snapshots are not compatible. They are not compatible because the size of managed pointers is different, but also because the representation of specific objects, such as floating numbers, is different. It is however possible to convert offline a 32 bit snapshot to 64 bit and vice-versa. 

As some optimisations related to number arithmetics, such as overflow checks elimination, depends on the number representations, Scorch is currently dependent on the machine word size. A fully portable solution would either need not to do optimisations on machine word specific number representations or deoptimise the affected code on startup.

\subsection{Productisation}
\label{ss:FWProduct}

With the current version, the Sista VM is able to run all our benchmark suite. We are now able to do part of our development with the development tools, written in Pharo, running on the Sista VM. We still have work to do in the integration with the development tools, especially the debugger, but it seems that the biggest part of the work has been done. 

There are still some edge cases where the Sista VM is unstable, which still need to be fixed, but most code can be run on top of the Sista VM as it would be run on the production VM. We have started to integrate the dependencies of Sista in Pharo, such as the new implementation of closures, the new bytecode set or read-only objects. We are now looking forward to integrate Scorch in Pharo.

\section{New optimisations}
\label{sec:newOpt}

Another direction for future work is the implementation of new optimisations in Scorch or Cogit. 

\subsection{State-of-the-art optimisations}

To compare Sista against other optimising JITs efficiently, the next thing to do is to implement all the state-of-the-art compiler optimisation in Scorch and Cogit. Scorch lacks multiple common optimisation passes, including important ones for performance such as floating-pointer related optimisations or advanced escape analysis. Cogit features a naive register allocator to set registers in its dynamic templates, but we have plan to build a more advanced one and evaluate the performance difference. 

\subsection{New optimisations}

Aside from existing optimisations, one could implement new optimisations that are not present in other compilers. 

One way to do so is to have new ideas on how to optimise code, to design and implement new algorithms. This is far from trivial as many experts have worked in compiler optimisations in other compilers, but, it is theoretically possible.

Alternatively, one could describe how traditional optimisations are implemented in the context of the split architecture present in Sista. For example, one could explain which optimisation should be implemented in Scorch, which one should be implemented in Cogit, which one should be implemented partly with both and which annotations are required in the optimised v-functions to communicate extra information from Scorch to Cogit. 

Lastly, and most interestingly, one could work on Smalltalk-specific optimisations that are not possible or not relevant in other programming languages because they do not usually support the unconventional features present in Smalltalk.

Indeed, Smalltalk provides some unconventional operations that are not usually available in other object-oriented languages. These operations are problematic for the optimising JIT. The main operations we are talking about are become, described in Section \label{par:become} and heavy stack manipulation APIs on reified stack frames detailed in Section \ref{par:frameToContext}. 

In each case, the feature has implications in the context of an optimising JIT as at any interrupt point, there any temporary variable of the optimised stack frame, or worst, any internal state (sender frame, program counter, etc.) could be edited to any object in the heap. This would invalidate all assumptions taken at compilation time by Scorch.

Fortunately, all these operations are uncommon in a normal Smalltalk program at runtime. They are usually used for implementing the debugging functionalities of Pharo. Currently, profiling production applications does not show we could earn noticeable performance if we would optimise such cases. The current solution is therefore to always deoptimise the stack frames involved when such unconventional operations happen. In the case of become, if a temporary variable in a stack frame executing an optimised method is edited, the frame is deoptimised. In the case of the stack manipulation, if the reification of the stack is mutated from the language, we deoptimise the corresponding mutated stack frames.

However, in specific libraries or workflow, such operations may be common enough to have some impact on performance. Especially, continuations and exceptions are built in Pharo in the language on top of the stack manipulation features, and a few libraries use them extensively. It could be possible to have Scorch aware of these features and to handle them specifically. Scorch would for example mark some temporary variables as being accessed frequently from the outside of the function: such temporaries would not be optimised and the deoptimisation metadata would include information on how to access those temporaries of inlined functions directly in the optimised frame. Optimising such features would allow to have efficient continuations and exceptions, in a similar way to the optimisation of exceptions described in \cite{Ogas01a}. 

\section{Application of Sista for quick start-ups}
\label{sec:useCase}

One point that is a bit unclear is how to use the Sista for quick start-up in a real-world application. It is possible to persist optimised functions across start-ups. However, how are the optimised functions generated in the first place ? Are they generated from warm-up runs using a test suite or are they generated the first day the application is running ?

A good example on how to use persisted optimisations across start-up is the success story of Azul~\cite{Azul}: a trading bank claims that they are able to use the optimised functions generated the day N-1 to improve the start-up performance of the day N. Another good example is the user-base of the cloneable Java VM~\cite{Kawa07a}.

It seems that depending on the use-case where start-up performance matters (distributed application with short-lived slaves, such as the ones on Amazon lambda, Android application or Web pages), different frameworks and solutions require to be set up to improve start-up performance using persisted optimisations. 

It would be very valuable to focus on one of those use-cases and build a solid framework showing how to use the persistence of optimisations to reduce warm-up time and evaluate what is the cost for the application programmer. Does the programmer have to do something specific to persist the optimisations as part of the deployment (warm-up runs, etc.) or is it done automatically as part of a framework ?

In the case of distributed application on Amazon lambda, slaves may live down to a couple seconds in case of a high-variant demand, so that there are very few slaves rent when the application is unused and a lot of slaves when the application is heavily used. Is it possible to build a fraework that automatically learn from the life of previous slaves what optimised functions are worth keeping, so when a new short-lived slaved is instantiated, it can reach peak performance very quickly ?

In the case of web pages, is it possible to build a global cache so all frequently used web pages would have pre-optimised code available from the runs of the previous users ? How would such a design work with modern security requirements ?

All these applications of the persistence of optimisations across start-ups of Sista are very interesting and could be analysed in future work.

\section{Energy consumption evaluation}
\label{sec:energy}

Another interesting aspect of Sista is the energy saved at start-up by re-using persisted optimised functions instead of wasting cpu cycles re-generating them. 

One of the most relevant use-case is Android application. With the Dalvik VM or the Android Runtime, Google's team on the VM for Android application have switched their VM design from JIT compilation to AOT compilation then back to JIT compilation. The main problem is that JIT compilation yields better peak performance, but requires warm-up time for each Android application start-up which wastes many cpu cycles, which corresponds in practice to an important part of the smart phone battery.

Sista would be relevant in this context as the application could be shipped unoptimised, but each time the user would use the application, the optimised functions generated from previous runs would be persisted so further uses won't waste battery. This way, theoretically, the application could have very good peak performance due to the JIT while not wasting too many battery at each Android application start-up.

In addition, power drain is becoming an important factor not just in mobile computing but also computers embedded in things (Internet of Things) and even on servers. Excessive power leads to increased cooling. In flash memory, excessive writes can not only shorten its life but also lead to increased power (writes draw more power than reads).

To work in this direction, one would need to evaluate the energy consumed by the VM to reach peak-performance. We did not work in this direction because we did not have expertise in energy consumption measurement, but this is definitely a relevant future work.

\section*{Conclusion}

This Chapter discussed the future works that may be done based on this thesis. The next Chapter summarises the contents of the thesis and concludes.

\ifx\wholebook\relax\else
    \end{document}
\fi